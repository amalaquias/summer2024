{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Speed and Accuracy of RAG (WIP)\n",
    "\n",
    "This notebook shows the following: \n",
    "* RAG LangChain pipeline with three different repositories\n",
    "*  speed tests\n",
    "   *  embedding time of different models with different VBs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from langchain.vectorstores import Chroma\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader, PythonLoader, NotebookLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter, PythonCodeTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Ingestion\n",
    "Using specific data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"./sage-website/docs/\", \"./sage-website/news/\", \"./sage-data-client/\", \"./pywaggle/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively loads all \"file_type\" files in a list of directory \"paths\" with the appropriate \"loader_class\"\n",
    "## returns a combined list of Documents from all paths\n",
    "def repo_class_loader(paths, glob, loader_cls):\n",
    "    combined_docs = []\n",
    "    for path in paths:\n",
    "        dir_loader = DirectoryLoader(path, glob=glob, loader_cls=loader_cls, recursive=True)\n",
    "        docs = dir_loader.load()\n",
    "        combined_docs.extend(docs)\n",
    "\n",
    "    return combined_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.01 s, sys: 461 ms, total: 3.47 s\n",
      "Wall time: 16.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sage-website/docs/contact-us.md'}, page_content='sidebar_label: Contact us\\n\\nContact us\\n\\nEmail\\n\\nFor support, general questions, or comments, you can always reach us at:\\n\\nsupport@waggle-edge.ai\\n\\nMessage Board\\n\\nWe also encourage developers and users to start a new topic or issue on the Waggle sensor message board:\\n\\nGitHub Discussions'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/sesctl.md'}, page_content=\"sidebar_label: sesctl sidebar_position: 2\\n\\nsesctl: a tool to schedule jobs in Waggle edge computing\\n\\nThe tool sesctl is a command-line tool that communicates with an Edge scheduler in the cloud to manage user jobs. Users can create, edit, submit, suspend, and remove jobs via the tool.\\n\\nInstallation\\n\\nThe tool can be downloaded from the edge scheduler repository and be run on person's desktop or laptop.\\n\\n:::note Please make sure to download the correct version of the tool based on the system architecture. For example, if you run it on a Mac download sesctl-darwin-amd64. :::\\n\\nbash chmod +x sesctl-<system>-<arch> ln sesctl-<system>-<arch> sesctl sesctl\\n\\nSubmit a job\\n\\nYou can follow the tutorial to submit an example job to understand how to design your own job.\\n\\nFor more tutorials\\n\\nThe in-depth tutorials on the functionalities that sesctl offers can be found in the README.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/pluginctl.md'}, page_content='sidebar_label: pluginctl sidebar_position: 1\\n\\npluginctl: a tool to develop and test plugins on a node\\n\\nWe developed the tool pluginctl to help end users develop and test their edge application (i.e., plugin) on a node before registering the plugin in Edge code repository. The tool helps on simplifying the process of testing the edge code and making changes as needed for development, by buildig the code into a container, running the container inside the node, and checking the result from the container.\\n\\nAll of Waggle nodes have the tool already installed. For plugin developers who have access to nodes, they can simply type the following to start with once they are logged into a node, bash sudo pluginctl\\n\\nThe in-depth tutorials on the functionalities that pluginctl offers can be found in the README.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/triggers.md'}, page_content='sidebar_label: Trigger examples sidebar_position: 3\\n\\nTrigger Examples\\n\\nThis page provides a few examples of triggers within Sage. Triggers are programs which generally use data and events from the edge or cloud to automatically drive or notify other behavior in the system.\\n\\nCloud-to-Edge Examples\\n\\nCloud-to-edge triggers are programs running in the cloud which monitor events or external data sources and then, in response, change some behavior on the nodes.\\n\\nSevere Weather Trigger\\n\\nThis example starts and stops jobs in response to severe weather events scraped from the National Weather Service API.\\n\\nWildfire Trigger\\n\\nThis example looks at results from the smoke detector job and modify its own scheduling interval in response. The concept is that as smoke is detected, we want to run more frequent detections.\\n\\nEdge-to-Cloud Examples\\n\\nEdge-to-cloud triggers are programs which monitor data published from the nodes and use it, potentially along with additional data sources, to perform some computation or actions.\\n\\nSage Data Client Batch Trigger\\n\\nThis is a simple batch trigger example of using Sage Data Client to print nodes where the internal mean temperature exceeds a threshold every 5 minutes.\\n\\nSage Data Client Stream Trigger\\n\\nThis is an example of using Sage Data Client to watch the data stream and print nodes where the internal temperature exceeds a threshold.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='Developer quick reference\\n\\nDisclaimer\\n\\n:warning: This is a quick-reference guide, not a complete guide to making a plugin. Use this to copy-paste commands while working on plugins and to troubleshoot them in the testing and scheduling stages. Please consult the official :green_book:Plugin Tutorials for detailed guidance.\\n\\nTips\\n\\n:information_source: Plugin=App\\n\\n:green_book: = recommended code docs and tutorials from Sage.\\n\\n:point_right: First make a minimalistic app with a core functionality to test on the node. Later you may add all the options you want.\\n\\n:point_up: Avoid making a plugin from scratch. Use another plugin or this template for your first plugin or use :new: Cookiecutter Template.\\n\\n:warning: Repository names should be all in small alphanumeric letters and - (Do not use _)\\n\\nRequirements : Install Docker, git, and Python\\n\\nComponents of a plugin\\n\\nTypical components of a Sage plugin are described below:\\n\\n1. An application\\n\\nThis is just your usual Python program, either a single .py script or a set of directories with many components (e.g. ML models, unit tests, test data, etc).\\n\\n:point_right: First do this step on your machine and perfect it until you are happy with the core functionality.\\n\\napp/app.py* : the main Python file (sometimes also named main.py) contains the code that defines the functionality of the plugin or calls other scripts to do tasks. It usually has from waggle.plugin import Plugin call to get the data from in-built sensors and publishes the output.\\n\\nNote: Variable names in plugin.publish should be descriptive and specific.\\n\\nInstall pywaggle pip3 install -U \\'pywaggle[all]\\'\\n\\napp/test.py : optional but recommended file, contains the unit tests for the plugin.\\n\\n2. Dockerizing the app\\n\\n:point_right: Put everything in a Docker container using a waggle base image and make it work. This may require some work if libraries are not compatible. Always use the latest base images from Dockerhub\\n\\nDockerfile* : contains instructions for building a Docker image for the plugin. It specifies the waggle base image from dockerhub, sets up the environment, installs dependencies, and sets the entrypoint for the container.\\n\\n:warning: Keep it simple ENTRYPOINT [\"python3\", \"/app/app.py\"]\\n\\nrequirements.txt* : lists the Python dependencies for the plugin. It is used by the Dockerfile to install the required packages using pip.\\n\\nbuild.sh : is an optional shell script to automate building the complicated Docker image with tags etc.\\n\\nMakefile : optional but the recommended file includes commands for building the Docker image, running tests, and deploying the plugin.\\n\\n3. ECR configs and docs\\n\\nYou can do this step (except sage.yaml) after testing on the node but before the ERC submission. :smile:\\n\\nsage.yaml* : is the configuration file useful for ECR and job submission? Most importantly it specifies the version and input arguments.\\n\\nREADME.md and ecr-meta/ecr-science-description.md* : a Markdown file describing the scientific rationale of the plugin as an extended abstract. This includes a description of the plugin, installation instructions, usage examples, data downloading code snippets, and other relevant information.\\n\\n:bulb: Keep the same text in both files and follow the template of ecr-science-description.md.\\n\\necr-meta/ecr-icon.jpg : is an icon (512px x 512px or smaller) for the plugin in the Sage portal.\\n\\necr-meta/ecr-science-image.jpg : is a key image or figure plot that best represents the scientific output of the plugin.\\n\\n:::info :green_book: Check Sage Tuorial Part1 and Part2 :::\\n\\nGetting access to the node\\n\\nFollow this page: https://portal.sagecontinuum.org/account/access to access the nodes.\\n\\nTo test your connection the first time, execute ssh waggle-dev-sshd and enter your ssh key passphrase. You should get the following output,\\n\\nEnter passphrase for key /Users/bhupendra/.ssh/id_rsa: no command provided Connection to 192.5.86.5 closed.\\n\\nEnter the passphrase to continue.\\n\\nTo connect to the node, execute ssh waggle-dev-node-V032 and enter your passphrase (required twice).\\n\\nYou should see the following message,\\n\\nWe are connecting you to node V032\\n\\n:::info :green_book: See Sage Tuorial: Part 3 for details on this topic. :::\\n\\nTesting plugins on the nodes\\n\\n:::danger :warning: Do not run any app or install packages directly on the node. Use Docker container or pluginctl commands. :::\\n\\n1. Download and run it\\n\\nDownload\\n\\nIf you have not already done it, you need your plugin in a public GitHub repository at this stage.\\n\\nTo test the app on a node, go to nodes W0xx (e.g. W023) and clone your repo there using the command git clone.\\n\\nAt this stage, you can play with your plugin in the docker container until you are happy. Then if there are changes made to the plugin, I reccomend replicating the same in your local repository and pushing it to the github and node.\\n\\nor do git commit -am \\'changes from node\\' and git push -u origin main.\\n\\nHowever, before commiting from node, you must run following commands at least once in your git repository on the node. git config [--locale] user.name \"Full Name\" git config [--locale] user.email \"email@address.com\"\\n\\n:::danger :warning: Make sure your Dockerfile has a proper entrypoint or the pluginctl run will fail. :::\\n\\nTesting with Pluginctl\\n\\n:::info :green_book: For more details on this topic check pluginctl docs. :::\\n\\nThen to test execute the command sudo pluginctl build .. This will output the plugin-image registry address at the end of the build. Example: 10.31.81.1:5000/local/my-plugin-name\\n\\nTo run the plugin without input argument, use sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name>\\n\\nExecute the command with input arguments. sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name> -- -i top_camera.\\n\\nIf you need GPU, use the selector sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name> -- -i top_camera.\\n\\n:exclamation: -- is a separator. After the -- all arguments are for your entrypoint i.e. app.py.\\n\\nTo check running plugins, execute sudo pluginctl ps.\\n\\nTo stop the plugin, execute sudo pluginctl rm cloud-motion.\\n\\nTo check the log pluginctl logs cloud-motion :warning:Do not forget to stop the plugins after testing or it will run forever.\\n\\nTesting USBSerial devices\\n\\n:point_right:The USBserial device template is in Cookiecutter Template. Also check wxt536 plugin.\\n\\nSteps for working with a USB serial device\\n\\nFirst, you need to confirm which computing unit the USB device is connected to, RPi or nxcore.\\n\\nThen, you add the --selector and --privileged options to the pluginctl command during testing and specifying the same in the job.yaml for scheduling.\\n\\nTo test the plugin on nxcore, which has the USB device, use the command sudo pluginctl run -n testname --selector zone=core --privileged 10.31.81.1:5000/local/plugin-name.\\n\\nThe --selector and --privileged attributes should be added to the pluginSpec in the job submission script as shown in the example YAML code.\\n\\nYou can check which computing unit is being used by the edge scheduler by running the kubectl describe pod command and checking the output.\\n\\n:warning: Re/Check that you are using the correct USB port for the device if getting empty output or folder not found error.\\n\\n2. Check if it worked?\\n\\nLogin to the Sage portal and follow the instructions from the section See Your Data on Sage Portal\\n\\n3. Check why it failed?\\n\\nWhen you encounter a failing/long pending job with an error, you can use the following steps to help you diagnose the issue:\\n\\nFirst check the Dockerfile entrypoint.\\n\\nUse the command sudo kubectl get pod to get the name of the pod associated with the failing job.\\n\\nUse the command sudo kubectl logs <<POD_NAME>> to display the logs for the pod associated with the failing job. These logs will provide you with information on why the job failed.\\n\\nUse the command sudo kubectl describe pod POD_NAME to display detailed information about the pod associated with the failing job.\\n\\nThis information can help you identify any issues with the pod itself, such as issues with its configuration or resources.\\n\\nBy following these steps, you can better understand why the job is failing and take steps to resolve the issue.\\n\\n4. Troubleshooting inside the container using pluginctl\\n\\nFollow this tutorial to get in an already running container to troubleshoot the issue. If the plugin fails instantly and your are not able to get inside the container use following commands to override the entrypoint\\n\\nFirst Deploy with Custom Entrypoint --entrypoint /bin/bash: sudo pluginctl deploy -n testnc --entrypoint /bin/bash 10.31.81.1:5000/local/plugin-mobotix-scan -- -c \\'while true; do date; sleep 1; done\\' Note the -c \\'while true; do date; sleep 1; done\\' instead of your usual plugin arguments. Now if you do sudo pluginctl logs testnc you will see the logs i.e. date.\\n\\nAccess the Plugin Container: sudo pluginctl exec -ti testnc -- /bin/bash\\n\\nEdge Code Repository\\n\\nHow to get your plugin on ECR\\n\\nTo publish your Plugin on ECR, follow these steps: 1. Go to https://portal.sagecontinuum.org/apps/. 2. Click on \"Explore the Apps Portal\". 3. Click on \"My Apps\". You must be logged in to continue. 4. Click \"Create App\" and enter your Github Repo URL. 5. \\'Click \"Register and Build App\". 6. On Your app page click on the \"Tags\" tab to get the registry link when you need to run the job on the node either using pluginctl or job script. This will look like:docker pull registry.sagecontinuum.org/bhupendraraut/mobotix-move:1.23.3.2 7. Repeat the above process for updating the plugin.\\n\\n:::warning After the build process is complete, you need to make the plugin public to schedule it. :::\\n\\n:point_right: If you have skipped step 3. ECR Configs and Docs, do it before submitting it to the ECR. Ensure that your ecr-meta/ecr-science-description.md and sage.yaml files are properly configured for this process.\\n\\nVersioning your code\\n\\n:::danger You can not resubmit the plugin to ECR with the same version number again. ::: - So think about how you change it every time you resubmit to ERC and make your style of versioning. :thinking_face: - I use \\'vx.y.m.d\\' e.g. \\'v0.23.3.4\\' but then I can only have 1 version a day, so now I am thinking of adding an incremental integer to it.\\n\\nAfter ECR registry test (generally not required)\\n\\nGenerally successfully tested plugins just work. However, in case they are failing in the scheduled jobs after running for a while or after successfully running in the above tests, do the following.\\n\\nTo test a plugin on a node after it has been built on the ECR, follow these steps: sudo pluginctl run --name test-run registry.sagecontinuum.org/bhupendraraut/cloud-motion:1.23.01.24 -- -input top\\n\\nThis command will execute the plugin with the specified ECR image (version 1.23.01.24), passing the \"-input top\" argument to the plugin (Note -- after the image telling pluginctl that these arguments are for the plugin).\\n\\n:point_right: Note the use of sudo in all pluginctl and docker commands on the node.\\n\\nAssuming that the plugin has been installed correctly and the ECR image is available, running this command should test the \"test-motion\" plugin on the node.\\n\\nYou may also have to call the kubectl <POD> commands as in the testing section if this fails.\\n\\nScheduling the job\\n\\n:::warning :exclamation: If you get an error like registry does not exist in ECR, then check that your plugin is made public. :::\\n\\nFollow this link to get an understanding of how to submit a job\\n\\nHere are the parameters we set for the Mobotix sampler plugin,\\n\\nless= -name thermalimaging registry.sagecontinuum.org/bhupendraraut/mobotix-sampler:1.22.4.13 \\\\ --ip 10.31.81.14 \\\\ -u userid \\\\ -p password \\\\ --frames 1 \\\\ --timeout 5 \\\\ --loopsleep 60 - Your science rule can be a cronjob (More information can be found here - This runs every 15 minutes \"thermalimaging\": cronjob(\"thermalimaging\", \"*/15 * * * *\"). - Use Crontab Guru. - You can also make it triggered by a value. Please read this for supported functions.\\n\\nScheduling scripts\\n\\n:sparkles: Check user friendly job submission UI.\\n\\n:green_book: Check sesctl docs for command line tool.\\n\\n:point_up: Do not use _, upper case letters or . in the job name. Use only lowercase letters, numbers and -.\\n\\n:point_up: Ensure that the plugin is set to \\'public\\' in the Sage app portal.\\n\\njob.yaml example for USB device\\n\\nyaml= name: atmoswxt plugins: - name: waggle-wxt536 pluginSpec: image: registry.sagecontinuum.org/jrobrien/waggle-wxt536:0.23.4.13 privileged: true selector: zone: core nodeTags: [] nodes: W057: true W039: true scienceRules: - \\'schedule(\"waggle-wxt536\"): cronjob(\"waggle-wxt536\", \"1/10 * * * *\")\\' successCriteria: - WallClock(\\'1day\\')\\n\\nMultiple jobs example\\n\\nIf you want to run your plugins not all at the same time. Use this example.\\n\\n```yaml= name: w030-k3s-upgrade-test plugins: - name: object-counter-bottom pluginSpec: image: registry.sagecontinuum.org/yonghokim/object-counter:0.5.1 args: - -stream - bottom_camera - -all-objects selector: resource.gpu: \"true\" - name: cloud-cover-bottom pluginSpec: image: registry.sagecontinuum.org/seonghapark/cloud-cover:0.1.3 args: - -stream - bottom_camera selector: resource.gpu: \"true\" - name: surfacewater-classifier pluginSpec: image: registry.sagecontinuum.org/seonghapark/surface_water_classifier:0.0.1 args: - -stream - bottom_camera - -model - /app/model.pth - name: avian-diversity-monitoring pluginSpec: image: registry.sagecontinuum.org/dariodematties/avian-diversity-monitoring:0.2.5 args: - --num_rec - \"1\" - --silence_int - \"1\" - --sound_int - \"20\" - name: cloud-motion-v1 pluginSpec: image: registry.sagecontinuum.org/bhupendraraut/cloud-motion:1.23.02.20 args: - --input - bottom_camera - name: imagesampler-bottom pluginSpec: image: registry.sagecontinuum.org/theone/imagesampler:0.3.1 args: - -stream - bottom_camera - name: audio-sampler pluginSpec: image: registry.sagecontinuum.org/seanshahkarami/audio-sampler:0.4.1 nodeTags: [] nodes: W030: true scienceRules: - \\'schedule(object-counter-bottom): cronjob(\"object-counter-bottom\", \"/5 * * * \")\\' - \\'schedule(cloud-cover-bottom): cronjob(\"cloud-cover-bottom\", \"01-59/5 * * * \")\\' - \\'schedule(surfacewater-classifier): cronjob(\"surfacewater-classifier\", \"02-59/5 * * * \")\\' - \\'schedule(\"avian-diversity-monitoring\"): cronjob(\"avian-diversity-monitoring\", \" * * * \")\\' - \\'schedule(\"cloud-motion-v1\"): cronjob(\"cloud-motion-v1\", \"03-59/5 * * * \")\\' - \\'schedule(imagesampler-bottom): cronjob(\"imagesampler-bottom\", \"04-59/5 * * * \")\\' - \\'schedule(audio-sampler): cronjob(\"audio-sampler\", \"/5 * * * \")\\' successCriteria: - Walltime(1day)\\n\\n```\\n\\nhere objecct-counter runs at 0, 5, 10, etc cloud-cover: 1, 6, 11, etc. surface water: 2, 7, 12, etc. cloud-motion: 3, 8, 13, etc. image-sampl: 4, 9, 14, etc.\\n\\nDebugging failed jobs\\n\\nDo you know how to identify why a job is failing\\n\\n:sparkles: When the job failures are seen as red markers on your job page, you can click them to see the error.\\n\\nOr detail errors can be found using using sage_data_client\\n\\nRequirements: sage_data_client and utils.py\\n\\nBy specifying the plugin name and node, the following code will print out the reasons for job failure within the last 60 minutes.\\n\\n```python= from utils import *\\n\\nmynode = \"w030\"\\n\\nmyplugin = \"water\" df = fill_completion_failure(parse_events(get_data(mynode, start=\"-60m\"))) for _, p in df[df[\"plugin_name\"].str.contains(myplugin)].iterrows(): print(p[\"error_log\"]) ```\\n\\nDownloading the data\\n\\nSage docs for accessing-data\\n\\nSee Your Data on Sage Portal\\n\\nTo check your data on Sage Portal, follow these steps: 1. Click on the Data tab at the top of the portal page. 2. Select Data Query Browser from the dropdown menu. 3. Then, select your app in the filter. This will show all the data that is uploaded by your app using the plugin.publish() and plugin.upload() methods.\\n\\nIn addition, you can data visualize as a time series and select multiple variables to visualize together in a chart, which can be useful for identifying trends or patterns.\\n\\nDownload all images with wget\\n\\nVisit https://training-data.sagecontinuum.org/\\n\\nselect the node and period for data.\\n\\nSelect the required data and download the text file urls-xxxxxxx.txt with urls\\n\\nTo select only the top camera images, use the vim command: g/^\\\\(.*top\\\\)\\\\@!.*$/d. This will delete URLs that do not contain the word \\'top\\'\\n\\nCopy the following command from the website and run it in your terminal. wget -r -N -i urls-xxxxxxx.txt\\n\\nSage data client for text data\\n\\nSage data client python Notebook Example\\n\\npypi link pip install sage-data-client\\n\\n:::info :green_book: Documentation for accessing the data. :::\\n\\nQuerying data example\\n\\nThe sage_data_client provides query() function which takes the parameters:\\n\\n```python import sage_data_client import pandas as pd\\n\\ndf = sage_data_client.query( start=\"2023-01-08T00:00:09Z\", # Start time in \"YYYY-MM-DDTHH:MM:SSZ\" or \"YYYYMMDD-HH:MM:SS\" format end=\"2024-01-08T23:23:24Z\", # End time in the same format as start time filter={ \"plugin\": \".mobotix-scan.\", # Regex for filtering by plugin name \"vsn\": \"W056\", # Specific node identifier \"name\": \"upload\", # Specific data field \"filename\": \".*_position1.nc\" # Regex for filtering filenames } )\\n\\ndf.sort_values(\\'timestamp\\') df ```\\n\\nFilter Criteria\\n\\nstart and end: Time should be specified in UTC, using the format YYYY-MM-DDTHH:MM:SSZ or YYYYMMDD-HH:MM:SS.\\n\\nfilter: A dictionary for additional filtering criteria. Each key is a column name in the df.\\n\\nUse regular expressions (denoted as .*pattern.*) for flexible matching within text fields like plugin or filename.\\n\\nDownloading Files\\n\\nUse additional pandas operations on df to to include only the records of interest and download the files using a function like the one provided below, which gets the URLs in the value column, using authentication.\\n\\n```python import requests import os from requests.auth import HTTPBasicAuth\\n\\nuname = \\'username\\' upass = \\'token_as_password\\'\\n\\ndef download_files(df, download_path, uname, upass): # check download directory if not os.path.exists(download_path): os.makedirs(download_path)\\n\\nfor index, row in df.iterrows(): # \\'value\\' column has url url = row[\\'value\\']\\n\\n  filename = url.split(\\'/\\')[-1]\\n\\n  # Download using credentials\\n  response = requests.get(url, auth=HTTPBasicAuth(uname, upass))\\n  if response.status_code == 200:\\n     # make the downloads path\\n     file_path = os.path.join(download_path, filename)\\n     # Write a new file\\n     with open(file_path, \\'wb\\') as file:\\n     file.write(response.content)\\n     print(f\"Downloaded {filename} to {file_path}\")\\n  else:\\n     print(f\"Failed to download {url}, status code: {response.status_code}\")\\n\\nusage\\n\\ndownload_files(df, \\'/Users/bhupendra/projects/epcape_pier/data/downloaded/nc_pos1\\', uname, upass) ```\\n\\nMore data analysis resources\\n\\nSAGE Examples\\n\\nCROCUS Cookbooks\\n\\nMiscellaneous\\n\\nFind PT Mobotix thermal camera ip on the node\\n\\nLogin to the node where the PTmobotix camera is connected. 1. run nmap -sP 10.31.81.1/24\\n\\nNmap scan report for ws-nxcore-000048B02D3AF49F (10.31.81.1) Host is up (0.0012s latency). Nmap scan report for switch (10.31.81.2) Host is up (0.0058s latency). Nmap scan report for ws-rpi (10.31.81.4) Host is up (0.00081s latency). Nmap scan report for 10.31.81.10 Host is up (0.0010s latency). Nmap scan report for 10.31.81.15 Host is up (0.00092s latency). Nmap scan report for 10.31.81.17 Host is up (0.0014s latency). Nmap done: 256 IP addresses (6 hosts up) scanned in 2.42 seconds\\n\\nFrom the output run any command for each ip e.g. curl -u admin:meinsm -X POST http://10.31.81.15/control/rcontrol?action=putrs232&rs232outtext=%FF%01%00%0F%00%00%10\\n\\nThe ip for which output is OK is the Mobotix.\\n\\nSSH \\'Broken Pipe\\' Issue and Solution\\n\\nA \\'Broken pipe\\' occurs when the SSH session to waggle-dev-node is inactive for longer than 10/15 minutes, resulting in a closed connection.\\n\\nclient_loop: send disconnect: Broken pipe Connection to waggle-dev-node-w021 closed by remote host. Connection to waggle-dev-node-w021 closed.\\n\\nSolution\\n\\nTo prevent the SSH session from timing out and to maintain the connection, the following configuration options can be added to the SSH config file: ```ssh\\n\\nKeep the SSH connection alive by sending a message to the server every 60 seconds\\n\\nHost * TCPKeepAlive yes ServerAliveInterval 60 ServerAliveCountMax 999 ```'),\n",
       " Document(metadata={'source': 'sage-website/docs/installation-manuals/wsn-manual.md'}, page_content='Wild Sage Node manual\\n\\nThe Wild Sage Node \"Getting Started\" manual is a complete overview of getting started with your new WSN.\\n\\nDownload WSN manual'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='sidebar_label: Architecture sidebar_position: 2\\n\\nArchitecture\\n\\nThe cyberinfrastructure consists of coordinating hardware and software services enabling AI at the edge. Below is a quick summary of the different infrastructure pieces, starting at the highest-level and zooming into each component to understand the relationships and role each plays.\\n\\nHigh-Level Infrastructure\\n\\nThere are 2 main components of the cyberinfrastructure: - Nodes that exist at the edge - The cloud that hosts services and storage systems to facilitate running “science goals” @ the edge\\n\\nEvery edge node maintains connections to 2 core cloud components: one to a Beehive and one to a Beekeeper\\n\\nBeekeeper\\n\\nThe Beekeeper is an administrative server that allows system administrators to perform actions on the nodes such as gather health metrics and perform software updates. All nodes \"phone home\" to their Beekeeper and maintain this \"life-line\" connection.\\n\\nDetails & source code: https://github.com/waggle-sensor/beekeeper\\n\\nBeehive\\n\\nThe Node-to-Beehive connection is the pipeline for the science. It is over this connection that instructions for the node will be sent, in addition to how data is published into the Beehive storage systems from applications (plugins) running on the nodes.\\n\\nThe overall infrastructure supports multiple Beehives, where each node is associated with a single Beehive. The set of nodes associated with a Beehive creates a \"project\" where each \"project\" is separate, having its own data store, web services, etc.\\n\\nIn the example above, there are 2 nodes associated with Beehive 1, while a single node is associated with Beehive 2. With all nodes, in this example, being administered by a single Beekeeper.\\n\\nNote: the example above shows a single Beekeeper, but a second Beekeeper could have been used for administrative isolation.\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-beehive-v2\\n\\nBeehive Infrastructure\\n\\nLooking deeper into the Beehive infrastructure, it contains 2 main components: - software services such as the Edge Scheduler (ES), Lambda Triggers (LT), data APIs, and websites/portals - data storage systems such as the Data Repository (DR) and the Edge Code Repository (ECR)\\n\\nThe Beehive is the “command center” for interacting with the Waggle nodes at the edge. Hosting websites and interfaces allowing scientists to create science goals to run plugins at the edge & browse the data produced by those plugins.\\n\\nThe software services and data storage systems are deployed within a kubernetes environment to allow for easy administration and to support running in a multiple server architecture, supporting redundancy and service replication.\\n\\nWhile the services running within Beehive are many (both graphical and REST style API interfaces), the following is an outline of the most vital.\\n\\nData Repository (DR)\\n\\nThe Data Repository is the data store for housing all the edge produced plugin data. It consists of different storage technologies (i.e. influxdb) and techniques to store simple textual data (i.e. key-value pairs) in addition to large blobular data (i.e. audio, images, video). The Data Repository additionally has an API interface for easy access to this data.\\n\\nThe data store is a time-series database of key-value pairs with each entry containing metadata about how and when the data originated @ the edge. Included in this metadata is the data collection timestamp, plugin version used to collect the data, the node the plugin was run on, and the specific compute unit within the node that the plugin was running on.\\n\\njson { \"timestamp\":\"2022-06-10T22:37:47.369013647Z\", \"name\":\"iio.in_temp_input\", \"value\":25050, \"meta\":{ \"host\":\"0000dca632ed6d06.ws-rpi\", \"job\":\"sage\", \"node\":\"000048b02d35a97c\", \"plugin\":\"plugin-iio:0.6.0\", \"sensor\":\"bme680\", \"task\":\"iio-rpi\", \"vsn\":\"W08C\" } }\\n\\nIn the above example, the value of 25050 was collected @ 2022-06-10T22:37:47.369013647Z from the bme680 sensor on node 000048b02d35a97c via the plugin-iio:0.6.0 plugin.\\n\\nNote: see the Access and use data site for more details and data access examples.\\n\\nDetails & source code: https://github.com/waggle-sensor/data-repository\\n\\nEdge Scheduler (ES)\\n\\nThe Edge Scheduler is defined as the suite of services running in Beehive that facilitate running plugins @ the edge. Included here are user interfaces and APIs for scientists to create and manage their science goals. The Edge Scheduler continuously analyzes node workloads against all the science goals to determine how the science goals are deployed to the Beehive nodes. When it is determined that a node\\'s science goals are to be updated, the Edge Scheduler interfaces with WES running on those nodes to update the node\\'s local copy of the science goals. Essentially, the Edge Scheduler is the overseer of all the Beehive\\'s nodes, deploying science goals to them to meet the scientists plugin execution objectives.\\n\\nDetails & source code: https://github.com/waggle-sensor/edge-scheduler\\n\\nEdge Code Repository (ECR)\\n\\nThe Edge Code Repository is the \"app store\" that hosts all the tested and benchmarked edge plugins that can be deployed to the nodes. This is the interface allowing users to discover existing plugins (for potential inclusion in their science goals) in addition to submitting their own. At it\\'s core, the ECR provides a verified and versioned repository of plugin Docker images that are pulled by the nodes when a plugin is to be downloaded as run-time component of a science goal.\\n\\nDetails & source code: https://github.com/waggle-sensor/edge-code-repository\\n\\nLambda Triggers (LT)\\n\\nThe Lambda Triggers service provides a framework for running reactive code within the Beehive. There are two kinds of reaction triggers considered here: From-Edge and To-Edge.\\n\\nFrom-Edge triggers, or messages that originate from an edge node, can be used to trigger lambda functions -- for example, if high wind velocity is detected, a function could be triggered to determine how to reconfigure sensors or launch a computation or send an alert.\\n\\nTo-Edge triggers are messages that are to change a node\\'s behavior. For example an HPC calculation or cloud-based data analysis could trigger an Edge Scheduler API call to request a science goal to be run on a particular set of edge nodes.\\n\\nDetails & source code: https://github.com/waggle-sensor/lambda-triggers\\n\\nNodes\\n\\nNodes are the edge computing component of the cyberinfrastructure. All nodes consist of 3 items: 1. Persisent storage for housing downloaded plugins and caching published data before it is transferred to the node\\'s Beehive 2. CPU and GPU compute modules where plugins are executed and perform the accelerated inferences 3. Sensors such as environment sensors, cameras and LiDAR systems\\n\\nEdge nodes enable fast computation @ the edge, leveraging the large non-volatile storage to handle caching of high frequency data (including images, audio and video) in the event the node is \"offline\" from its Beehive. Through expansion ports the nodes support the adding and removing of sensors to fully customize the node deployments for the particular deployment environment.\\n\\nOverall, even though the nodes may use different CPU architectures and different sensor configurations, they all leverage the same Waggle Edge Stack (WES) to run plugins.\\n\\nWild Sage Node (Wild Waggle Node)\\n\\nThe Wild Sage Node (or Wild Waggle Node) is a custom built weather-proof enclosure intended for remote outdoor installation. The node features software and hardware resilience via a custom operating system and custom circuit board. Internal to the node is a power supply and PoE network switch supporting the addition of sensors through standard Ethernet (PoE), USB and other embedded protocols via the node expansion ports.\\n\\nThe technical capabilities of these nodes consists of: - NVidia Xavier NX ARM64 Node Controller w/ 8GB of shared CPU/GPU RAM - 1 TB of NVMe storage - 4x PoE expansion ports - 1x USB2 expansion port - optional Stevenson Shield housing a RPi 4 w/ environmental sensors & microphone - optional 2nd NVidia Xavier NX ARM64 Edge Processor\\n\\nNode installation manual: https://sagecontinuum.org/docs/installation-manuals/wsn-manual\\n\\nDetails & source code: https://github.com/waggle-sensor/wild-waggle-node\\n\\nBlade Nodes\\n\\nA Blade Node is a standard commercially available server intended for use in a climate controlled machine room, or extended temperature range telecom-grade blades for harsher environments. The AMD64 based operating system supports these types of nodes, enabling the services needed to support WES.\\n\\nThe above diagram shows the basic technical configuration of a Blade Node: - Multi-core ARM64 - 32GB of RAM - Dedicated NVida T4 GPU - 1 TB of SSD storage\\n\\nNote: it is possible to add the same optional Stevenson Shield housing that is available to the Wild Sage Nodes\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-blade\\n\\nRunning plugins @ the Edge\\n\\nIncluded in the Waggle operating systems are the core components necessary to enable running plugins @ the edge. At the heart of this is k3s, which creates a protected & isolated run-time environment. This environment combined with the tools and services provided by WES enable plugin access to the node\\'s CPU, GPU, sensors and cameras.\\n\\nWaggle Edge Stack (WES)\\n\\nThe Waggle Edge Stack is the set of core services running within the edge node\\'s k3s run-time environment that supports all the features that plugins need to run on the Waggle nodes. The WES services coordinate with the core Beehive services to download & run scheduled plugins (including load balancing) and facilitate uploading plugin published data to the Beehive data repository. Through abstraction technologies and WES provided tools, plugins have access to sensor and camera data.\\n\\nThe above diagram demonstrates 2 plugins running on a Waggle node. Plugin 1 (\"neon-kafka\") is an example plugin that is running alongside Plugin 2 (\"data-smooth\"). In this example, \"neon-kafka\" (via the WES tools) is reading metrics from the node\\'s sensors and then publishing that data within the WES run-time environment (internal to the node). At the same time, the \"data-smooth\" plugin is subscribing to this data stream, performing some sort of inference and then publishing the inference results (via WES tools) to Beehive.\\n\\nNote: see the Edge apps guide on how to create a Waggle plugin.\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-edge-stack\\n\\nWhat is a plugin?\\n\\nPlugins are the user-developed modules that the cyberinfrastructure is designed around. At it\\'s simplest definition a \"plugin\" is code that runs @ the edge to perform some task. That task may be simply collecting sample camera images or a complex inference combining sensor data and results published from other plugins. A plugin\\'s code will interface with the edge node\\'s sensor(s) and then publish resulting data via the tools provided by WES. All developed plugins are hosted by the Beehive Edge Code Repository.\\n\\nSee how to create plugins for details.\\n\\nScience Goals\\n\\nA \"science goal\" is a rule-set for how and when plugins are run on edge nodes. These science goals are created by scientist to accomplish a science objective through the execution of plugins in a specific manner. Goals are created, in a human language, and managed within the Beehive Edge Scheduler. It is then the cyberinfrastucture responsibility to deploy the science goals to the edge nodes and execute the goal\\'s plugins. The tutorial walks through running a science goal.\\n\\nLoRaWAN\\n\\nThe Waggle Edge Stack includes the ChirpStack software stack and other services to facilitate communication between Nodes and LoRaWAN devices. This empowers Nodes to effortlessly establish connections with wireless sensors, enabling your plugins to seamlessly access and harness valuable data from these sensors.\\n\\nTo get started using LoRaWAN, head over to the Contact Us page. A tutorial will be available soon showing you how to get started with LoRaWAN.\\n\\nThe above diagram demonstrates the hardware in Nodes and services in WES that enable Nodes to use LoRaWAN and publish the measurements to a Beehive. The following sections will explain each componenent and service.\\n\\nsource code: - wes-chirpstack - wes-chirpstack-server - wes-rabbitmq - Tracker - Lorawan Listener Plugin\\n\\nWhat is LoRaWAN?\\n\\nLoRaWAN, short for \"Long Range Wide Area Network,\" is a wireless communication protocol designed for low-power, long-range communication between IoT (Internet of Things) devices. It employs a low-power wide-area network (LPWAN) technology, making it ideal for connecting remote sensors and devices. For more information view the documentation here.\\n\\nChirpstack\\n\\nChirpStack is a robust and open-source LoRaWAN Network Server that enables efficient management of LoRaWAN devices, gateways, and data. Its architecture consists of several crucial components, each serving a distinct role in LoRaWAN network operations. Below, we provide a brief overview of these components along with links to ChirpStack documentation for further insights.\\n\\nChirpstack documentation\\n\\nUDP Packet Forwarder\\n\\nThe UDP Packet Forwarder is an essential component that acts as a bridge between LoRa gateways and the ChirpStack Network Server. It receives incoming packets from LoRa gateways and forwards them to the ChirpStack Gateway Bridge for further processing. To learn more about the UDP Packet Forwarder, refer to the documentation here.\\n\\nChirpStack Gateway Bridge\\n\\nThe ChirpStack Gateway Bridge is responsible for translating gateway-specific protocols into a standard format for the ChirpStack Network Server. It connects to a UDP Packet Forwader, ensuring that data is properly formatted and can be seamlessly processed by the network server. For in-depth information on the ChirpStack Gateway Bridge, explore the documentation here.\\n\\nMQTT Broker\\n\\nWES includes a MQTT (Message Queuing Telemetry Transport) broker to handle communication between various services. MQTT provides a lightweight and efficient messaging system. This service ensures that data flows smoothly between the network server, gateways, and applications. You can find detailed information about the MQTT broker integration in the ChirpStack documentation here.\\n\\nChirpStack Server\\n\\nThe ChirpStack Server serves as the core component, managing device sessions, data, and application integrations. It utilizes Redis for device sessions, metrics, and caching, ensuring efficient data handling and retrieval. For persistent data storage, ChirpStack uses PostgreSQL, accommodating records for tenants, applications, devices, and more. For a comprehensive understanding of the ChirpStack Server and its associated database technologies, consult the ChirpStack documentation here.\\n\\nNOTE: Chirpstack v4 combined the application and network server into one component.\\n\\nTracker\\n\\nThe Tracker is a service designed to record the connectivity of LoRaWAN devices to the Nodes. This service uses the information received from the MQTT broker to call ChirpStack\\'s gRPC API. The information received from the API is then used to keep the Node\\'s manifest up-to-date. Subsequently, it forwards this updated manifest to the Beehive. For more information, view the documentation here.\\n\\nLorawan Listener Plugin\\n\\nThe LoRaWAN Listener is a plugin designed to publish measurements collected from LoRaWAN devices. It simplifies the process of extracting and publishing valuable data from these devices. For more information about the plugin view the plugin page here.\\n\\nLorawan Device Compatibility\\n\\nThe Wild Sage Node is designed to support a wide range of Lorawan devices, ensuring flexibility and adaptability for various applications. If you are wondering which Lorawan devices can be connected to a Wild Sage Node, the device must have the following tech specs:\\n\\ndesigned for US915 (902–928 MHz) frequency region.\\n\\ncompatible with Lorawan Mac versions 1.0.0 - 1.1.0\\n\\ncompatible with Chirpstack\\'s Lorawan Network Server\\n\\nThe device supports Over-The-Air Activation (OTAA) or Activation By Personalization (ABP)\\n\\nThe device has a Lorawan device class of A, B, or C\\n\\nIt is important to note that all channels within the US915 frequency band are enabled in a Wild Sage Node. If you wish to learn more about our Lorawan Gateway, please visit our portal. For inquiries about supporting Lorawan regions other than US915, please Contact Us.\\n\\nDevice Examples\\n\\nWhether you are designing your own Lorawan sensor, looking for a Lorawan data logger, or seeking an off-the-shelf Lorawan device the Wild Sage Node will support it, we have examples for you:\\n\\nDesigning your own Lorawan sensor?\\n\\nArduino MKR WAN 1310\\n\\nLooking for a Lorawan data logger?\\n\\nICT International MFR Node\\n\\nLooking for an off-the-shelf Lorawan device?\\n\\nICT International SFM1X Sap Flow Meter\\n\\nSeeking Lorawan device manufacturers?\\n\\nICT International\\n\\nRAKwireless\\n\\nThe Things Network Device Marketplace\\n\\nDecentLab'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='sidebar_label: Overview sidebar_position: 1\\n\\nSage: A distributed software-defined sensor network.\\n\\nWhat is Sage?\\n\\nGeographically distributed sensor systems that include cameras, microphones, and weather and air quality stations can generate such large volumes of data that fast and efficient analysis is best performed by an embedded computer connected directly to the sensor. Sage is exploring new techniques for applying machine learning algorithms to data from such intelligent sensors and then building reusable software that can run programs within the embedded computer and transmit the results over the network to central computer servers. Distributed, intelligent sensor networks that can collect and analyze data are an essential tool for scientists seeking to understand the impacts of global urbanization, natural disasters such as flooding and wildfires, and climate change on natural ecosystems and city infrastructure.\\n\\nSage is deploying sensor nodes that support machine learning frameworks in environmental testbeds in California, Colorado, and Kansas and in urban environments in Illinois and Texas. The reusable cyberinfrastructure running on these testbeds will give climate, traffic, and ecosystem scientists new data for building models to study these coupled systems. The software components developed are open source and provide an open architecture to enable scientists from a wide range of fields to build their own intelligent sensor networks.\\n\\nPartners are deploying testbeds in Australia, Japan, UK, and Taiwan, providing scientists with even more data for analysis. The toolkit is also extending the current educational curriculum used in Chicago to inspire young people – with an emphasis on women and minorities, to pursue science, technology, and mathematics careers – by providing a platform for students to explore measurement-based science questions related to the natural and built environments.\\n\\nThe data from sensors and applications is hosted in the cloud to facilitate easy data analysis.\\n\\nWho are the users?\\n\\nThe most common users have included:\\n\\nDomain scientists interested in developing edge AI applications.\\n\\nUsers interested in sensor and application-produced datasets.\\n\\nCyberinfrastructure researchers interested in platform research.\\n\\nDomain scientists interested in adding new sensors and deploying nodes to answer specific science questions.\\n\\nHow do I use the platform?\\n\\nThis depends on your desired interaction interest. The platform consists of edge compute applications which process data (ex. sensor readings, camera images, audio recordings, etc). These edge applications then produce their own data (ex. inferences) and upload the results to a cloud database. This cloud database can be accessed directly and/or additional compute can be performed on the cloud data.\\n\\nThe entry-point into learning about your interaction with the system might be best directed by getting answers (by following the links) to the question(s) you are most interested in.\\n\\nHow do I access sensors? - Want to learn about existing, supported sensors? - Do you have a new sensor that you want to write an edge application for?\\n\\nHow do I run edge apps? - Want to know how to create an edge app? - Want to know how your edge app can get access to edge sensor data? - Want to share your edge app data with other edge applications? - Want to know how to upload data to the cloud?\\n\\nHow do I access and use data? - Want to learn about how data is stored/organized? - Do you have data that is up in the cloud and want to know how to access it?\\n\\nHow do I compute in the cloud? - Want to know how to autonomously react to edge produced data? - Want to know how to trigger an HPC event? - Want to get a text message when your edge application does something cool?\\n\\nHow do I build my own device? - Want to set up your own device for local edge app development? - Want to teach AI to a classroom of students?\\n\\nHow is the cyberinfrastructure architected?\\n\\nIf you are interested in learning more about how the cyberinfrastructure works you can head on over to the Architecture Overview page.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='sidebar_position: 3\\n\\nSubmit your job\\n\\nAre you ready to deploy your plugins to measure the world? We will use edge scheduler to submit a job and demonstrate how you can deploy plugins to field-deployed Waggle nodes.\\n\\n:::caution If you have not created your account, please go to https://portal.sagecontinuum.org and sign in to create a new account with your email. Once signed in, you will be able to create and edit your jobs, but will need a permission to submit jobs to the scheduler. Please contact-us to request for the job submission permission. :::\\n\\nJobs are an instance of a science goal. They detail what needs to be accomplished on Waggle nodes. A science goal may have multiple jobs to fill the missing data to answer scientific questions of the goal. A job describes, - plugins that are registered and built in edge code repository with specification including any plugin arguments, - a list of Waggle nodes on which the plugins will be scheduled and run, - science rules describing a condition-action set that includes when the plugins should be scheduled, - conditions to determine when the job is considered as completed\\n\\nCreating and submitting jobs are an important step for successful science mission using Waggle nodes.\\n\\nCreate a job\\n\\nWe create a job file in YAML format (JSON format is also supported. Please check out details of job attributes.)\\n\\n```bash cat << EOF > myjob.yaml\\n\\nname: myjob plugins: - name: image-sampler pluginSpec: image: registry.sagecontinuum.org/theone/imagesampler:0.3.0 args: - -stream - bottom_camera nodes: W023: scienceRules: - \"schedule(image-sampler): cronjob(\\'image-sampler\\', \\' * * * \\')\" successcriteria: - WallClock(1d) EOF ```\\n\\nIn this example, we want to schedule a plugin named image-sampler to collect an image from the camera named bottom_camera on W023 node. As a result of the job execution, we will get images from the node\\'s camera. The job also specifies that the plugin needs to be scheduled every minute (i.e., * * * * * in crontab expression). The job completes 24 hours after the job started to run on the node.\\n\\n:::info We support human-friendly names for the sensors we host. The \"bottom_camear\" is named based on the orientation the camera is attached to the node. The full list of sensors including cameras for the W023 node can be found here :::\\n\\n:::note We currently do not check job\\'s success criteria. This means that once a job is submitted it is served forever. We will update our system to support different conditions for the success criteria attribute. :::\\n\\nUpload your job to the scheduler\\n\\nsesctl is a command-line tool to manage jobs in the scheduler. You can download the latest version from our Github repository. Please make sure you download the tool supported for your machine. For example, on Linux desktop or laptop you would download linux-amd64 version of the tool. Please see the sesctl document for more details.\\n\\n:::note Once you have contacted us for access permissions, you will need a token provided from the access page. Replace the <<user token>> below with the access token provided on this page. :::\\n\\nYou can set the SES host and user token as an environmental variable to your terminal. Please follow your shell\\'s guidance to set them properly. In Bash shell, bash export SES_HOST=https://es.sagecontinuum.org export SES_USER_TOKEN=<<user token>>\\n\\nLet\\'s ping the scheduler in the cloud, bash sesctl ping\\n\\nYou will get a response \"pong\" from the scheduler, { \"id\": \"Cloud Scheduler (cloudscheduler-sage)\", \"version\": \"0.18.0\" }\\n\\nTo create a job using the job file, bash sesctl create --file-path myjob.yaml\\n\\nThe scheduler will return a job id and the state for the job creation, bash { \"job_id\": \"56\", \"job_name\": \"myjob\", \"state\": \"Created\" }\\n\\nTo verify that we have uploaded the job, bash sesctl stat\\n\\nYou will see the job entry from the response of the command, bash JOB_ID NAME USER STATUS AGE ==================================================================== ... 56 myjob theone Created - ...\\n\\nSubmit the job\\n\\nTo submit the job,\\n\\nbash sesctl submit --job-id 56\\n\\nThe response should indicate that the job state is changed to \"Submitted\", bash { \"job_id\": \"56\", \"state\": \"Submitted\" }\\n\\n:::note You may receive a list of errors from the scheduler if the job fails to be validated. For instance, your account may not have scheduling permission on the node W023. Please consult with us for any error, especially errors related to scheduling permission on nodes in the job. :::\\n\\nCheck status of jobs\\n\\nWe check status of the job we submitted, bash sesctl stat --job-id 56\\n\\nThe tool will print details of the job, ```bash ===== JOB STATUS ===== Job ID: 56 Job Name: myjob Job Owner: Job Status: Submitted Job Starttime: 2022-10-10 02:21:37.373437 +0000 UTC\\n\\n===== SCHEDULING DETAILS ===== Science Goal ID: 45afe963-5b8b-4e15-654c-54e2946f2ddb Total number of nodes 1 ```\\n\\nThe job status can be also shown in job status page.\\n\\nAccess to data\\n\\nA few minutes later, the W023 Waggle node would start collecting images by scheduling the plugin on the node. Collected images are transferred to Beehive for users to download.\\n\\nconsole curl -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-5m\", \"filter\": { \"task\": \"image-sampler\", \"vsn\": \"W023\", \"name\": \"upload\" } } \\'\\n\\nClean it up\\n\\nAs we approach to the end of this tutorial, we need to clean up the job because otherwise it will be served forever. To remove the job from the scheduler, ```bash\\n\\nsince the job is running, we remove the job forcefully\\n\\nsesctl rm --force 56 ```\\n\\nYou should see output that looks like, bash { \"job_id\": \"56\", \"state\": \"Removed\" }\\n\\nMore tutorials using sesctl\\n\\nMore tutorials can be found in our Github repository.\\n\\nCreating job description with advanced science rules for supporting realistic science mission\\n\\nThe science rule used in the tutorial asked the scheduler to schedule the image sampler plugin every minute. For collecting training images from a set of Waggle nodes this makes total sense with the science rule. However, users in Waggle should want more complex behaviors at the node to not only schedule plugins, but enable cloud computation triggered by sending local events to the cloud. The events and triggers can be captured by creating science rules that monitor local sensor measurement on nodes. Please visit the science rules to know more complex science rules that user can create.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-waggle.md'}, page_content='sidebar_position: 7\\n\\nBuilding your own Waggle device\\n\\nAre you a professor that wants to use affordable Waggle devices to teach students interested in AI? Are you someone interested in developing a new edge app using a local development platform? Are you a Waggle user interested in using a new sensor (i.e. a new camera, a bat signal detector, a custom sensor they built)? If you would like to build, design and deploy software that could answer your questions above, then Waggle is the right choice for you.\\n\\nThis tutorial will guide you in preparing your own Waggle device and (optionally) registering it to upload data to a shared development Beehive. This Waggle device is a fully unlocked development platform running the same WES infrastructure that runs in production Waggle edge devices (ex. the Wild Waggle Node). This is an ideal platform for users interested in developing a new edge app and/or experimenting with a new sensor.\\n\\nGetting Started\\n\\nTo get started in boot-strapping your Waggle Edge Computing kit you can follow the instructions for the various supported platforms on the node-platforms GitHub page.\\n\\nWe currently support a limited set of hardware platform because making edge devices into Waggle requires some hardware specific instructions. Check out the platforms we support as of now. More platforms will be added in the future. However, if you would like to add support for other platforms go ahead and submit a pull request to node-platforms.\\n\\nRegistering your Waggle device\\n\\nDuring the bootstrapping process you will have the option to register your device within the web portal here. It is highly recommended to register your device, as this enables all the core WES tools to be automatically downloaded, enabling the edge app development and run-time environment. Additionally, this enables your edge apps to publish data to the development Beehive, accessible to cloud-based analysis tools and workflow frameworks.\\n\\nTo register your device, use the dev devices form. Enter your device ID (which you will obtain through the hardware boot-strapping process) then click \"Get Keys\" button. A \"registration zip\" file will be generated and available for download. Then follow the instructions for your device to load the registration keys.\\n\\nYou may register as many times as you want. But note that each registration key has a short expiration time and should be used shortly after generation.\\n\\nNow you are ready to develop your edge apps and/or introduce new sensors to the Waggle platform. Head over to the overview to find the instructions you need for development.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='sidebar_position: 4\\n\\nAccess and use data\\n\\nRaw sensor data is collected by edge code. This edge code can either talk to sensor hardware directly or may obtain data from an abstraction layer (not show in image above). Edge code may forward unprocessed sensor data, do light processing to convert raw sensor values into final data products, or may use CPU/GPU-intensive workloads (e.g. AI application) to extract information from data-intensive sensors such as cameras, microphone or LIDAR.\\n\\nSensor data from nodes that comes in numerical or textual form (e.g. temperature) is stored natively in our time series database. Sensor data in form of large files (images, audio, movies..) is stored in the Waggle object store, but is referenced in the time series data (thus the dashed arrow in the figure above). Thus, the primary way to find all data (sensor and large files) is via the Waggle sensor query API described below.\\n\\nCurrently the Waggle sensor database contains data such as:\\n\\nRelative humidity, barometric pressure, ambient temperature and gas (VOC) BME680.\\n\\nRainfall measurements (Hydreon RG-15).\\n\\nAI-based cloud coverage estimation from camera images.\\n\\nAI-based object counts from camera images.\\n\\nSystem data such as uptime, cpu and memory.\\n\\nData can be accessed in realtime via our data API or in bulk via data bundles.\\n\\nData API\\n\\nWaggle provides a data API for immediate and flexible access to sensor data via search over time and metadata tags. It is primarily intended to support exploratory and near real time use cases.\\n\\nDue to the wide variety of possible queries, we do not attempt to provide DOIs for results from the data API. Instead, we leave it up to users to organize and curate datasets for their own applications. Long term, curated data is instead provided via data bundles.\\n\\nThere are two recommended approaches to working with the Data API:\\n\\nUsing the Python Sage Data Client.\\n\\nUsing the HTTP API.\\n\\nEach is appropriate for different use cases and integrations, but generally the following rule applies:\\n\\nIf you just want to get data into a Pandas dataframe for analysis and plotting, use the sage-data-client, otherwise use the HTTP API.\\n\\nUsing Sage data client\\n\\nThe Sage data client is a Python library which streamlines querying the data API and getting the results into a Pandas dataframe. For details on installation and usage, please see the Python package.\\n\\nUsing HTTP API\\n\\nThis example shows how to retrieve data the latest data from a specific sensor (you can adjust the start field if you do not get any recent data):\\n\\n```console curl -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-10s\", \"filter\": { \"sensor\": \"bme680\" } } \\'\\n\\nExample results:json {\"timestamp\":\"2021-08-09T19:26:03.880781217Z\",\"name\":\"iio.in_humidityrelative_input\",\"value\":70.905,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} {\"timestamp\":\"2021-08-09T19:26:03.878659392Z\",\"name\":\"iio.in_pressure_input\",\"value\":975.78,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} {\"timestamp\":\"2021-08-09T19:26:03.872652127Z\",\"name\":\"iio.in_resistance_input\",\"value\":93952,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} {\"timestamp\":\"2021-08-09T19:26:03.874998057Z\",\"name\":\"iio.in_temp_input\",\"value\":27330,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} ```\\n\\n:::tip More details of using the data API and the data model can be found here and here. :::\\n\\nData bundles\\n\\nData bundles provide sensor data and associated metadata in a single, large, downloadable file. Soon, each Data Bundle available for download will have a DOI that can be used for publication citations.\\n\\nData Bundles are compiled nightly and may be downloaded in this archive.\\n\\nAccessing file uploads\\n\\nUser applications can upload files for AI training purposes. These files stored in an S3 bucket hosted by the Open Storage Network.\\n\\nTo find these files use the filter \"name\":\"upload\" and specify additional filters to limit search results, for example:\\n\\nconsole curl -s -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\'{ \"start\": \"2021-09-10T12:51:36.246454082Z\", \"end\":\"2021-09-10T13:51:36.246454082Z\", \"filter\": { \"name\":\"upload\", \"plugin\":\"imagesampler-left:0.2.3\" } }\\'\\n\\nOutput: json {\"timestamp\":\"2021-09-10T13:19:27.237651354Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d05a0a4/1631279967237651354-2021-09-10T13:19:26+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d05a0a4\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}} {\"timestamp\":\"2021-09-10T13:50:32.29028603Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bc3d/1631281832290286030-2021-09-10T13:50:32+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bc3d\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}} {\"timestamp\":\"2021-09-10T12:52:59.782262376Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdc2/1631278379782262376-2021-09-10T12:52:59+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}} {\"timestamp\":\"2021-09-10T13:49:49.084350086Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdd2/1631281789084350086-2021-09-10T13:49:48+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bdd2\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}}\\n\\nFor a quick way to only extract the urls from the json objects above, a tool like jq can be used:\\n\\nconsole curl -s -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\'{ \"start\": \"2021-09-10T12:51:36.246454082Z\", \"end\":\"2021-09-10T13:51:36.246454082Z\", \"filter\": { \"name\":\"upload\", \"plugin\":\"imagesampler-left:0.2.3\" } }\\' | jq -r .value > urls.txt\\n\\nThe resulting file urls.txt will look like this: text https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d05a0a4/1631279967237651354-2021-09-10T13:19:26+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bc3d/1631281832290286030-2021-09-10T13:50:32+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdc2/1631278379782262376-2021-09-10T12:52:59+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdd2/1631281789084350086-2021-09-10T13:49:48+0000.jpg\\n\\nTo download the files: console wget -i urls.txt\\n\\nIf many files are downloaded, it is better to preserve the directory tree structure to prevent filename collision: console wget -r -i urls.txt\\n\\nProtected data\\n\\nWhile most Waggle data is open and public - some types of data, such as raw images and audio from sensitive locations, may require additional steps:\\n\\nYou will need a Sage account.\\n\\nYou will need to sign our Data Use Agreement for access.\\n\\nYou will need to provide authentication to tools you are using to download files. (ex. wget, curl)\\n\\nAttempting to download protected files without meeting these criteria will yield a 401 Unauthorized response.\\n\\nIf you\\'ve identified protected data you are interested in, please contact us so we can help get you access.\\n\\nIn the case of protected files, you\\'ll need to provide authentication to your tool of choice. These will be your portal username and access token which can be found in the Access Credentials section of the site.\\n\\nThese can be provided to tools like wget and curl as follows:\\n\\n```console\\n\\nexample using wget\\n\\nwget --user=\\n\\nexample using curl\\n\\ncurl -u'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-an-account.md'}, page_content='sidebar_label: Create an account sidebar_position: 1\\n\\nOverview\\n\\nWhile some Sage features are open for public use, you\\'ll need an approved account to perform tasks such as:\\n\\nGet access to protected data.\\n\\nPublish apps to the ECR.\\n\\nSchedule app on nodes.\\n\\nIn this document, we\\'ll walk though creating an account.\\n\\nCreating an account\\n\\nClick on the Portal button in the upper right corner.\\n\\nClick on the Sign In button in the upper right corner.\\n\\nThis will take you to the Globus login page where you\\'ll need to provide your organization credentials. If you do not see your organization, please see the \"Didn\\'t find your organization?\" note at the bottom of the Globus login page.\\n\\nFinally, if this is your first time signing in, you\\'ll need to choose a username which will complete your account creation.\\n\\nAt this point, our team will need to review and approve your account before you\\'ll have permission to perform certain tasks. If you your account is not approved within 72 hours or you have special requirements, please Contact us so that we can help perform any account configuration.\\n\\nNext steps\\n\\nOnce your account is approved, you will have scheduling access and protected data browsing in the portal for nodes we\\'ve assigned to your account.\\n\\nFor CLI tools and SSH access to nodes, please go to Portal → Your Account → Access Creds and follow the Update SSH Public Keys and Finish Setup for Node Access instructions.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content=\"sidebar_position: 5\\n\\nAccess Waggle sensors\\n\\nA Waggle sensor is an entity that produces measurements of a phenomenon and that helps users analyze what is happening in the environment. There are sensors already hosted by Waggle and also sensors that are being integrated into Waggle as a user-hosted sensor. A sensor does not necessarily mean a physical device, but can be a program producing measurements from data -- we call it software-defined sensor. Once those sensors become available in Waggle nodes edge applications running inside the nodes can pull measurements from the sensors to process them.\\n\\nIn general, Waggle sensors are desinged to be accessible from any edge applications running on the Waggle node that hosts the sensors, but can be limited their access to groups and personnel. For example, a pan-tilt-zoom featured camera may be only accessed from authorized applications in order to prevent other applications from operating the camera. Ideally, Waggle sensors can form and support the Waggle ecosystem where sensor measurements are integrated and used by edge applications for higher level computation and complex decision making.\\n\\nWaggle physical sensors\\n\\nThe Waggle node is designed to accommodate sensors commonly used to support environmental science, but not limited to host other sensors. The currently supported sensors are,\\n\\nNOTE: not all Waggle nodes have the same set of sensors, and the sensor configuration depends on what to capture from the environment where the node is deployed\\n\\nBME680 temperature, humidity, pressure, and gas preview RG-15 rainfall preview ETS ML1-WS 20-16 kHz microphone recording sound XNV-8080R 5 MP camera with 92.1 degree horizontal and 67.2 degree vertical angle view XNV-8082R 6 MP camera with 114 degree horizontal and 62 degree vertical angle view XNF-8010RV 6 MP fisheye camera with 192 degree horizontal and vertical angle view XNV-8081Z 5 MP digital pan-tilt-rotate-zoom camera\\n\\nAny collaborators and user communities can bring up their sensors to Waggle node. The node can easily host sensor devices that support serial interface as well as network interface (e.g., http, rtsp, etc). Other currently supported user sensors include:\\n\\nSoftware-defined Radio: detecting raindrops and snow flakes\\n\\nRadiation detector: radiation detector\\n\\nLIDAR: distance of nearby objects\\n\\nMobotix: infrared camera\\n\\n[view more...]\\n\\nWaggle software-defined sensors\\n\\nSoftware-defined sensors are limitless as edge applications define them. You can start building your edge application that publishes outputs using PyWaggle's basic example that can become a software-defined sensor. Later, such outputs can be consumed by other edge applications to produce higher level information about the measurements. A few example of Waggle software-defined sensors are,\\n\\nObject Counter: env.count.OBJECT counts objects from an image, where OBJECT is the object name that is recognized\\n\\nCloud Coverage Estimator: env.coverage.cloud provides a percentage of cloud covered in an image\\n\\nAccess to Waggle sensors\\n\\nWaggle sensors are integrated into Waggle using the PyWaggle library. PyWaggle utilizes AMQP, the message publishing and subscribing mechanism, to support exchanging sensor measurements between device plugins and edge applications. An edge application can subscribe and process those measurements using PyWaggle's subscriber. The application then produces its output and publishes it as a measurement back to the system using PyWaggle publisher.\\n\\nPyWaggle often provides edge applications direct access to physical sensors. For sensors that support realtime protocols like RTSP and RTP and others, PyWaggle exposes those protocols to edge applications, and it is up to the applications to process data using given protocol. For example, RTSP protocol can be handled by OpenCV's VideoCapture class inside an application. If any physical sensor device that requires a special interfacing to the device, an edge application that supports the interfacing need to run in order to publish sensor measurements to the system, and later those measurements are used by other edge applications.\\n\\nExample: sampling images from camera\\n\\nIt is often important to sample images from cameras in the field to create initial dataset for a machine learning algorithm. The example describes how to access to a video stream from a camera sensor using PyWaggle.\\n\\nBring your own sensor to Waggle\\n\\nUsers may need to develop their own device plugin to expose the sensor to the system, or to publish measurement data from the sensor to the cloud. Unlike an edge application or software-defined sensors, device plugins communicating with a physical sensor may need special access, e.g. serial port, in order to talk to the sensor attached to Waggle node. Such device plugin may need to be verified by the Waggle team. Visit the Building your own Waggle device page for the guide to set up your Waggle device.\\n\\nTo integrate your sensor device into Waggle, head over to the Contact Us page\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/cloud-compute.md'}, page_content='sidebar_position: 6\\n\\nCloud compute & HPC on edge data\\n\\nWaggle provides a number of interfaces which other computing and HPC systems can build on top of. In this section, we explore some of the most common applications of Waggle.\\n\\nTriggering on data from the edge\\n\\nA common application is monitoring data from the edge and triggering actions when values exceed a threshold or an unusual event is detected.\\n\\nAs a getting started example, we demonstrate an outline of how this can be done in Waggle using the Sage data client.\\n\\n```python import sage_data_client import time\\n\\nwhile True: # query pressure data in recent 10 minute window df = sage_data_client.query( start=\"-10m\", filter={ \"name\": \"env.pressure\", \"sensor\": \"bme680\", } )\\n\\n# compute stddev for nodes\\' pressure data in window\\nstd = df.groupby(\"meta.vsn\").value.std()\\n\\n# find all pressure events exceeding an example threshold\\nevents = std[std > 8.0]\\n\\n# \"post\" vsn to alert system\\nfor vsn in events.index:\\n    print(f\"post {vsn} to alert system\")\\n\\ntime.sleep(60)\\n\\n```\\n\\nThe above code queries a 10 minute window of atmospheric pressure data every minute and \"posts\" alerts for nodes exceeding a predefined standard deviation threshold.\\n\\nThis example and more can be found in the Sage data client examples directory.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/4-publishing-to-ecr.md'}, page_content='sidebar_position: 4\\n\\nPart 4: Publishing to ECR\\n\\nNow that we\\'ve finished preparing our code and testing it, we\\'re almost ready to publish it to the Edge Code Repository!\\n\\nPreparing our app\\n\\nBefore publishing an app to the Edge Code Repository, we need to add a few packaging items to it.\\n\\nFirst, update the homepage in your sage.yaml to point to your app-tutorial Github repo and verify that it matches the following:\\n\\nyaml name: \"app-tutorial\" version: \"0.1.0\" description: \"My really amazing app!\" keywords: \"\" authors: \"Your name\" collaborators: \"\" funding: \"\" license: \"\" homepage: \"https://github.com/username/app-tutorial\" source: architectures: - \"linux/amd64\" - \"linux/arm64\"\\n\\nNext, create an ecr-meta directory in your repo and populate it with the following text and media:\\n\\necr-science-description.md - Markdown with in depth description of the science being done here (1 page of text).\\n\\necr-icon.jpg - An icon for the project/work 512x512px.\\n\\necr-science-image.jpg - A science image for the project with a minimum size of 1920x1080px.\\n\\nOnce we\\'ve commited and pushed those files to your repo, we\\'re ready to publish our app!\\n\\nPublishing our app\\n\\nPlease visit the Edge Code Repository and complete the following steps:\\n\\nGo to \"Sign In\" and follow the instructions.\\n\\nGo to \"My Apps\".\\n\\nGo to \"Create app\" and follow the instructions.\\n\\nIf everything is successful, your plugin will appeared and be marked as \"Built\".\\n\\nConclusion\\n\\nCongratulation! You\\'ve successfully written, tested and published an app to ECR!\\n\\nWe encourage you to check out other apps in the ECR and explore additional functionality provided by pywaggle.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/1-intro-to-edge-apps.md'}, page_content=\"sidebar_position: 1\\n\\nPart 1: Intro to edge apps\\n\\nWhat are edge apps?\\n\\nEdge apps are programs which read data (ex. sensors, audio, video), process it and then publish information derived from that data.\\n\\nA basic example of an app is one which reads and publishes a value from a sensor every minute. A more complex example could publish the number of birds in a scene using a deep learning model.\\n\\nEdge apps are composed of code, dependencies and models which are packaged so they can be scheduled on Waggle nodes. At a high level, the typical app lifecycle is:\\n\\nExploring existing edge apps\\n\\nOne of the major goals of Waggle is to provide the science community with a diverse catalog of edge apps to enable the sharing of new research. This catalog is maintained as part of the Edge Code Repository where you can find more background information and links to their source repos.\\n\\nWe encourage users to explore the ECR to get familiar with existing apps as well a references if you develop your own edge app.\\n\\nNext steps\\n\\nIf this sounds exciting and you'd like to write you own edge app, please continue to part 2!\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='sidebar_position: 2\\n\\nPart 2: Creating an edge app\\n\\nIn part 1, we showed an overview of what edge apps are and how they fit into the Waggle ecosystem. Now, we\\'ll dive right in and start writing our very own edge app!\\n\\nPrerequisites\\n\\nFor this part of the tutorial, we\\'ll assume you are developing directly on a laptop or machine with a camera or webcam available. You should have some basic development experience in Python and with git for version control.\\n\\nDevelopment workflow\\n\\nIn the next few parts of this tutorial, we\\'ll deep dive into the following app development workflow:\\n\\nFirst, data and model selection is where you scope the problem and identify a new or existing model for your application. This typically happens outside of our ecosystem.\\n\\nSecond, develop and test is where you begin to integrate your initial code with our ecosystem, test and finally build your application in ECR.\\n\\nFinally, deploy and iterate is where you schedule your application for deployment and look at the results.\\n\\nA driving example\\n\\nIn order to illustrate progress through each of these stages, we\\'ll start with a concrete code example and iterate on it over the next few sections.\\n\\nIn practice, lots of work goes into the data and model selection step. For now, we\\'ll assume that groundwork has already been done and we\\'ve settled on the following code snippit to start with.\\n\\n```python import numpy as np import cv2\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): # read example image from file image = cv2.imread(\"example.jpg\")\\n\\n# compute mean color\\nmean_color = compute_mean_color(image)\\n\\n# print mean color\\nprint(mean_color)\\n\\nif name == \"main\": main() ```\\n\\nBootstrapping our app from a template\\n\\nWe\\'ll start our by using a cookiecutter template to bootstrap our app.\\n\\nFirst, ensure the latest cookiecutter is installed:\\n\\nsh pip3 install --upgrade cookiecutter\\n\\nNow, run the following command:\\n\\nsh cookiecutter gh:waggle-sensor/cookiecutter-sage-app\\n\\nYou should be prompted to fill in the following fields:\\n\\ntxt [1/5] name (my-amazing-app-name): my-amazing-app-name [2/5] description (My really amazing app!): [3/5] author (My name): [4/5] version (0.1.0): [5/5] Select kind 1 - vision 2 - usbserial_sensor 3 - minimal 4 - tutorial <<< use 4 for tutorial Choose from [1/2/3/4] (1): 4\\n\\nIf this succeeds, a new app-tutorial directory will be created with the following files:\\n\\nName Description main.py Main code requirements.txt Code dependencies Dockerfile App build instructions sage.yaml App metadata\\n\\nInstalling the dependencies\\n\\nThe first step in preparing our example for the edge is to install pywaggle in our local development environment.\\n\\npywaggle is our Python SDK which provides edge apps access to devices (ex. cameras and microphones) and messaging within a node.\\n\\nFor this tutorial, we\\'ll install the latest version of the requirements included in the template:\\n\\nsh pip3 install --upgrade --requirement requirements.txt\\n\\nAccessing a camera\\n\\nNow that we have pywaggle, the first change we\\'ll make is to use a camera as input rather than a static image file. We\\'ll use the following shapshot() function to take an RGB snapshot from the camera.\\n\\n```python import numpy as np\\n\\nfrom waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n# compute mean color\\nmean_color = compute_mean_color(snapshot.data)\\n\\n# print mean color\\nprint(mean_color)\\n\\nif name == \"main\": main() ```\\n\\nNow, we can try this out by running:\\n\\nsh python3 main.py\\n\\nYou should see output like:\\n\\ntxt [51.43575738 51.83611871 54.64226671]\\n\\nYou\\'re exact numbers may differ as this is computed using your default camera.\\n\\nPublishing results\\n\\nThe next change we\\'ll make is to publish our data to the Beehive Data Repository instead of just print it. This will allow it to be sent to a Beehive once it\\'s scheduled on a node.\\n\\n```python import numpy as np\\n\\nfrom waggle.plugin import Plugin from waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): with Plugin() as plugin: # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n    # compute mean color\\n    mean_color = compute_mean_color(snapshot.data)\\n\\n    # publish mean color\\n    plugin.publish(\"color.mean.r\", mean_color[0], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.g\", mean_color[1], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.b\", mean_color[2], timestamp=snapshot.timestamp)\\n\\nif name == \"main\": main() ```\\n\\nNow, we\\'ll run this using:\\n\\nsh python3 main.py\\n\\nYou may notice something... there\\'s no output! Usually, published data is sent to a beehive where it can be viewed later. However, because we\\'re developing locally and have not configured a beehive, the data isn\\'t going anywhere. In the next section, we\\'ll see how we can tap into our published data.\\n\\nViewing run logs\\n\\nIn order to make developing and debugging apps easier, pywaggle can write out a log directory as follows:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nThis will create a new directory named test-run and will contain a file named data.ndjson which contains something like:\\n\\njson {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":32.67932074652778} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":19.087491319444446} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":10.337491319444444}\\n\\nIf we run python3 main.py again, then we\\'ll see new data appended to that file:\\n\\njson {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":32.67932074652778} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":19.087491319444446} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":10.337491319444444} {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":30.90709743923611} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":16.61302517361111} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":8.565154079861111}\\n\\nThis provides a convenient way to understand the behavior of an app, particularly one with a more complicated flow.\\n\\nUploading a snapshot\\n\\nFinally, the last change we\\'ll make is to upload our snapshots after publishing the mean color.\\n\\nWe\\'ll upload every snapshot for demonstration purposes, but you wouldn\\'t want to do this in a real app. Instead, you\\'d typically upload in response to detecting an event such as an anomalous object or loud noise.\\n\\n```python import numpy as np\\n\\nfrom waggle.plugin import Plugin from waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): with Plugin() as plugin: # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n    # compute mean color\\n    mean_color = compute_mean_color(snapshot.data)\\n\\n    # publish mean color\\n    plugin.publish(\"color.mean.r\", mean_color[0], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.g\", mean_color[1], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.b\", mean_color[2], timestamp=snapshot.timestamp)\\n\\n    # save and upload image\\n    snapshot.save(\"snapshot.jpg\")\\n    plugin.upload_file(\"snapshot.jpg\", timestamp=snapshot.timestamp)\\n\\nif name == \"main\": main() ```\\n\\nLet\\'s run our app again using:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nIf you take a look in the test-run/uploads directory, you should now see an image.\\n\\nUploads are added to the run log directory using the format nstimestamp-filename.\\n\\nYou should also see a corresponding item in the data.ndjson file.\\n\\njson {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":29.601871744791666} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":16.004838324652777} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":8.217218967013888} {\"meta\":{\"filename\":\"snapshot.jpg\"},\"name\":\"upload\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":\"/Users/sean/dev/pw-example/test-run/uploads/1661279974985679000-snapshot.jpg\"}\\n\\nTools for analyzing run logs (Optional)\\n\\nIf you find yourself working with run logs frequently, we recommend the Sage data client which provides convenient functionality for loading and doing analysis on the data.ndjson file. See the \"Load results from file\" example for more info.\\n\\nNext steps\\n\\nCongratulations! You\\'ve finished preparing our example code for the edge!\\n\\nIn the part 3, we\\'ll look at how we can build and test our app on a real node!'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='sidebar_position: 3\\n\\nPart 3: Testing an edge app\\n\\nIn the previous part, we took a code snippit and iterated on it until it was ready for the edge. By the end, we had basic camera access and publishing working!\\n\\nNow, we\\'re ready to start testing it on a development node and describing our build steps.\\n\\nAccessing development nodes\\n\\nThe first thing we need to do is get access to a development node. Unfortunately, we are still developing the infrastructure to open this up to general users.\\n\\nFor now, please contact us to request access to a development node and we\\'ll work with you to setup access.\\n\\nCreating a repo for our app\\n\\nBefore connecting to our node, let\\'s take a moment to organize our code into a repo we will later use on the node.\\n\\nGo ahead and create a new Github repo named app-tutorial and commit the files from previous part.\\n\\nBuilding our app\\n\\nNow that we\\'ve setup node access, ssh to the node then clone and cd into your app-tutorial repo:\\n\\nsh git clone https://github.com/username/app-tutorial cd app-tutorial\\n\\nThe first thing we\\'ll do is build our app on the node:\\n\\nsh sudo pluginctl build .\\n\\nThis may take some time, but once it completes you should see something like:\\n\\n```txt Sending build context to Docker daemon 59.39kB Step 1/6 : FROM waggle/plugin-base:1.1.1-base ... Step 2/6 : WORKDIR /app ... Step 3/6 : COPY requirements.txt . ... Step 4/6 : RUN pip3 install --no-cache-dir -r requirements.txt ... Step 5/6 : COPY . . ... Step 6/6 : ENTRYPOINT [\"python3\", \"main.py\"] ... b38bc0a208d0: Pushed 1101ffccd70a: Pushed latest: digest: sha256:7bee2a62fbcc9913f1c53bbdab79e973e70947618ffe4db90cae6a8f0ff6c8d7 size: 2407 Successfully built plugin\\n\\n10.31.81.1:5000/local/app-tutorial ```\\n\\nOnce we see Successfully built plugin, we can continue to running our app.\\n\\nRunning our app\\n\\nWhen we successfully built our app, the last line of output was 10.31.81.1:5000/local/app-tutorial. We will now use this reference to run our app.\\n\\nsh sudo pluginctl run --name app-tutorial 10.31.81.1:5000/local/app-tutorial\\n\\nWhen you run this, you\\'ll see that there\\'s a bug in the code:\\n\\n```sh Launched the plugin app-tutorial-1659971085 successfully INFO: 2022/08/08 15:04:45 run.go:63: Plugin is in \"Pending\" state. Waiting...\\n\\n[ WARN:0@0.032] global /io/opencv/modules/videoio/src/cap_v4l.cpp (902) open VIDEOIO(V4L2:/dev/video0): can\\'t open camera by index Traceback (most recent call last): File \"main.py\", line 32, in\\n\\nThis was caused by the fact that most nodes have multiple cameras, so we need to be more specific about which camera to use.\\n\\nTo address this, we\\'ll change the following line in main.py from:\\n\\npython with Camera() as camera:\\n\\nto:\\n\\npython with Camera(\"left\") as camera:\\n\\nThe specific camera name will depend on your specific node. If you are having problems accessing a camera, please contact us for more details.\\n\\nAfter rebuilding and running this again, the plugin should run and exit cleanly:\\n\\n```txt Launched the plugin app-tutorial-1659971085 successfully INFO: 2022/08/08 15:04:45 run.go:63: Plugin is in \"Pending\" state. Waiting...\\n\\nshould exit cleanly with no output\\n\\n```\\n\\nNow that we know this works, please commit and push the change to the repo from your machine.\\n\\nFinally, if you are rebuilding and running code frequently, you can combine the build and run into a single step as follows:\\n\\nsh sudo pluginctl run --name app-tutorial $(sudo pluginctl build .)\\n\\nViewing our output\\n\\nWe\\'ll close this part, by looking at the data we just published. To do this, we\\'ll query the Beehive Data Repository:\\n\\nsh curl -s \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-5m\", \"filter\": { \"task\": \"app-tutorial\" } }\\'\\n\\nYou should see some results like:\\n\\njson {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.b\",\"value\":133.61671793619792,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}} {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.g\",\"value\":136.46639404296874,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}} {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.r\",\"value\":134.48696818033855,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}} {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/Pluginctl/sage-app-tutorial-app-tutorial/000048b02d15bdc2/1659971088820981933-snapshot.jpg\",\"meta\":{\"filename\":\"snapshot.jpg\",\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}}\\n\\nThese are exactly the mean color values we computed and published!\\n\\nThis is intended to be a quick preview of how to access data to help get you started. If you are interested, we cover this topic in much depth here.\\n\\nNext steps\\n\\nNow we\\'ve been able to build, run and even fix a bug in our code! In part 4, we\\'ll see how to publish a first release of our code to the Edge Code Repository!'),\n",
       " Document(metadata={'source': 'sage-website/docs/events/past/hackathon-2023.md'}, page_content='sidebar_label: August 2023 Hackathon\\n\\nAugust 2023 Sage Hackathon\\n\\nWe are hosting a Hackathon for Sage in late August ~~with preliminary dates August 30-31~~!\\n\\nUpdate: Based on user feedback, we will run afternoon sessions on both August 30 and 31 starting at 1pm CST. We will email out an agenda and invite to Slack to participants who have signed up a few days before the event.\\n\\nThe goal of the Hackathon is to set aside a few hours dedicated to working through user applications, code and science examples in depth. If this interests you, please fill out the signup form as soon as possible!\\n\\nPrior to the Hackathon, we request that you do a few things: 1. Read the Sage Overview in the Sage docs to understand what Sage is and how it might connect to your work. 2. Start assembling the people on your team likely to participate. 3. Ensure they have accounts in the Sage Portal. 4. Start working through the Edge apps tutorial in the Sage docs.\\n\\nDuring the Hackathon, we will review how to use the portal and parts of the Edge app tutorial. However, already having done some preliminary work will allow more time for our team to provide support for your unique application.'),\n",
       " Document(metadata={'source': 'sage-website/news/2023-10-04-emiquon-preserve.md'}, page_content='title: Tower installation with Sage nodes in Emiquon Preserve hide_reading_time: true tags: [wildlife, environmental science, solar-powered]\\n\\nIn collaboration with The Nature Conservancy, two Sage nodes were deployed at the Emiquon Preserve to monitor wildlife and understand conditions of the local environment. The Sage nodes W020 and W01B are equipped with both stationary and pan-tilt-zoom-able cameras and meteorological sensors including dust and weather sensors. The Sage node W01B is the first field-deployed node powered by renewable energy from eight solar panels.'),\n",
       " Document(metadata={'source': 'sage-website/news/2023-10-25-team-hawaii.md'}, page_content='title: Hawaii for Scientists hide_reading_time: true authors: pbeckman tags: [interns, hawaii, science] slug: hawaii-intro\\n\\nThe Sage project combines advanced cyberinfrastructure, artificial intelligence (AI), and sensors to create intelligent, autonomous, new instruments to help us explore and understand climate change, natural hazards, urban landscapes, and the biosphere. The Sage team has deployed more than 100 nodes across the United States – but this installation was special…. Sponsored by NAISE, our goal was more than simply connecting an NVIDIA Jetson GPU, infrared camera, and anemometer to build a Wild Sage Node and create a secure infrastructure for scientific discovery – we were also intent on experiential learning -- from carrying scientific equipment (watch video) through the dense forest of Volcanos National Park to learning about the restoration of a 400 year old native pond. Together with a team of students from Northwestern University, University of Chicago, and University of Illinois, we set out to learn and contribute – Science! Hawaii is beautiful, but Science is our goal. Enjoy the reports that will show up here.'),\n",
       " Document(metadata={'source': 'sage-website/news/2020-07-28-world-watchers.md'}, page_content='hide_reading_time: true tags: [in the news, NAISE]\\n\\nWorld Watchers\\n\\nLed by investigators at the Northwestern-Argonne Institute of Science and Engineering, researchers are designing a smart new way to monitor our surroundings, from natural ecosystems to urban infrastructure. Read more at NAISE Research News...'),\n",
       " Document(metadata={'source': 'sage-website/news/2024-01-10-sage-testbed.md'}, page_content='title: Introducing the Sage Testbed date: 2024-01-10 authors: sshahkarami tags: [Sage, Nodes, Testbed, Sensors, Instruments, Development, Community]\\n\\nThe Sage team is thrilled to kick off 2024 by introducing our new Sage Testbed!\\n\\nFor Sage users, one of the major challenges is bridging the gap between local development and real production deployment.\\n\\nWhile local development is fantastic for prototyping ideas quickly, it often lacks the nuance and intricacy of real hardware, particularly when it comes to sensors and instruments. Further, specific technical hurdles with tools like Docker or Rancher Desktop prevent Mac or Windows users from even integrating certain sensors with their own machine for development and testing.\\n\\nIn response to this challenge, the team has dedicated a focused effort on building out a comprehensive testbed. This testbed, funded by the NSF Sage project, consists of 16 Wild Sage Nodes and 14 Sage Blades which all have access to a range of sensors such as PTZ and thermal cameras, with the explicit intent of being made widely available to the community for development access.\\n\\nThe Northwestern University Sage Testebed is hosted at a testing site on the Argonne National Laboratory campus. Because it is easily accessible and maintained, we can better support users interested trying more cutting-edge or low level experiments on devices.\\n\\nDoes this sound interesting to you? If so, visit the Access Credentials section of the Sage Portal and request dev access to the Sage Testbed to get started.'),\n",
       " Document(metadata={'source': 'sage-website/news/2023-10-30-joann-blog-hawaii.md'}, page_content='title: Joann Hawaii Blog Post hide_reading_time: true authors: jlenart tags: [Hawaii, Blog Post, social science, environment]\\n\\nJoann Hawaii Blog Post\\n\\nMy name is Joann Lenart and I am a current senior at Northwestern University. As a social scientist-- majoring in Political Science and Legal Studies while also conducting research regarding environmental justice within Indigenous groups-- I joined a research trip to Hawaii that was primarily focused on building sensors and using codes and AI to track everything from birds sounds to heat sensing. It seems out of my wheel-house but it was also an eye-opening experience to see how we can bridge the social sciences with computer science.\\n\\nWhile in Hawaii, I observed some societal patterns and themes that were interesting to me. One of the first examples was at the airport. Directions and announcements were first stated in Native Hawaiian, then in English. Having done research with Indigenous groups in the past, this small detail has a broader impact regarding their status in the state.\\n\\nAdditionally, the environmental laws passed in Hawaii are noticeable and prevalent in a positive way. To enter national parks, shoes need to be scrubbed to prevent invasives. Plastic bags are banned and the single-use cups and utensils are all eco friendly. Certain chemicals are banned from sunscreens. These laws work here but they are not seen on the mainland. It appears the key for these environmental laws to pass and function is the need from citizens to also want to help the environment. Helping the environment is a collective effort and there must be motivation from both parties to combat the effects of climate change.\\n\\nOn Day 6 of the trip, the research team went to a water treatment plant to survey the area since a new Sage Node will eventually be installed there on a weather tower. The tower overlooked the town of Lahaina which unfortunately was devastated by the wildfires earlier in August. This node will be used to detect smoke and track temperatures to see which area is most vulnerable to wildfires to try to prevent another disaster from occurring. The community members are still recovering and are still terrified months after this devastation. Once the sage node is up, the team will work with the local community by creating a dashboard with accessible information such as if the air quality is healthy. Additionally, after discussing with Chris, a hydrologist, about this data, there is potential to give this data to the government and council in charge to decide what policies to pass and to use this data in court cases to pass climate legislation and advance environmental justice. It is about being transparent and having the local community involved.\\n\\nDespite not being a computer scientist, there are many areas where the social sciences can be bridged with the computer sciences. It is about how you use the data collected with the community groups most impacted. On paper I may have been an outlier with my background, but during discussions with many other scientists and researchers, it did not feel as if there was a gap between our research. Many computer scientists want to bring in social sciences to their work and be more than just programmers. There is work to be done between these fields but it will bring in many benefits later on.\\n\\nThis was an amazing opportunity and I am very grateful to have gone :)'),\n",
       " Document(metadata={'source': 'sage-website/news/2020-08-07-lightning-the-way-with-software-defined-radios.md'}, page_content='title: Lighting the Way with Software-Defined Radios hide_reading_time: true author: Scott Collis author_url: https://www.anl.gov/profile/scott-m-collis author_image_url: https://www.anl.gov/sites/www/files/styles/profile_teaser_square_350px/public/Collis%20Scott%2032749D03_0.jpg?h=d69be872&itok=6k63Thxu tags: [in the news, lightning detection]\\n\\nLighting the Way with Software-Defined Radios\\n\\nScott Collis and Jim Olds have been exploring how software-defined radios could be used to improved lightning detection. They have been tinkering at home with several technologies and their hackery was picked up by a few websites:\\n\\nAnalyzing Lightning Discharges with an RTL-SDR and the Sage Network\\n\\nLightning Analysis With Your SDR'),\n",
       " Document(metadata={'source': 'sage-website/news/2023-10-31-Hawaiian-Community.md'}, page_content=\"title: Serving the Hawaiian Community with Sage Nodes date: 2023-10-31 authors: acabrera tags: [Hawaii, Node deployment, Students]\\n\\nHello, my name is Aldo. I study computer science at the University of Chicago, and I've worked with Argonne National Laboratory since June 2021. My research involves wireless sensors and integrating the Sage infrastructure with low-cost nodes.\\n\\nThere aren't many employers that fly out their interns to tropical islands. But that's why Argonne National Lab is so incredible! This last week, I learned what it means to be a computer scientist that helps people. My biggest dilemma with applying to graduate school has been figuring out how I'd positively impact people's lives with my research. Argonne allowed me to see exactly that on this trip. I saw how lab work leads to applicable instrumentation, which allows different scientists to answer their research questions to help people. And it was incredible to see people so passionate about their work.\\n\\nMeeting Derek Esibill and his son Kai in O'ahu taught me that science is beautiful. It was clear from the beginning they both care deeply about Hawaiian culture and its people. Derek's research aims to restore the traditional ways of aquaculture to support the growing needs of Hawaii. He uses sensors to track the flow of fish and to study the composition of the pond water. We visited the pond to explore how Sage can introduce wireless sensors to facilitate the process of data gathering. I saw how my research on wireless sensors could help other scientists to answer questions relating to food sustainability. To me, that means a lot because it revealed to me how remote lab research translates directly to helping people across the world. And in return, hopefully, restore the tradition of cultivating food in Hawaii.\\n\\nAnother insightful moment was when we went up to the water tower in Lahaina. As a computer scientist, you don't learn about the social aspect of using your research for the betterment of society, so before the trip, I saw computer science research unrelated to social impact. The Sage project is special because apart from the interdisciplinary work to answer climate and ecological questions, the scientists on the team also care about the people they work to help. For example, it's important to make scientific data and discovery more digestible for the people in Lahaina to provide insight to their questions, and computer science visualization techniques do exactly that. It's also important to be sensible to community needs with the research questions to ask and prioritize. Again, I saw how my research could help on-the-field scientists surveil the land and skies with greater efficiency, allowing for quicker answers to pending questions.\\n\\nI'm forever thankful for this opportunity to not only hike above the clouds and swim in the Pacific Ocean but to meet scientists and understand their motivation behind discovery. Their insights helped me understand that computer science can directly help people through collaboration with other scientists and providing better data collection. I know now that lab work doesn't have to stay just in the lab but can be used in real-life situations to help people, as long as there's an understanding between the scientists and communities. Thank you again to all the team members from Illinois and Hawaii for sharing their experiences.\"),\n",
       " Document(metadata={'source': 'sage-website/news/2020-07-29-sage-community-workshop.md'}, page_content='title: 2020 Sage Community Workshop hide_reading_time: true tags: [workshop, NAISE]\\n\\nSage Community Workshop May 11-12, 2020\\n\\nWorkshop Explores Potential of ‘Smart Sensors’ for Environmental Monitoring\\n\\nHeld on the May 11-12, the virtual workshop brought together researchers and scientists to discuss progress on a Northwestern-led project, called SAGE, to develop machine learning-based sensors for environmental monitoring. Read more at NAISE...\\n\\nIncluded here is the agenda of the workshop with links to the presentation slides:\\n\\nView the Sage Community Workshop Agenda'),\n",
       " Document(metadata={'source': 'sage-website/news/2021-02-12-ped-count-for-cross.md'}, page_content='title: Pedestrian Count for Crosswalk Violations author: Pratool Bharti author_title: Assistant Professor, Dept. of Computer Science, Northern Illinois University author_url: https://pratoolbharti.github.io/NIU/ author_image_url: https://pratoolbharti.github.io/NIU/images/profile.png tags: [AI applications, NIU]\\n\\nPedestrian Count for Crosswalk Violations\\n\\nHi, I am Pratool Bharti, an assistant professor in Computer Science department at Northern Illinois University (NIU). Before joining NIU, I worked for 2 years in a Florida based startup as a research and development manager. There, my role was to design and build computer vision and machine learning based yard management system that automatically tracks the vehicles inside a freight yard. At NIU, I am deeply interested in solving complex real-life problems by employing computer science tools and techniques, especially artificial intelligence and computer vision. While working at Argonne National Lab in summer 2020, I worked on to design and build an AI-enabled computer vision system that counts the pedestrians who violate the crosswalk while crossing the street. The goal of this project is 3-fold; first – detect every pedestrian in the image, second – re-identify the pedestrian in successive frames to avoid their recounting, third – count the pedestrians who do not follow the crosswalk. A sample output image from the project is shown in Figure 1.\\n\\nFigure 1: A sample output image of pedestrian count project. Green box represents that the pedestrian has taken the crosswalk while crossing the street. White box represents that the pedestrian has not crossed the street yet. Motivation\\n\\nAn accurate and clear information about pedestrian travel patterns is a critical component of transportation planning, management and safety. Sound data on pedestrian system usage is needed for traffic safety, operations, maintenance as well as system user outreach and education. According to CDC report [1], in 2017 alone, 5977 pedestrians were killed in traffic crashes in the United States. That’s about one death every 88 minutes. The broad motivation of this study is to explore the pedestrian travel patterns to understand the contexts in which they violate the traffic rules. To do so, the immediate goal is to count the number of pedestrians who do not follow the crosswalk while crossing the street.\\n\\nData Description\\n\\nIn this study, a total of 2,580,468 images were collected by employing a vision camera embedded in an AoT (Array of Things) [2] node (shown in Fig. 2). The AoT node is installed on a streetlight pole at Northern Illinois University, DeKalb, IL in front of the Computer Science building. The camera captures the image (as shown in Figure 2) at 1 Hz frequency with the resolution of 96 dpi.\\n\\nFigure 2: AoT node installed on a light pole at NIU campus Approach\\n\\nIn this section, I present a brief overview of the pedestrians counting process for crosswalk violation. The complete process is divided into 3 sequential sections as shown in Figure 3. First – detect every pedestrian in the image, second – re-identify same pedestrian in successive images to avoid their recounting and third – detect when the pedestrian finished crossing the street. Each part is explained in the following sections.\\n\\nFigure 3: Workflow diagram for pedestrian crosswalk violation Pedestrian Detection\\n\\nThe first step is to detect every pedestrian along with their position in the image. To do so, one approach could be to train a neural network-based pedestrian detection model that identifies and locates the pedestrian in the image. However, this process would require a lot of manual image tagging without getting any new results since several popular pre-trained models are already available that can do a fine job in person detection. These pre-trained models are trained on a popular COCO dataset [3] which includes more than hundred thousand images. Model accuracy is an important factor here because if it misses any pedestrian in the image then the final pedestrian counting cannot be accurate. To take it into consideration, I selected the Faster R-CNN [4] based NasNet [5] object detection model from the TensorFlow model zoo [9]. Although NasNet model has high latency, it has very good mean average precision value to detect the objects precisely in the image. The images are input to NasNet model and store the prediction results in the XML format. A sample of image and generated XML is shown in Figure 4. XML stores each detected objects’ name and their box coordinates.\\n\\nFigure 4: Output image and XML generated from NasNet object detection model. Detected objects are boxed in the image. Pedestrian Re-identification (ReID)\\n\\nOnce each pedestrian’s position is stored in the XML, the next part is to identify them in successive images to avoid their recounting. In computer vision community, this task is called pedestrian re-identification (ReID) [7]. The idea behind ReID is to find a metrics or representation of a pedestrian in the image that is invariant of different angles, distance, zoom level, etc. Neural networks-based models try to learn local regions (shoes, glasses, hair color, etc.) as well as global full body region (t-shirt and shorts color, design, etc.) features to discriminate the one pedestrian from others. At the end of training, these models aim to generate invariant multi-dimensional features of a pedestrian from different angles, distance, clothes, etc. In this study, I have leveraged a deep learning-based model deep-person-reid [7,8] to generate such features of each pedestrian detected in the frame to compare with the pedestrians from the following frames for re-identification. The model generates 1024-dimensional features for each pedestrian cropped in a rectangular box. The cosine similarity is calculated for feature vectors of one frame against successive frames. Pedestrians are considered same if they have high cosine score, hence, assigned the same pedestrian id. In other cases where cosine similarity is below a pre-defined threshold, both pedestrians are assigned different ids. Low matching score may also happen where pedestrian is partially occluded by a car or another pedestrian in next frame, the similarity score gets very low. To mitigate this issue, the algorithm compared the current frame against last 5 consecutive frames to avoid assigning new id to same pedestrian. Another challenge I faced with the threshold-based matching is when one pedestrian had high similarity scores against multiple pedestrians. To fix this issue, I employed the greedy method in which it ranks each pair according to their similarity score. Based on their ranking, the algorithm picks the top pair and assigns same id to both pedestrians, and subsequently removes other pairs where any one of the pedestrians from the top pair is present. By employing these techniques, I was able to assign unique id to distinct pedestrian.\\n\\nPedestrian Count for Crosswalk Violation\\n\\nNow that a unique id is assigned to each distinct pedestrian (barring any errors), the next and the final step in the pipeline is to count the number of pedestrians who have violated the crosswalk while crossing the street. The output of this step will be two metrics for any given time period – 1) number of pedestrians crossed the street and 2) number of pedestrians followed the crosswalk while crossing the street. Subtracting the 2nd metric from 1st one will give the count of crosswalk violations. To compute these metrices, it is important to locate the street and crosswalk in the image. Fortunately, in this case, camera is installed on a fixed streetlight pole which didn’t shake or vibrate significantly due to wind or heavy vehicles. Taking advantage of it, I pre-set the location of crosswalk and street in the image (as shown in Figure 1, the crosswalk is highlighted in yellow and both sides of street in red). While the pre-set of crosswalk is represented in a form of convex polygon, both sides of the street are depicted by two parallel straight lines. These representations helped to determine the location of any pedestrian with respect to the crosswalk and the street. To recall, pedestrian’s location in the image is stored as co-ordinates of 4 corners of a rectangular box. In a 2D image, it is essential to measure each pedestrian’s location by a single (x,y) co-ordinate to make a concrete decision about their position with reference to crosswalk and street. If we observe the Figure 5, the lady is walking on the pavement towards the computer science building, but her head and center of body are still in the street (due to 2D image display) while legs are on the pavement. Similar observations in multiple images made me select the legs position to represent the pedestrian’s location because when pedestrian moves, legs represent the current location in the 2D image.\\n\\nFigure 5 : Our algorithm detects that a person has crossed the street.\\n\\nNow that a pedestrian’s position has been established, I will briefly discuss about the simple rules to determine if a pedestrian has crossed the street and followed/ violated the crosswalk. A pedestrian is considered to have crossed the street if they are detected on both sides of the street within a fixed time. To recall, the street has been represented by two straight lines, one for each side (as shown in red in Figure 1). The sign of these straight lines against the pedestrian’s coordinates exhibits their position relative to the street. For example, as we see in Figure 6, points A and B are in opposite sides of the straight line which can be verified by putting the value of these points coordinates in the line equation. While the value for point A is -2, for point B it is +3. Opposite signs of both points resemble that they are in opposite sides of the straight line. By similar means, we can compute if the pedestrian has been present to both sides of the street which will confirm that the pedestrian has crossed it. Additionally, to determine if the pedestrian has used the crosswalk, we can similarly verify their positions in the middle of the street. If all of their detected positions are inside the crosswalk polygon, I consider that the pedestrian has used the crosswalk. Using these 2 metrics, the count for crosswalk violations can be easily computed.\\n\\nFigure 6: Determining the position of the points with respect to the straight line Future Works\\n\\nIn the current study, the implemented system is able to count the pedestrians who violates the crosswalk. Currently, data processing and AI algorithms execute on a server located in computer science building where AoT node periodically stores the images. Many images are lost due to wireless nature of communication. One potential solution can be to implement the algorithms on the AoT node and only transmit the results (instead of images) to server to reduce the overhead on wireless connection significantly. Further, I would like to generalize the system for other sites as well. For that, algorithms have to identify the street and crosswalk automatically in the image.\\n\\nReferences\\n\\nA CDC Report on pedestrian Safety. (2020, March 06). Retrieved December 22, 2020, from https://www.cdc.gov/transportationsafety/pedestrian_safety/index.html\\n\\nP. Beckman, R. Sankaran, C. Catlett, N. Ferrier, and M. Papka, “Waggle: An open sensor platform for edge computing,” in 2016 IEEE Sensors, 2016, pp. 1-3.\\n\\nT. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. Zitnick. “Microsoft coco: Common objects in context.” In European conference on computer vision, pp. 740-755. Springer, Cham, 2014.\\n\\nK. He, G. Gkioxari, P. Dollár, and R. Girshick. “Mask r-cnn.” In Proceedings of the IEEE international conference on computer vision, pp. 2961-2969. 2017.\\n\\nB. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable architectures for scalable image recognition,” in Proc. IEEE Conf. CVPR, Jun. 2017, pp. 8697–8710.\\n\\nL. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian. “Scalable person re-identification: A benchmark.” In Proceedings of the IEEE international conference on computer vision, pp. 1116-1124. 2015.\\n\\nK. Zhou, Y. Yang, A. Cavallaro, and T. Xiang. “Omni-scale feature learning for person re-identification.” In Proceedings of the IEEE International Conference on Computer Vision, pp. 3702-3712. 2019.\\n\\nK. Zhou, and T. Xiang. “Torchreid: A library for deep learning person re-identification in pytorch.” arXiv preprint arXiv:1910.10093 (2019).\\n\\nTensorflow. “Tensorflow/Models.” GitHub, github.com/tensorflow/models.'),\n",
       " Document(metadata={'source': 'sage-website/news/2023-10-30-alex-blog-hawaii.md'}, page_content=\"title: The Waikalua Loko Fish Pond authors: aarnold tags: [Hawaii, Node deployment, Fish ponds]\\n\\nOctober 25th 2023\\n\\nHi there! My name is Alex Arnold and I'm currently a senior at Northwestern University studying computer and cogntitive science. I worked with the team over the summer as an intern and also had the chance to help with a node deployment in Volcanos National Park in Hawai'i.\\n\\nToday we arrived in Honolulu. We checked into our hotel right off of the famous Waikiki beach, where we saw a lot of surfers catching waves out in the water. After a nine hour flight we were incredibly hungry so Aldo and I picked up a local snack: musubi spam. It’s basically spam on top of rice and wrapped together with seaweed. Served hot, it was unexpectedly good and a pleasant surprise. After a long flight without much food it tasted incredible.\\n\\nAfter taking a short break, we headed out to the restored indigenous fish pond Waikalua Loko, which is run by the Pacific American Foundation (PAF). We crossed through a tunnel under the mountains that divide Oahu to get to the northern side of the island. We saw beautifulk views as the sun set over the mountains. We then met Dr. Derek Esibill from Windward Community College. He is working on restoring a pond that can be used to sustainably harvest fish from the bay. His work is also through a cultural lens and makes sure to incorporate education about Hawaiian culture and practices.\\n\\nThe pond is full of brackish water due to the combination of water coming in from the ocean and from a stream coming down the mountains. The two combine to make a hospitable environment for the fish to come in as youths through gates with wooden slats and fatten up. These fish are then unable to escape making them easy picking for fishermen. This method was used by the indigenous peoples of Hawai'i to sustainably feed the island. Each rock surrounding the pond was taken and placed intentionally by hand. At first glance it may seem that they made the walls too short since the ocean water spills over into the pond, but in actuality the ocean has risen due to climate change over the years. While most of the rocks are originally, these walls have been reinforced to combat the rising sea levels.\\n\\nThere used to be hundreds of these ponds, but most of them were filled in and now host fish inside of a Costco instead of the water. This particular pond also isn’t quite ready yet. A sewage treatment plant used to dump human waste into the water (remnants of which only disappeared less than two years ago) and now invasive mangrove trees have disrupted the sediments. Luckily, is working on restoring the pond and in the meantime he’s growing fish in tanks to “seed” other ponds in the area.\\n\\nThere’s currently a Sage node (W071) set up right above the fish tanks that we saw in action. It’s monitoring the air quality levels and environmental data right now, but there’s a lot more the team is wanting to do with it. Computer vision and remote sensors could replace a lot of the work needed to monitor the water conditions in each tank. Currently somebody has to go out and measure the water conditions, and automating that process would make the work much easier. A node could also be used to monitor the water level and flow of the pond with nothing but an image, and more work needs to be done to create robust models that can predict that better than the current sensors can.\\n\\nAfter we left the fish pond we all went out to dinner and learned about what other foods we should try. Derek’s son Kai (our honorary guide and recommended Poi and Lau Lau) and tried to stay up to combat jet lag. By the time we got back to our hotel we were all exhausted, but we made one more trip to the beach to take in the ocean to relax before turning in for he night.\"),\n",
       " Document(metadata={'source': 'sage-website/news/2020-08-12-derecho-talk-with-scott-collis.md'}, page_content='title: Derecho Talk with Scott Collis hide_reading_time: true tags: [talks/presentations, in the news]\\n\\nScott Collis, one of the leaders of the Sage project was on local public television August 12th to discuss the devastating derecho storms that hit the Chicago area.\\n\\nYou can catch the story here: Chicago Tonight, PBS, Aug 11th, 2020\\n\\nScott’s explanation of the storm start at about minute 19:00'),\n",
       " Document(metadata={'source': 'sage-website/news/2020-09-02-big-data.md'}, page_content=\"slug: big-data title: Argonne's Big Data Camp Goes Virtual author: Laura Wolf author_title: Coordinating Writer/Editor author_url: https://www.alcf.anl.gov/about/people/laura-wolf author_image_url: https://upload.wikimedia.org/wikipedia/commons/7/7c/Profile_avatar_placeholder_large.png tags: [education, big data, ALCF]\\n\\nFor five days last July, fourteen high school students attended a virtual coding camp sponsored by Argonne National Laboratory’s Educational Programs and Outreach and the Argonne Leadership Computing Facility (ALCF), co-taught by laboratory scientists and informal learning educators, and focused on learning techniques for probing and analyzing massive scientific datasets.\\n\\nAnd while the COVID-19 pandemic fundamentally changed how these students met, worked, and collaborated over the course of the week, they, and their instructors, found new and creative methods to explore the fascinating world of data science.\\n\\nArgonne’s Big Data Camp is the most recent addition to a growing number of STEM camps for middle school and high school students, offered by the laboratory’s education and outreach department, and aimed at teaching computer science skills and computational thinking. The curriculums are developed and taught by Argonne Educational Programs staff and Argonne scientists. The Big Data Camp curriculum targets juniors and seniors who have programming experience.\\n\\n“We teach students how researchers produce and process data to better understand a whole range of complex problems from the urgent, like those posed by the current pandemic, to the theoretical, such as the nature of dark matter,” said Michael E. Papka, Argonne senior scientist and camp instructor, who also helped establish the Big Data Camp three years ago. “We build on their existing programming knowledge and introduced them to some of the same techniques researchers use at the lab to interrogate data and gain new insights.”\\n\\nCampers learned some of the history of data science, such as how physician John Snow first studied patterns in data to map the spread of cholera in mid-nineteenth century Britain, and identified the source of the outbreak. Today’s scientific experimental facilities generate datasets that are vastly larger, and the camp covered contemporary search and analysis techniques such as data visualization methods.\\n\\nA toolkit for accessing, exploring, and sharing data\\n\\nFor one task, students accessed and explored massive datasets generated by the University of Chicago’s Array of Things (AoT) project, which employs a vast network of computer-embedded ‘intelligent’ sensors located throughout Chicago to monitor various activities, such as traffic hotspots, and environmental conditions, such as air quality. The AoT project, and its follow-on Software-Defined Sensor Network (SAGE) project, also funded by the National Science Foundation (NSF), will deploy sensor nodes that support machine learning frameworks in three environmental testbeds and one additional urban testbed in the U.S., and in four other countries, to provide even more data—all of which will be open and hosted in the cloud.\\n\\nTo bring order to the massive, amorphous datasets generated by the AoT project, the campers used the open-source web application Jupyter to create documents that contain live code, equations, narrative text, and visualizations—all the essential tools that allowed the campers to analyze the data, but also to share what they learned and communicate it.\\n\\nLater in the week, the campers worked together in small groups to first design a problem and then apply their newly learned programming knowledge and analysis techniques.\\n\\n“Creating their own project allowed them to see the potential challenges in solving big data problems,” said Janet Knowles, a member of ALCF’s visualization team who mentored one of the groups. “I was there to provide guidance, but the kids had to come up with an interesting project with a useful solution.”\\n\\nPivoting for the pandemic\\n\\nBy late March, shortly after schools were required to move instruction online because of the COVID-19 pandemic, the organizing team that included Papka, visualization specialists Joe Insley and Silvio Rizzo, operations specialist Ti Leggett, and Argonne Learning Center Lead John Domyancich, needed a strategy to adapt the camp’s hands-on, largely group-oriented lesson plan to a fully-remote experience. They saw several challenges, starting with how to provide students with resources capable of supporting the large data processing needs of the camp, and considered possible workarounds—but never once did they consider cancelling the course.\\n\\nThe team assembled a toolkit that everyone could use to access datasets and to collaborate in live active sessions. Firstly, the campers needed significant data storage and processing power—nearly all of these datasets involved were large: too large to e-mail, and certainly too large for campers to store or process locally. Second, the campers would be working on a variety of devices, from tablets to desktop computers. Lastly, the campers would need a persistent resource for the entire week. The team worked with the developers of Chameleon, a cloud infrastructure service that typically supports the NSF research community, to stand-up a virtual server that could be configured to meet all of the needs of the camp. “Chameleon gave us a powerful computing backend, which the students could access and fully utilize from any device,” said Papka.\\n\\nParticipants of the 2020 Big Data Camp seemed to take all the changes in stride. “I learned a lot about data visualization that I hope to apply to projects in the future,” said one camper at the conclusion of the course. “Thanks for making Big Data Camp possible and for turning it into such a wonderful online learning experience,” said another.\\n\\n“The feedback we received from the campers is part of a conversation we’re having now about ways to adapt the camp’s curriculum and develop learning environments to make it accessible to a larger group of students,” said Domyancich.\\n\\nBig Data Camp volunteer instructors and mentors for 2020 included ALCF staff members Michael E. Papka, Joe Insley, Silvio Rizzi, Janet Knowles, Ti Leggett, and Katherine Riley; Argonne Mathematics and Computer Science Director Valerie Taylor; Argonne nuclear engineering postdoctoral student Aaron Oaks; Northern Illinois University Assistant Professor David Koop; former Deputy Associate Laboratory Director for Computing, Environment and Life Sciences Robin Graham; and Argonne Educational Programs and Outreach staff members Kelly Sturner and John Domyancich.\\n\\nThe ALCF is a U.S. Department of Energy (DOE) Office of Science User Facility.\\n\\nAbout Argonne\\n\\nArgonne National Laboratory seeks solutions to pressing national problems in science and technology. The nation’s first national laboratory, Argonne conducts leading-edge basic and applied scientific research in virtually every scientific discipline. Argonne researchers work closely with researchers from hundreds of companies, universities, and federal, state and municipal agencies to help them solve their specific problems, advance America’s scientific leadership and prepare the nation for a better future. With employees from more than 60nations, Argonne is managed by UChicago Argonne, LLC for the U.S. Department of Energy’s Office of Science.\\n\\nThe U.S. Department of Energy’s Office of Science is the single largest supporter of basic research in the physical sciences in the United States and is working to address some of the most pressing challenges of our time. For more information, visit https://\\u200bener\\u200bgy\\u200b.gov/\\u200bs\\u200bc\\u200bience.\"),\n",
       " Document(metadata={'source': 'sage-website/news/2020-07-27-sage-ai-at-edge-workshop.md'}, page_content='title: 2019 Sage AI@Edge Science Workshop hide_reading_time: true tags: [workshop, NAISE]\\n\\nSage AI@Edge Science Workshop April 29-30, 2019\\n\\nView the Full Summary and Agenda'),\n",
       " Document(metadata={'source': 'sage-website/news/2023-05-05-neiu-crocus-deploy.md'}, page_content=\"title: Sage's Scott Collis and Waggle in the News hide_reading_time: true tags: [Waggle, Climate Science, in the news]\\n\\nSage's Scott Collis and Argonne researchers deploy Waggle nodes to study the impacts of climate change in Chicago. Read more at Argonne National Laboratory's Press Release\"),\n",
       " Document(metadata={'source': 'sage-website/news/2023-11-17-hawaii-anagha.md'}, page_content=\"title: Deployment Success at Hawaii authors: atiwari tags: [node deployment, hawaii, collaborations]\\n\\nDeployment Success at Hawaii\\n\\nHello! I’m Anagha - a Computer Science & Statistics major at the University of Illinois at Urbana Champaign. This past summer, I had the unique opportunity to work as a research intern for the Sage team to create a rideshare vehicle detection algorithm and app.\\n\\nThis past month, I traveled to Hilo, Hawai'i with the Sage team to install the latest node W097 near Volcanoes National Park. This was a truly unique experience and taught me the value of collaboration among scientists and communities to reach goals.\\n\\nBackground Context & Preparation\\n\\nOn 10/26, the team met at the University of Hawaiʻi at Mānoa with scientists Thomas Giambelluca, Han Tseng, and Dylan Giardina. We were all excited to begin preparation and unpacked the shipments for deployment. The interns helped with transporting materials and installing stainless steel equipment into the node; Raj worked with Han to explain the input-ouputs and wiring for installment; and the rest of us helped pack other equipment and ropes for an efficient deployment experience on the tower. With all hands on deck, we finished the deployment procedures swiftly and ended the day with some delicious food from a local Hawaiian eatery!\\n\\nSetup and preparation for deployment at the University of Hawaii @ Manoa\\n\\nD-Day: The Deployment\\n\\nAt 7:00 AM in the morning, the team drove to Volcanoes National Park with the hiking equipment and sensor parts in hand, ready to mount the 80-foot tower! Jason Leigh and Ryan Theriot flew down from the Lava Lab to the site to assist with the installation procedure. With over 11 people on-site, we all got our hard-hats on, geared up with equipment, and got the exciting momentum running for the mission ahead of us!\\n\\nThe video footage below exhibits some of the challenges we faced, and how teamwork was absolutely essential for installation on quite a tall tower with dense canopy, fragile equipment, and limited time. The interns were handling the ropes; Raj was directing the installation; Han, Dylan, and Tom were the tower climbers; Jason, Ryan, and I were recording video footage of the experience; and Pete was overseeing the climbers at the top. Every individual, every effort, every responsibility was integral to the deployment experience, and after 10 hours of dedication, we successfully installed the Wild Sage node and were even able to see live audio and image feedback!\\n\\nThe node is installed at approximately 80 feet high and includes equipment such as Lorowan, a rain gauge, gps, and high-accuracy cameras for audio, imaging, and sensor capabilities. Edge apps on these sensors will collect data and analytics on air quality, pollution levels, cloud cover, diversity monitors, solar irradiance levels, and more. We hope to provide this data to local Hawaiian communities and to the national parks to assess the impact of wildfires, volcanic eruptions, and diversity in plant and animal species in the surrounding area.\\n\\nRelationships and Collaborations for Current and Future\\n\\nI believe that the foundations for such a monumental and successful deployment are the relationships and collaborations Sage developed and nurtured with individuals and organizations in Hawaii.\\n\\nSuch collaborations, especially when interdisciplinary and geographically diverse, can be multifaceted and often complex. Relationships between scientists, their scientific domain, and their research can be varied yet unique. The field trip to Hawaii represents key interactions and collaborations between scientists, where diverse scientific domains such as edge computing, computer science, and the climate sciences can be integrated to pursue groundbreaking research goals.\\n\\nThe foundational relationships between universities, scientists, and the Sage team in Hawaii are key for future relationships and deployments that encourage the pursuit of science and research. For instance, Han, Tom, and Dylan were all key scientists to the installation success because of their contagious passion for science and research, their willingness to support the Hawaiian communities through providing accessible environmental data, and their hard work during deployment.\\n\\nSage sensors and edge computing act as vital segways where data collection benefits climate scientists and provides a unique insight into the Hawaiian culture and environment. Through the beauty of teamwork and collaboration, science can be transformed into a catalyst for transformational feedback and change.\\n\\nMission Success! From left to right: Anagha, Pete, Joann, Raj, Aldo, Alex, Dylan, Tom, Han\"),\n",
       " Document(metadata={'source': 'sage-website/news/2024-3-22-sage-hawaii.md'}, page_content=\"title: Sage Field Experience in Hawai'i hide_reading_time: true authors: atiwari tags: [students, edge applications, deployment, hawaii] slug: sage-hawaii\\n\\nSage Field Experience in Hawai'i\\n\\nSponsored by NAISE, the primary goal for the SAGE visit in Hawai’i was centered around experiential learning. Engaging directly with the local communities and university scientists on the island, we learned about traditional aquaculture farming practices, successfully deployed a Wild Sage Node in Pāhoa for climate research, surveyed future deployment areas needed to fulfill data needs in Lahaina, and more. This immersive field experience combined scientific exploration with community, highlighting the critical importance of conducting science that drives actionable impact and social change.\"),\n",
       " Document(metadata={'source': 'sage-website/news/2023-10-14-summer-interns-develop-for-sage.md'}, page_content='title: Summer Interns Develop Next Generation Edge Applications for Sage hide_reading_time: true authors: atiwari tags: [interns, edge applications] slug: summer-interns\\n\\nSummer Interns Develop Next Generation Edge Applications for Sage\\n\\nThe summer 2023 intern projects were filled with exciting and impactful science that truly pushed the boundaries of edge computing. This standout work includes using AI for: snow detection on the Bad River, utilizing image super-resolution, estimating solar irradiance, automating rideshare vehicle detection, lightning detection with software defined radio, smoke & wildfire detection, and building an API for Waggle infrastructure. The interns came together from a wide variety of STEM backgrounds, experiences, and universities to bring new insights into the novel applications they developed. Each intern left with a deeper understanding of various fundamental concepts of Sage - software-defined sensor networks, edge computing, and machine learning.'),\n",
       " Document(metadata={'source': 'sage-website/news/2023-05-31-scalable-ci-in-aps.md'}, page_content=\"title: Scalable AI@Edge at Argonne's Advanced Photon Source hide_reading_time: true tags: [edge computing, computer science, computational science, Argonne APS, Sage science]\\n\\nEarlier this year, Sage computer science researchers and computational scientists from Argonne's Advanced Photon Source (APS) collaborated to answer the question: Can edge computing be used in X-ray beamline experiments to process high-volume and fast data streams for real-time decision making? Read more about the experiment here under Sage Science.\"),\n",
       " Document(metadata={'source': 'sage-website/news/2022-11-03-sage-neon-deploy.md'}, page_content='slug: sage-neon-deploy-konza title: Sage Neon deployment to the Konza LTER site in Kansas author: Joe Swantek author_title: Software Engineer author_image_url: https://avatars.githubusercontent.com/u/63811401?v=4Profile_avatar_placeholder_large.png tags: [node, deployment]\\n\\nIn April 2022, 4 Sage nodes (3 Wild Sage Nodes and 1 Blade Node) were deployed with an array of sensors (thermographic camera, air quality sensor, etc.) to collect data to better our understanding of smoke and wildfire detection.\\n\\nThermal imaging was used to understand how air temperature can be used to detect smoke and the early warning signs of fire.\\n\\nDeployed MDP tower hosting Sage nodes in the middle of the controlled burn'),\n",
       " Document(metadata={'source': 'sage-data-client/README.md'}, page_content='Sage Data Client\\n\\nThis is the official Sage Python data API client. Its main goal is to make writing queries and working with the results easy. It does this by:\\n\\nProviding a simple query function which talks to the data API.\\n\\nProviding the results in an easy to use Pandas data frame.\\n\\nInstallation\\n\\nSage Data Client can be installed with pip using:\\n\\nsh pip3 install sage-data-client\\n\\nIf you prefer to install this package into a Python virtual environment or are unable to install it system wide, you can use the venv module as follows:\\n\\n```sh\\n\\n1. Create a new virtual environment called my-venv.\\n\\npython3 -m venv my-venv\\n\\n2. Activate the virtual environment\\n\\nsource my-venv/bin/activate\\n\\n3. Install sage data client in the virtual environment\\n\\npip3 install sage-data-client ```\\n\\nNote: If you are using Linux, you may need to install the python3-venv package which is outside of the scope of this document.\\n\\nNote: You will need to activate this virtual environment when opening a new terminal before running any Python scripts using Sage Data Client.\\n\\nUsage Examples\\n\\nQuery API\\n\\n```python import sage_data_client\\n\\nquery and load data into pandas data frame\\n\\ndf = sage_data_client.query( start=\"-1h\", filter={ \"name\": \"env.temperature\", } )\\n\\nprint results in data frame\\n\\nprint(df)\\n\\nmeta columns are expanded into meta.fieldname. for example, here we print the unique nodes\\n\\nprint(df[\"meta.vsn\"].unique())\\n\\nprint stats of the temperature data grouped by node + sensor.\\n\\nprint(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"])) ```\\n\\n```python import sage_data_client\\n\\nquery and load data into pandas data frame\\n\\ndf = sage_data_client.query( start=\"-1h\", filter={ \"name\": \"env.raingauge.*\", } )\\n\\nprint number of results of each name\\n\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size()) ```\\n\\nLoad results from file\\n\\nIf we have saved the results of a query to a file data.json, we can also load using the load function as follows:\\n\\n```python import sage_data_client\\n\\nload results from local file\\n\\ndf = sage_data_client.load(\"data.json\")\\n\\nprint number of results of each name\\n\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size()) ```\\n\\nIntegration with Notebooks\\n\\nSince we leverage the fantastic work provided by the Pandas library, performing things like looking at dataframes or creating plots is easy.\\n\\nA basic example of querying and plotting data can be found here.\\n\\nAdditional Examples\\n\\nAdditional code examples can be found in the examples directory.\\n\\nIf you\\'re interested in contributing your own examples, feel free to add them to examples/contrib and open a PR!\\n\\nReference\\n\\nThe query function accepts the following arguments:\\n\\nstart. Absolute or relative start timestamp. (required)\\n\\nend. Absolute or relative end timestamp.\\n\\nhead. Limit results to head earliest values per series. (Only one of head or tail can be provided.)\\n\\ntail. Limit results to tail latest values per series. (Only one of head or tail can be provided.)\\n\\nfilter. Key-value patterns to filter data on.'),\n",
       " Document(metadata={'source': 'pywaggle/README.md'}, page_content='Waggle Python Module\\n\\npywaggle is a Python module for implementing Waggle plugins and system services.\\n\\nInstallation Guides\\n\\nMost users getting started with pywaggle will want to install latest version with all optional dependencies using:\\n\\nsh pip install -U pywaggle[all]\\n\\nAdvanced users can install specific subsets of functionality using the following extras flags:\\n\\naudio - Audio and microphone support for plugins.\\n\\nvision - Image, video and camera support for plugins.\\n\\n```sh\\n\\ninstall only core plugin features\\n\\npip install pywaggle\\n\\ninstall only audio features\\n\\npip install pywaggle[audio]\\n\\ninstall only vision features\\n\\npip install pywaggle[vision]\\n\\ninstall both audio and vision features\\n\\npip install pywaggle[audio,vision] ```\\n\\nUsage Guides\\n\\nWriting a plugin'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='Writing a plugin\\n\\nAlthough this doc is still useful as a reference for more advanced pywaggle features, we have migrated the getting started portions to their own tutorial on our central docs site. The pywaggle docs will become more of a reference guide in the near future.\\n\\nIn this guide, we\\'ll walk through writing a basic plugin and exploring some of the functionality provided by pywaggle.\\n\\nThat being said, we do want to emphasize that pywaggle is designed to make it easy to interface existing Python code with the Waggle stack. To a first approximation, pywaggle aims to augment print statements with publish statements.\\n\\nIf you\\'d like to jump ahead to real code, please see the following examples:\\n\\nMinimal Numpy Example\\n\\nHello World ML Example\\n\\nThese repos can be used as starter templates for your own plugin development.\\n\\nWhat is a plugin?\\n\\nA plugin is a self-contained program which typically reads sensors, audio or video data, does some processing and finally publishes results derived from that data.\\n\\nThe most basic example of a plugin is one which simply reads and publishes a value from a sensor. A more complex plugin could publish the number of cars seen in a video stream using a deep learning model.\\n\\nPlugins fit into the wider Waggle infrastructure by being tracked in the Edge Code Repository, deployed to nodes and publishing data to our data repository.\\n\\nWriting \"Hello World\" plugin code\\n\\nNote: In this guide, we currently only cover writing the plugin __code__. We still are updating the docs on building and running a plugin inside Virtual Waggle and natively. As such, this guide will help you structure and run your code locally but not against the rest of platform.\\n\\nWe\\'ll walk through writing a \"hello world\" plugin which simply publishes a increasing counter as measurement hello.world.counter every second.\\n\\n1. Install pywaggle\\n\\nFirst, we\\'ll install the latest version of pywaggle.\\n\\nsh pip install -U pywaggle[all]\\n\\nThis will install the core pywaggle modules along with the extra developer modules.\\n\\n2. Create empty plugin directory\\n\\nCreate a new empty directory which we\\'ll write our plugin in.\\n\\nsh mkdir plugin-hello-world cd plugin-hello-world\\n\\n3. Create a requirements.txt dependency file\\n\\nCreate a new requirements.txt file and add this following:\\n\\nsh pywaggle[all]\\n\\nThis will be used when building our plugin to ensure all dependencies are available. Right now, it only contains pywaggle but you can add your own custom dependencies here.\\n\\n4. Create main.py file\\n\\nCreate a new file called main.py with the following code:\\n\\n```python from waggle.plugin import Plugin import time\\n\\nwith Plugin() as plugin: for i in range(10): print(\"publishing value\", i) plugin.publish(\"hello.world.value\", i) time.sleep(1) ```\\n\\n5. Run plugin\\n\\nThe plugin can now be run using:\\n\\nsh python3 main.py\\n\\nYou should see the output:\\n\\ntxt publishing value 0 publishing value 1 publishing value 2 ...\\n\\n6. Access run logs (Optional)\\n\\nAs you\\'re developing and debugging a plugin, it can be very helpful to see the run log of published messages and uploads.\\n\\nYou can enable this by defining the PYWAGGLE_LOG_DIR=path/to/run/logs environment variable as follows:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nNow, you should see a new directory named test-run with the following contents:\\n\\ntxt test-run/ data.ndjson <- measurements published uploads/ <- timestamped files uploaded nstimestamp1-filename1 nstimestamp2-filename2 ...\\n\\nThe data.ndjson file is a newline delimited JSON file containing raw measurement messages.\\n\\nHere\\'s an example from a more complete plugin:\\n\\njson {\"name\":\"env.temperature\",\"timestamp\":\"2022-08-23T13:27:10.562104000\",\"meta\":{\"sensor\":\"bme280\"},\"value\":23.0} {\"name\":\"upload\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{\"camera\":\"top\",\"filename\":\"test.png.webp\"},\"value\":\"/Users/sean/git/pywaggle-log-dir-example/testrun/uploads/1661279233561615000-test.png.webp\"} {\"name\":\"image.cats\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{},\"value\":0} {\"name\":\"image.birds\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{\"camera\":\"left\"},\"value\":8} {\"name\":\"timeit.inference\",\"timestamp\":\"2022-08-23T13:27:11.562104000\",\"meta\":{},\"value\":1005408000}\\n\\nThe contents of the log directory operates in an append mode, so you may safely run the plugin multiple times without losing previous data.\\n\\nAdding \"Hello World\" plugin packaging info\\n\\nNow that we have the basic plugin code working, let\\'s prepare this code to be submitted to the Edge Code Repository.\\n\\n1. Create a Github repo for plugin\\n\\nFirst, we need to create a Github repo for our plugin. Go ahead a create one called \"plugin-hello-world\" and add the contents from our plugin-hello-world directory.\\n\\nFor the purposes of this example, we\\'ll assume our plugin URL is https://github.com/username/plugin-hello-world.\\n\\n2. Add Dockerfile\\n\\nCreate and add a new file called Dockerfile with the following contents:\\n\\ndockerfile FROM waggle/plugin-base:1.1.1-ml COPY requirements.txt /app/ RUN pip3 install --no-cache-dir --upgrade -r /app/requirements.txt COPY . /app/ WORKDIR /app ENTRYPOINT [\"python3\", \"/app/main.py\"]\\n\\nThis file defines what base image should be used by a plugin and how it should be run. In more complex examples, additional dependencies may be specified here.\\n\\n3. Add sage.yaml\\n\\nCreate and add a new file called sage.yaml with the following contents:\\n\\nyaml name: \"hello-world\" description: \"My hello world plugin\" keywords: \"hello, testing\" authors: \"Your Name <your.email@somewhere.org>, A Coworker <your.coworker@somewhere.org>\" collaborators: \"Helpful Collaborator <our.collaborator@otherplace.edu>\" funding: \"\" license: \"\" homepage: \"https://github.com/username/plugin-hello-world/blob/main/README.md\" source: architectures: - \"linux/amd64\" - \"linux/arm64\"\\n\\nThis file contains metadata about what your plugin is called and what it\\'s supposed to do. It is used by the Edge Code Repository when submitting plugins.\\n\\n4. Add ECR media\\n\\nCreate a ecr-meta directory and populate it with the following text and media:\\n\\necr-science-description.md - Markdown with in depth description of the science being done here (1 page of text).\\n\\necr-icon.jpg - An icon for the project/work 512x512px.\\n\\necr-science-image.jpg - A science image for the project with a minimum size of 1920x1080px.\\n\\nBeyond the basics\\n\\nMore about the publish function\\n\\nIn the previous example, we saw the most basic usage of the publish function. Now, we want to talk about a couple additional features available to you.\\n\\nFirst, metadata can be added to measurements to provide context to how a measurement was created. For example, suppose we had a left and a right facing camera on a node and wanted to track which one was used.\\n\\npython plugin.publish(\"my.sensor.name\", 123, meta={\"camera\": \"left\"})\\n\\nThis will bind the meta data together with the measurement and will be available throughout the rest of the data pipeline.\\n\\nSecond, you can explicitly provide a timestamp for situations where you have more information on when a measurement was taken. For example:\\n\\npython plugin.publish(\"my.sensor.name\", 123, timestamp=my_timestamp_in_ns)\\n\\nNote: Timestamps are expected to be in nanoseconds since epoch. In Python 3.7+, this is available through the standard time.time_ns() function.\\n\\nSubscribing to other measurements\\n\\nPlugins can subscribe to measurements published by other plugins running on the same node. This allows users to leverage existing work or compose a larger application of multiple independent components.\\n\\nThe followng basic example simply waits for measurements named \"my.sensor.name\" and prints the value it received.\\n\\n```python from waggle.plugin import Plugin from random import random\\n\\nwith Plugin() as plugin: plugin.subscribe(\"my.sensor.name\")\\n\\nwhile True:\\n    msg = plugin.get()\\n    print(\"Another plugin published my.sensor.name value\", msg.value)\\n\\n```\\n\\nIn the case you need multiple multiple measurements, you can simply use:\\n\\npython plugin.subscribe(\"my.sensor.name1\", \"my.sensor.name2\", \"my.sensor.name3\")\\n\\nTo differentiate the results, you can use the message name:\\n\\npython msg = plugin.get() if msg.name == \"my.sensor.name1\": # do something elif msg.name == \"my.sensor.name2\": # do something else\\n\\nIn more complex examples, the full message metadata can also be used to differentiate behavior:\\n\\n```python plugin.subscribe(\"env.temperature\")\\n\\nwhile True: msg = plugin.get() if msg.meta.get(\"sensor\") == \"bme280\": # do something elif msg.meta.get(\"sensor\") == \"bme680\": # do something else ```\\n\\nMore about the subscribe function\\n\\nThe subscribe function can match two kinds of wildcard patterns. Measurement names are treated as \"segments\" broken up by a dot and we can match various segments using the star and hash operators.\\n\\nFirst, we can match a single wildcard segment using the \"my.sensor.*\" pattern. This will match all measurements with exactly three segments and whose first segment is \"my\", second segment is \"sensor\" and third segment can be anything.\\n\\nSecond, we can match zero or more segments using the \"my.#\" pattern. This will match all measurements whose first segment is \"my\" like \"my.sensor\", \"my.sensor.name\" or \"my.sensor.name.is.cool\".\\n\\nWorking with camera and microphone data\\n\\npywaggle provides a simple abstraction to cameras and microphones.\\n\\nAccessing a video stream\\n\\n```python from waggle.plugin import Plugin from waggle.data.vision import Camera import time\\n\\nuse case 1: take a snapshot and process\\n\\nwith Plugin() as plugin: sample = Camera().snapshot() # do processing result = process(sample.data) plugin.publish(\"my.measurement\", result, timestampe=sample.timestamp)\\n\\nuser case 2: process camera frames\\n\\nwith Plugin() as plugin, Camera() as camera: # process samples from video stream for sample in camera.stream(): count = count_cars_in_image(sample.data) if count > 10: sample.save(\"cars.jpg\") plugin.upload_file(\"cars.jpg\") ```\\n\\nThe camera.snapshot() function returns a camera frame. The function provides a convenient way to capture a frame and process it.\\n\\nThe camera.stream() function yields a sequence of ImageSample with the following properties:\\n\\nsample.data. captured image\\'s numpy data array.\\n\\nsample.timestamp. captured image\\'s nanosecond timestamp.\\n\\nAdditionally, the Camera class accepts URLs and video files as input. For example:\\n\\n```python\\n\\nopen an mjpeg-over-http stream\\n\\ncamera = Camera(\"http://camera-server/profile1.mjpeg\")\\n\\nopen an rtsp stream\\n\\ncamera = Camera(\"rtsp://camera-server/v0.mp4\")\\n\\nopen a local file using file:// url\\n\\ncamera = Camera(\"file://path/to/my_cool_video.mp4\")\\n\\nopen a camera by device id (when plugin runs on a node)\\n\\ncamera = Camera(\"bottom_camera\") ```\\n\\nCamera buffering and use cases\\n\\n```\\n\\nQ1. How often do you need camera frames? A1. (As many as possible) ---> Refer to use case 1 A2. (Occasionally) ---> Go to Q2\\n\\nQ2. How sensitive is your application to a short delay when capturing an image? A1. (Very sensitive) ---> Refer to use case 1 A2. (A second is ok) ---> Refer to use case 2 ```\\n\\nThe Camera class wrapped in the Python with statement runs a background thread to keep up with the camera stream. This allows users to get the latest frame whenever .stream() or .snapshot() are called. However, this may be uncessary when users want to close the stream after grabbing a frame or the Camera class is used with a file, not a stream.\\n\\nTherefore, it is highly recommended to use the Camera class with the Python with statement when users want to process consequtive frames.\\n\\nUse case 1\\n\\n```python from time import sleep from waggle.data.vision import Camera\\n\\nwith Camera() as camera: former_frame = camera.snapshot() sleep(5) # the current_frame gets the latest frame current_frame = camera.snapshot() calculate_motion(current_frame, former_frame) ```\\n\\nFor simple grab-and-go use cases, users use the Camera class without the with statement to avoid the background process and its resource consumption.\\n\\nUse case 2 ```python from time import sleep from waggle.data.vision import Camera\\n\\nThe Camera class closes the stream after obtaining\\n\\na frame\\n\\ncamera = Camera() former_frame = camera.snapshot() sleep(5)\\n\\nThe Camera class opens the stream and grabs a frame\\n\\ncurrent_frame = camera.snapshot() calculate_motion(current_frame, former_frame) ```\\n\\nRecording video data\\n\\n```python from waggle.data.vision import Camera\\n\\ncamera = Camera()\\n\\nrecord a 30-second video from the camera\\n\\nvideo = camera.record(duration=30) with video: for frame in video: process(frame.data) ```\\n\\nThe Camera class allows users to record a video from camera and store the clip into a file. Because it relies on ffmpeg user code and its container (if in a Docker container) must have ffmpeg installed. You may install it as follow,\\n\\n```bash\\n\\nfor ubuntu\\n\\napt-get update && apt-get install -y ffmpeg ```\\n\\nAlso, the .record() function may NOT be used with Python with statement for USB cameras.\\n\\n```python from waggle.data.vision import Camera\\n\\nthe camera is a USB camera\\n\\ndevice = \"/dev/camera0\" with Camera(device) as camera: # this raises an exception as the camera stream is already open by the with statement video = camera.record(duration=30)\\n\\nUSB cameras can be used as below\\n\\nvideo = Camera(device).record(duration=30) ```\\n\\nRecording audio data\\n\\n```python from waggle.plugin import Plugin from waggle.data.audio import Microphone import time\\n\\nwith Plugin() as plugin, Microphone() as microphone: # record and upload a 10s sample periodically while True: sample = microphone.record(10) sample.save(\"sample.ogg\") plugin.upload_file(\"sample.ogg\") time.sleep(300) ```\\n\\nSimilar to ImageSample, AudioSample provide the following properties:\\n\\nsample.data. captured audio\\'s numpy data array.\\n\\nsample.timestamp. captured audio\\'s nanosecond timestamp.\\n\\nsample.samplerate. captured audio\\'s sample rate.\\n\\nAudioFolder and ImageFolder for testing\\n\\nWe provide a couple simple classes to provide audio and image data from a directory for testing.\\n\\nIn the following example, we assume we have directories:\\n\\ntxt audio_data/ example1.ogg example2.ogg ... image_data/ img1.png cat-7.png example10.jpg ...\\n\\nWe can load up all the audio files in the audio_data folder for testing as follows:\\n\\n```python from waggle.data.audio import AudioFolder\\n\\ndataset = AudioFolder(\"audio_data\")\\n\\nfor sample in dataset: process_data(sample.data) ```\\n\\nSimilarly, we can do something similar for all the image files in the image_data folder.\\n\\n```python from waggle.data.vision import ImageFolder\\n\\ndataset = ImageFolder(\"image_data\")\\n\\nfor sample in dataset: process_image_frame(sample.data) ```\\n\\nAdvanced: Choosing a color format\\n\\nBy default, the waggle.data.vision submodule uses an RGB color format. If you need more control, you can specify one of RGB or BGR to both the Camera and ImageFolder objects as follows:\\n\\n```python from waggle.data.vision import Camera, ImageFolder, RGB, BGR\\n\\nuse BGR data instead of RGB\\n\\ncamera = Camera(format=BGR)\\n\\nuse BGR data instead of RGB\\n\\ncamera = ImageFolder(format=BGR) ```\\n\\nAdvanced: Timing a block\\n\\nThe Plugin class provides a simple utility for timing how long a block of code takes.\\n\\nThe following example shows how we can instrument our code using a typical AI/ML example plugin.\\n\\n```python from waggle.plugin import Plugin\\n\\nwith Plugin() as plugin: # measures duration of input block and publishes to plugin.duration.input with plugin.timeit(\"plugin.duration.input\"): get_inputs(...)\\n\\n# measures duration of inference block and publishes to plugin.duration.inference\\nwith plugin.timeit(\"plugin.duration.inference\"):\\n    do_inference(...)\\n\\npublish_results(...)\\n\\n```\\n\\nIn the example above, the duration of the input and inference steps are measured and then the plugin publishes the duration in nanoseconds to the name provided to plugin.timeit as each block finishes.\\n\\nSeeing the internal details\\n\\nIf we run the basic example, the only thing we\\'ll see is the message \"publishing a value!\" every second. If you need to see more details, pywaggle is designed to easily interface with Python\\'s standard logging module. To enable debug logging, simply make the following additions:\\n\\n```python from waggle.plugin import Plugin from time import sleep\\n\\n1. import standard logging module\\n\\nimport logging\\n\\n2. enable debug logging\\n\\nlogging.basicConfig(level=logging.DEBUG)\\n\\nwith Plugin() as plugin: while True: sleep(1) print(\"publishing a value!\") plugin.publish(\"my.sensor.name\", 123) ```\\n\\nYou should see a lot of information like:\\n\\ntext DEBUG:waggle.plugin:starting plugin worker thread DEBUG:waggle.plugin:connecting to rabbitmq broker at rabbitmq:5672 with username \"plugin\" DEBUG:waggle.plugin:rabbitmq connection error: [Errno 8] nodename nor servname provided, or not known publishing a value! DEBUG:waggle.plugin:adding message to outgoing queue: Message(name=\\'my.sensor.name\\', value=123, timestamp=1619628240863845000, meta={}) DEBUG:waggle.plugin:connecting to rabbitmq broker at rabbitmq:5672 with username \"plugin\" DEBUG:waggle.plugin:rabbitmq connection error: [Errno 8] nodename nor servname provided, or not known publishing a value!\\n\\nThe most important lines are:\\n\\ntext publishing a value! DEBUG:waggle.plugin:adding message to outgoing queue: Message(name=\\'my.sensor.name\\', value=123, timestamp=1619628240863845000, meta={})\\n\\nThese are telling us that our messages are being queued up in an outgoing queue to be shipped.\\n\\nYou\\'ll also see a number of messages related to rabbitmq.\\n\\nThese are simply indicating the our plugin is waiting to connect to the Waggle ecosystem. This is normal when testing a standalone plugin without the rest of the Waggle stack. Plugins will simply queue up measurements in-memory until they exit.'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/README.md'}, page_content='Waggle Module\\n\\nThis page gives an overview of the core functionality provided by this module.\\n\\nPlugin Submodule\\n\\nProvides functionality for publishing sensor data and for processing messages.\\n\\nProtocol Submodule\\n\\nProvides functionality for packing and unpacking sensor and messaging data.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load all Markdown files\n",
    "md_docs = repo_class_loader(paths, './*.md', UnstructuredMarkdownLoader)\n",
    "md_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 ms, sys: 14.3 ms, total: 25.4 ms\n",
      "Wall time: 39.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='import unittest\\nimport sage_data_client\\nfrom io import BytesIO\\nfrom datetime import datetime, timedelta\\nimport pandas as pd\\n\\n\\nclass TestQuery(unittest.TestCase):\\n    def assertValueResponse(self, df):\\n        self.assertIn(\"name\", df.columns)\\n        df.name.str\\n        self.assertIn(\"timestamp\", df.columns)\\n        df.timestamp.dt\\n        self.assertIn(\"value\", df.columns)\\n\\n    def test_empty_response(self):\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"-2000d\",\\n                filter={\\n                    \"name\": \"should.not.every.exist.XYZ\",\\n                },\\n            )\\n        )\\n\\n    def test_check_one_of_head_or_tail(self):\\n        with self.assertRaises(ValueError):\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00\",\\n                end=\"2021-01-01T10:31:00\",\\n                head=3,\\n                tail=3,\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n\\n    def test_queries(self):\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00\",\\n                end=\"2021-01-01T10:31:00\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00Z\",\\n                end=\"2021-01-01T10:31:00Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00.123Z\",\\n                end=\"2021-01-01T10:31:00.123Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00.123456Z\",\\n                end=\"2021-01-01T10:31:00.123456Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01 10:30:00\",\\n                end=\"2021-01-01 10:31:00\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=datetime(2021, 1, 1, 10, 31, 0),\\n                end=datetime(2021, 1, 1, 10, 32, 0),\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=pd.to_datetime(\"2021-01-01 10:30:00\"),\\n                end=pd.to_datetime(\"2021-01-01 10:31:00\"),\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        for dt in [\"-30s\", \"-3m\", \"-3min\", \"-1d\", \"-1w\"]:\\n            self.assertValueResponse(\\n                sage_data_client.query(\\n                    start=dt,\\n                    tail=1,\\n                    filter={\\n                        \"name\": \"env.temperature\",\\n                    },\\n                )\\n            )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"-4h\",\\n                end=\"-2h\",\\n                tail=1,\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        for dt in [\\n            timedelta(seconds=-30),\\n            timedelta(minutes=-1),\\n            timedelta(hours=-1),\\n            timedelta(days=-1),\\n        ]:\\n            self.assertValueResponse(\\n                sage_data_client.query(\\n                    start=dt,\\n                    tail=1,\\n                    filter={\\n                        \"name\": \"env.temperature\",\\n                    },\\n                )\\n            )\\n\\n    def test_load(self):\\n        sample_data = BytesIO(\\n            b\"\"\"{\"timestamp\":\"2021-10-14T21:42:21.149425156Z\",\"name\":\"env.temperature\",\"value\":21.74,\"meta\":{\"host\":\"0000dca632a3069f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31f\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"env.temperature\",\"value\":26.09,\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}\\n{\"timestamp\":\"2021-10-14T21:42:19.087014343Z\",\"name\":\"env.temperature\",\"value\":28.14,\"meta\":{\"host\":\"0000dca632a3074d.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc73\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W024\"}}\\n{\"timestamp\":\"2021-10-14T21:42:23.475857326Z\",\"name\":\"env.temperature\",\"value\":28.16,\"meta\":{\"host\":\"0000dca632a3076b.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc6d\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01B\"}}\\n{\"timestamp\":\"2021-10-14T21:42:34.995766556Z\",\"name\":\"env.temperature\",\"value\":33.27,\"meta\":{\"host\":\"0000dca632a3078f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bdc7\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W020\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.803472584Z\",\"name\":\"env.temperature\",\"value\":9.8,\"meta\":{\"host\":\"0000dca632a30792.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc42\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01D\"}}\\n{\"timestamp\":\"2021-10-14T21:42:30.9261079Z\",\"name\":\"env.temperature\",\"value\":25.63,\"meta\":{\"host\":\"0000dca632a307b6.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc8c\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W039\"}}\\n{\"timestamp\":\"2021-10-14T21:42:24.228048661Z\",\"name\":\"env.temperature\",\"value\":23.96,\"meta\":{\"host\":\"0000dca632a307bf.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc7d\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W028\"}}\\n{\"timestamp\":\"2021-10-14T21:42:13.914329997Z\",\"name\":\"env.temperature\",\"value\":21.84,\"meta\":{\"host\":\"0000dca632a307e6.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d05a1c2\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W02C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:17.300924641Z\",\"name\":\"env.temperature\",\"value\":30.12,\"meta\":{\"host\":\"0000dca632a307fb.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c328\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W016\"}}\\n\"\"\"\\n        )\\n        df = sage_data_client.load(sample_data)\\n        self.assertEqual(len(df), 10)\\n        self.assertValueResponse(df)\\n\\n    def test_load_small_numbers(self):\\n        sample_data = BytesIO(\\n            b\"\"\"{\"timestamp\":\"2023-09-25T19:26:26.18944512Z\",\"name\":\"sensor_body_temperature\",\"value\":-655230609742226300000,\"meta\":{\"applicationId\":\"ac81e18b-1925-47f9-839a-27d999a8af55\",\"applicationName\":\"ATMOS test app\",\"devAddr\":\"00f06b4b\",\"devEui\":\"98208e0000032a15\",\"deviceName\":\"MFR Node\",\"deviceProfileId\":\"cf2aec2f-03e1-4a60-a32c-0faeef5730d8\",\"deviceProfileName\":\"MFR node\",\"host\":\"0000e45f014caee8.ws-rpi\",\"node\":\"000048b02d15bc8c\",\"plugin\":\"registry.sagecontinuum.org/flozano/lorawan-listener:0.0.6\",\"task\":\"lorawan-listener\",\"tenantId\":\"52f14cd4-c6f1-4fbd-8f87-4025e1d49242\",\"tenantName\":\"ChirpStack\",\"vsn\":\"W039\",\"zone\":\"shield\"}}\\n\"\"\"\\n        )\\n        df = sage_data_client.load(sample_data)\\n        self.assertValueResponse(df)\\n        self.assertEqual(len(df), 1)\\n\\n    def test_load_empty_file(self):\\n        df = sage_data_client.load(\"tests/test-empty.ndjson\")\\n        self.assertEqual(len(df), 0)\\n        self.assertValueResponse(df)\\n\\n        df = sage_data_client.load(\"tests/test-empty.ndjson.gz\")\\n        self.assertEqual(len(df), 0)\\n        self.assertValueResponse(df)\\n\\n    def test_load_compressed_file(self):\\n        df1 = sage_data_client.load(\"tests/test-data.ndjson\")\\n        df2 = sage_data_client.load(\"tests/test-data.ndjson.gz\")\\n        self.assertEqual(len(df1), 1611)\\n        self.assertEqual(len(df2), 1611)\\n        # NOTE In Pandas Nones and NaNs do not equal themselves, so will fill them to make df1 == df2 work.\\n        self.assertTrue((df1.fillna(\"\") == df2.fillna(\"\")).all().all())\\n\\n    def test_mixed_types(self):\\n        sample_data = BytesIO(\\n            b\"\"\"{\"timestamp\":\"2021-10-14T21:42:21.149425156Z\",\"name\":\"test\",\"value\":21.74,\"meta\":{\"host\":\"0000dca632a3069f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31f\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"test\",\"value\":\"26.09\",\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"test\",\"value\":123,\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}\\n\"\"\"\\n        )\\n        df = sage_data_client.load(sample_data)\\n        self.assertEqual(len(df), 3)\\n        self.assertValueResponse(df)\\n        self.assertAlmostEqual(df.iloc[0].value, 21.74)\\n        self.assertEqual(df.iloc[1].value, \"26.09\")\\n        self.assertEqual(df.iloc[2].value, 123)\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/load_data_from_file.py'}, page_content='\"\"\"\\nThis example demonstrates loading a local data file containing 5 minutes of temperature data\\nand printing the mean value grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# load results from local file\\ndf = sage_data_client.load(\"data.json\")\\n\\n# print number of results of each name\\nprint(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.mean())\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/trigger-stream.py'}, page_content='\"\"\"\\nThis is an example of a simple edge-to-cloud stream trigger which uses sage-data-client\\nto watch the latest internal temperature values and print records which exceed a threshold.\\n\\nAlthough it\\'s simple, this example could easily be extended in multiple ways. For example:\\n* Instead of just printing, an alert could be posted to Slack.\\n* Instead of a fixed threshold, you could learn a moving average per node and flag outliers.\\n\\nNote: In the future, this kind of streaming functionality *might* be provided by sage-data-client,\\nbut for now you can adapt this example to fit you use case.\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\nimport time\\n\\n\\ndef watch(start=None, filter=None):\\n    if start is None:\\n        start = pd.Timestamp.utcnow()\\n\\n    while True:\\n        df = sage_data_client.query(\\n            start=start,\\n            filter=filter,\\n        )\\n\\n        if len(df) > 0:\\n            start = df.timestamp.max()\\n            yield df\\n\\n        time.sleep(3.0)\\n\\n\\ndef main():\\n    filter = {\\n        \"name\": \"env.temperature\",\\n        \"sensor\": \"bme280\",\\n    }\\n\\n    threshold = 50.0\\n\\n    for df in watch(filter=filter):\\n        # print values which exceed threshold\\n        print(df[df.value > threshold].sort_values(\"timestamp\"))\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/print_rain_event_image_urls.py'}, page_content='\"\"\"\\nThis example demonstrates cross referencing rain gauge data to find rainy images. It outputs a list\\nof urls which can be saved and downloaded as follows:\\n\\npython3 print_rain_event_image_urls.py > urls.txt\\nwget -r -N -i urls.txt\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\n\\nvsn = \"W039\"\\n\\n# query raingauge data for the last week\\ndf = sage_data_client.query(\\n    start=\"2021-12-20\",\\n    end=\"2021-12-27\",\\n    filter={\\n        \"name\": \"env.raingauge.acc\",\\n        \"vsn\": vsn,\\n    }\\n)\\n\\n# compute mean rain in hour window\\nmean_acc = df.resample(\"1h\", on=\"timestamp\").value.mean()\\n\\n# find rain accumulation events\\nrain_events = mean_acc[mean_acc > 0]\\n\\n# collect uploads in each rain event window\\nuploads = pd.concat(sage_data_client.query(\\n        start=ts,\\n        end=ts + pd.to_timedelta(\"1h\"),\\n        filter={\\n            \"name\": \"upload\",\\n            \"vsn\": vsn,\\n            \"task\": \"imagesampler-top\",\\n        }\\n    ) for ts in rain_events.index)\\n\\n# print all urls found\\nfor url in uploads.value.values:\\n    print(url)\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/pressure_event_trigger.py'}, page_content='\"\"\"\\nThis example is a skeleton of how to poll the data system every minute for unusual\\npressure events.\\n\\nIn this case, events are determined windows with a stddev above an example\\nthreshold. For applications, you will need to provide your own criteria for\\nevents.\\n\\nAdditionally, you will need to provide a specific mechanism to carry out the\\nalerts (ex. email, Slack, dedicated alerting / ticketing system, etc).\\n\"\"\"\\nimport sage_data_client\\nimport time\\n\\nwhile True:\\n    # query pressure data in recent 10 minute window\\n    df = sage_data_client.query(\\n        start=\"-10m\",\\n        filter={\\n            \"name\": \"env.pressure\",\\n            \"sensor\": \"bme680\",\\n        }\\n    )\\n\\n    # compute stddev for nodes\\' pressure data in window\\n    std = df.groupby(\"meta.vsn\").value.std()\\n\\n    # find all pressure events exceeding an example threshold\\n    events = std[std > 8.0]\\n\\n    # \"post\" vsn to alert system\\n    for vsn in events.index:\\n        print(f\"post {vsn} to alert system\")\\n\\n    time.sleep(60)\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/temperature_stats.py'}, page_content='\"\"\"\\nThis example demonstrates querying all temperature data and printing basic stats grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# query and load data into pandas data frame\\ndf = sage_data_client.query(\\n    start=\"-1h\",\\n    filter={\\n        \"name\": \"env.temperature\",\\n    }\\n)\\n\\n# print stats of the temperature data grouped by node + sensor.\\nprint(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"]))\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/join_queries.py'}, page_content='\"\"\"\\nThis example demonstrates one approach for combining multiple queries by resampling\\nresults into 30 minute windows and merging those into a new data frame.\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\n\\n\\ndef join_resampled_queries(start, end, window, filters):\\n    \"\"\"\\n    join_resampled_queries joins resampled data for a set of filters together\\n    into a single data frame\\n    \"\"\"\\n    return pd.DataFrame({\\n        name: sage_data_client.query(\\n            start=start,\\n            end=end,\\n            filter=filter,\\n        ).resample(window, on=\"timestamp\").value.mean()\\n        for name, filter in filters.items()\\n    })\\n\\n\\ndef main():\\n    start = \"2022-01-10T00:00:00Z\"\\n    end = \"2022-01-11T00:00:00Z\"\\n    vsn = \"W023\"\\n\\n    # combine lat, lon, temperature, pressure and humidity into data frame\\n    df = join_resampled_queries(start, end, \"30min\", {\\n        \"lat\": {\\n            \"name\": \"sys.gps.lat\",\\n            \"vsn\": vsn,\\n        },\\n        \"lon\": {\\n            \"name\": \"sys.gps.lon\",\\n            \"vsn\": vsn,\\n        },\\n        \"temperature\": {\\n            \"name\": \"env.temperature\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n        \"pressure\": {\\n            \"name\": \"env.pressure\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n        \"humidity\": {\\n            \"name\": \"env.relative_humidity\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n    })\\n\\n    # print out data for quick inspection\\n    print(df)\\n\\n    # save data to csv\\n    df.to_csv(\"combined.csv\")\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/raingauge_totals.py'}, page_content='\"\"\"\\nThis example demonstrates querying rain gauge data and printing the total\\nnumber of measurements grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# query and load data into pandas data frame\\ndf = sage_data_client.query(\\n    start=\"-1h\",\\n    filter={\\n        \"name\": \"env.raingauge.*\",\\n    }\\n)\\n\\n# print number of results of each name\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size())\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/trigger-batch.py'}, page_content='\"\"\"\\nThis is an example of a simple edge-to-cloud batch trigger which uses sage-data-client\\nto gather and aggregate internal temperature data every 5 minutes and prints all\\nnodes which exceed a threshold.\\n\\nAlthough it\\'s simple, this example could easily be extended in multiple ways. For example:\\n* Instead of just printing, an alert could be posted to Slack.\\n* Instead of a fixed threshold, the typical value across all nodes could be used to determine outliers.\\n\"\"\"\\nimport sage_data_client\\nimport time\\n\\n\\ndef main():\\n    filter = {\\n        \"name\": \"env.temperature\",\\n        \"sensor\": \"bme280\",\\n    }\\n\\n    threshold = 55.0\\n\\n    while True:\\n        # get the last 5m of temperature data\\n        df = sage_data_client.query(start=\"-5m\", filter=filter)\\n\\n        # get mean temperature by node in batch query\\n        mean_temps = df.groupby(\"meta.vsn\").value.mean()\\n\\n        # print values which exceed threshold\\n        print(mean_temps[mean_temps > threshold])\\n\\n        # wait 5m\\n        time.sleep(300)\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='from gzip import GzipFile\\nimport json\\nfrom pathlib import Path\\nfrom urllib.request import urlopen, Request\\nimport pandas as pd\\n\\n\\ndef resolve_time(t):\\n    try:\\n        return pd.to_datetime(t)\\n    except (TypeError, ValueError):\\n        pass\\n    return pd.to_datetime(\"now\", utc=True) + pd.to_timedelta(t)\\n\\n\\ndef timestr(t):\\n    return t.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\\n\\n\\ndef query(\\n    start,\\n    end=None,\\n    head: int = None,\\n    tail: int = None,\\n    experimental_func=None,\\n    bucket: str = None,\\n    filter: dict = None,\\n    endpoint: str = \"https://data.sagecontinuum.org/api/v1/query\",\\n) -> pd.DataFrame:\\n    \"\"\"\\n    query makes a query request to the data API and returns the results in a data frame.\\n\\n    Parameters\\n    ----------\\n    start : query start time, required\\n        Timestamps can be a relative like \"-1h\" or absolute like \"2021-05-01T10:30:00Z\".\\n\\n    end : query end time, default: None\\n        Timestamps can be a relative like \"-1h\" or absolute like \"2021-05-01T10:30:00Z\".\\n\\n    head : limit query response to earliest `head` records, default: None (only one of `head` or `tail` can be provided)\\n\\n    tail : limit query response to latest `tail` records, default: None (only one of `head` or `tail` can be provided)\\n\\n    experimental_func : aggregation function to apply to series.\\n\\n    bucket: name of bucket to query\\n\\n    filter : dictionary of query filters, default: None\\n\\n    endpoint : url of query api, default: \"https://data.sagecontinuum.org/api/v1/query\"\\n\\n    Returns\\n    -------\\n    result : pandas.DataFrame\\n        The data frame will contain the query response records.\\n\\n        See the Returns section for the `load` function for more details.\\n\\n    Examples\\n    --------\\n\\n    Querying and perform simple data aggregation\\n\\n    ```python\\n    import sage_data_client\\n\\n    # query and load data into pandas data frame\\n    df = sage_data_client.query(\\n        start=\"-1h\",\\n        filter={\\n            \"name\": \"env.temperature\",\\n        }\\n    )\\n\\n    # print results in data frame\\n    print(df)\\n\\n    # meta columns are expanded into meta.fieldname. for example, here we print the unique nodes\\n    print(df[\"meta.node\"].unique())\\n\\n    # print stats of the temperature data grouped by node + sensor.\\n    print(df.groupby([\"meta.node\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"]))\\n    ```\\n    \"\"\"\\n    # build query\\n    q = {\"start\": timestr(resolve_time(start))}\\n    if end is not None:\\n        q[\"end\"] = timestr(resolve_time(end))\\n    if filter is not None:\\n        q[\"filter\"] = filter\\n    if head is not None and tail is not None:\\n        raise ValueError(\"only one of `head` or `tail` can be provided\")\\n    elif head is not None:\\n        q[\"head\"] = head\\n    elif tail is not None:\\n        q[\"tail\"] = tail\\n    if experimental_func is not None:\\n        q[\"experimental_func\"] = experimental_func\\n    if bucket is not None:\\n        q[\"bucket\"] = bucket\\n\\n    data = json.dumps(q).encode()\\n    headers = {\"Accept-Encoding\": \"gzip\"}\\n    req = Request(endpoint, data, headers=headers)\\n\\n    with urlopen(req) as f:\\n        content_encoding = f.headers.get(\"Content-Encoding\", \"\")\\n        if \"gzip\" in content_encoding:\\n            f = GzipFile(fileobj=f, mode=\"rb\")\\n        return load(f)\\n\\n\\ndef load(path_or_buf) -> pd.DataFrame:\\n    \"\"\"\\n    load reads a path or file like object containing a response from the data api and returns the results in a data frame.\\n\\n    Parameters\\n    ----------\\n    path_or_buf : path like or file like object\\n\\n    Returns\\n    -------\\n    result : pandas.DataFrame\\n        The data frame will contain the query response records. Standard columns names are:\\n\\n        `name`: measurement name (ex. \"env.temperature\")\\n\\n        `timestamp`: measurement timestamp (nanoseconds since epoch resolution)\\n\\n        `value`: measurement value\\n\\n        Metadata fields like \"node\" and \"vsn\" are stored in columns named \"meta.node\" or \"meta.vsn\".\\n\\n    Examples\\n    --------\\n\\n    Loading saved query results from a file\\n\\n    Suppose we\\'ve saved the results of a query to a file `data.json`. We can load them using the following:\\n\\n    ```python\\n    import sage_data_client\\n\\n    # load results from local file\\n    df = sage_data_client.load(\"data.ndjson\")\\n\\n    # print number of results of each name\\n    print(df.groupby([\"meta.node\", \"name\"]).size())\\n    ```\\n    \"\"\"\\n    if isinstance(path_or_buf, str):\\n        if path_or_buf.endswith(\".gz\"):\\n            with GzipFile(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n        else:\\n            with open(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n\\n    if isinstance(path_or_buf, Path):\\n        if path_or_buf.suffix == \".gz\":\\n            with GzipFile(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n        else:\\n            with open(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n\\n    return _load(path_or_buf)\\n\\n\\ndef _load_row(r):\\n    input = json.loads(r)\\n    output = {}\\n    output[\"timestamp\"] = pd.to_datetime(input[\"timestamp\"], unit=\"ns\", utc=True)\\n    output[\"name\"] = input[\"name\"]\\n    output[\"value\"] = input[\"value\"]\\n    for k, v in input[\"meta\"].items():\\n        output[f\"meta.{k}\"] = v\\n    return output\\n\\n\\ndef _load(fileobj) -> pd.DataFrame:\\n    df = pd.DataFrame(map(_load_row, fileobj))\\n\\n    # if dataframe is empty, return empty with known columns\\n    if len(df) == 0:\\n        return pd.DataFrame(\\n            {\\n                \"timestamp\": pd.to_datetime([], utc=True),\\n                \"name\": pd.Series([], dtype=str),\\n                \"value\": [],\\n            }\\n        )\\n\\n    return df\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/__init__.py'}, page_content='\"\"\"\\nsage_data_client - Official Sage Python data API client.\\n========================================================\\n\\nsage_data_client goals are to make writing queries and working with the results easy. It does this by:\\n\\n* Providing a simple query function which talks to the data API.\\n* Providing the results in an easy to use [Pandas](https://pandas.pydata.org) data frame.\\n\"\"\"\\nfrom .query import query, load\\n'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='import unittest\\nfrom pathlib import Path\\nimport json\\nfrom tempfile import TemporaryDirectory\\nimport time\\nfrom datetime import datetime\\nimport os\\nimport pika\\nimport subprocess\\n\\nfrom waggle.plugin import Plugin, PluginConfig, Uploader, get_timestamp\\nimport wagglemsg\\n\\n# TODO(sean) add integration testing against rabbitmq\\n# TODO(sean) clean up the queue interface. it would be better to not know about the plugin.send / plugin.recv queues explicitly.\\n\\n\\nclass TestPlugin(unittest.TestCase):\\n    def test_publish(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test.int\", 1)\\n            plugin.publish(\"test.float\", 2.0)\\n            plugin.publish(\"test.str\", \"three\")\\n            plugin.publish(\\n                \"cows.total\",\\n                391,\\n                meta={\\n                    \"camera\": \"bottom_left\",\\n                },\\n            )\\n\\n    def test_publish_check_reserved(self):\\n        with Plugin() as plugin:\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"upload\", \"path/to/data\")\\n\\n    def test_get(self):\\n        with Plugin() as plugin:\\n            plugin.subscribe(\"raw.#\")\\n            with self.assertRaises(TimeoutError):\\n                plugin.get(timeout=0)\\n            with self.assertRaises(TimeoutError):\\n                plugin.get(timeout=0.001)\\n\\n            msg = wagglemsg.Message(\"test\", 1.0, 0, {})\\n            plugin.recv.put(msg)\\n            msg2 = plugin.get(timeout=0)\\n            self.assertEqual(msg, msg2)\\n\\n    def test_get_timestamp(self):\\n        ts = get_timestamp()\\n        self.assertIsInstance(ts, int)\\n\\n    def test_valid_values(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test\", 1)\\n            plugin.publish(\"test\", 1.3)\\n            plugin.publish(\"test\", \"some string\")\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", b\"some bytes\")\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", [1, 2, 3])\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", {1: 1, 2: 2, 3: 3})\\n\\n    def test_valid_meta(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test\", 1, meta={\"k\": \"v\"})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": 10})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": 12.3})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": []})\\n\\n    def test_valid_timestamp(self):\\n        with Plugin() as plugin:\\n            # valid int, nanosecond timestamp\\n            plugin.publish(\"test\", 1, timestamp=1649694687904754000)\\n\\n            # must prevent a float type timestamp\\n            ts = datetime(2022, 1, 1, 0, 0, 0).timestamp()\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, timestamp=ts)\\n\\n            # must prevent int timestamp in seconds from being loaded by flagging\\n            # timestamps that are too early.\\n            testcases = [\\n                datetime(2022, 1, 1, 0, 0, 0),\\n                datetime(3000, 1, 1, 0, 0, 0),\\n                datetime(5000, 1, 1, 0, 0, 0),\\n            ]\\n\\n            for dt in testcases:\\n                with self.assertRaises(ValueError):\\n                    plugin.publish(\"test\", 1, timestamp=int(dt.timestamp()))\\n\\n    def test_valid_publish_names(self):\\n        with Plugin() as plugin:\\n            with self.assertRaises(TypeError):\\n                plugin.publish(None, 0)\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"\", 0)\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\".\", 0)\\n\\n            # check for reserved names\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"upload\", 0)\\n\\n            # use _ instead of -\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"my-metric\", 0)\\n            # correct alternative\\n            plugin.publish(\"my_metric\", 0)\\n\\n            # assert len(name) <= 128\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"x\" * 129, 0)\\n\\n            # no empty parts allowed\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"vision.count..bird\", 0)\\n            # correct alternative\\n            plugin.publish(\"vision.count.bird\", 0)\\n\\n            # no spaces allowed\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"sys.cpu temp\", 0)\\n            # correct alternative\\n            plugin.publish(\"sys.cpu_temp\", 0)\\n\\n    # TODO(sean) refactor messaging part to make testing this cleaner\\n    def test_upload_file(self):\\n        with TemporaryDirectory() as tempdir:\\n            pl = Plugin(\\n                PluginConfig(\\n                    host=\"fake-rabbitmq-host\",\\n                    port=5672,\\n                    username=\"plugin\",\\n                    password=\"plugin\",\\n                    app_id=\"0668b12c-0c15-462c-9e06-7239282411e5\",\\n                ),\\n                uploader=Uploader(Path(tempdir, \"uploads\")),\\n            )\\n\\n            data = b\"here some data in a data\"\\n            upload_path = Path(tempdir, \"myfile.txt\")\\n            upload_path.write_bytes(data)\\n\\n            pl.upload_file(upload_path)\\n            item = pl.send.get_nowait()\\n            msg = wagglemsg.load(item.body)\\n            self.assertEqual(item.scope, \"all\")\\n            self.assertEqual(msg.name, \"upload\")\\n            self.assertIsNotNone(msg.timestamp)\\n            self.assertIsInstance(msg.value, str)\\n            self.assertIsNotNone(msg.meta)\\n            self.assertIn(\"filename\", msg.meta)\\n\\n    def test_timeit(self):\\n        with Plugin() as plugin:\\n            with plugin.timeit(\"dur\"):\\n                time.sleep(0.001)\\n            item = plugin.send.get(0.01)\\n            msg = wagglemsg.load(item.body)\\n            self.assertEqual(msg.name, \"dur\")\\n\\n\\nclass TestUploader(unittest.TestCase):\\n    def test_upload_file(self):\\n        with TemporaryDirectory() as tempdir:\\n            uploader = Uploader(Path(tempdir, \"uploads\"))\\n\\n            data = b\"here some data in a data\"\\n\\n            upload_path = Path(tempdir, \"myfile.txt\")\\n            upload_path.write_bytes(data)\\n\\n            path = uploader.upload_file(upload_path)\\n            self.assertFalse(upload_path.exists())\\n\\n            self.assertEqual(data, Path(path, \"data\").read_bytes())\\n            meta = json.loads(Path(path, \"meta\").read_text())\\n            self.assertIn(\"timestamp\", meta)\\n            self.assertIn(\"shasum\", meta)\\n            self.assertEqual(meta[\"labels\"][\"filename\"], upload_path.name)\\n\\n\\ndef rabbitmq_available():\\n    try:\\n        subprocess.check_output([\"docker-compose\", \"exec\", \"rabbitmq\", \"true\"])\\n        return True\\n    except subprocess.CalledProcessError:\\n        return False\\n\\n\\ndef get_admin_connection():\\n    params = pika.ConnectionParameters(\\n        credentials=pika.PlainCredentials(\"admin\", \"admin\")\\n    )\\n    return pika.BlockingConnection(params)\\n\\n\\n@unittest.skipUnless(rabbitmq_available(), \"rabbitmq not available\")\\nclass TestPluginWithRabbitMQ(unittest.TestCase):\\n    def setUp(self):\\n        os.environ[\"WAGGLE_PLUGIN_USERNAME\"] = \"plugin\"\\n        os.environ[\"WAGGLE_PLUGIN_PASSWORD\"] = \"plugin\"\\n        os.environ[\"WAGGLE_PLUGIN_HOST\"] = \"127.0.0.1\"\\n        os.environ[\"WAGGLE_PLUGIN_PORT\"] = \"5672\"\\n\\n        with get_admin_connection() as conn, conn.channel() as ch:\\n            ch.queue_purge(\"to-validator\")\\n\\n    def test_publish(self):\\n        now = time.time_ns()\\n\\n        with Plugin() as publisher:\\n            publisher.publish(\"test\", 123, meta={\"sensor\": \"bme680\"}, timestamp=now)\\n\\n        with get_admin_connection() as conn, conn.channel() as ch:\\n            _, _, body = ch.basic_get(\"to-validator\", auto_ack=True)\\n            msg = wagglemsg.load(body)\\n\\n        self.assertEqual(\\n            msg,\\n            wagglemsg.Message(\\n                name=\"test\",\\n                value=123,\\n                meta={\"sensor\": \"bme680\"},\\n                timestamp=now,\\n            ),\\n        )\\n\\n    def test_subscribe(self):\\n        msg = wagglemsg.Message(\\n            name=\"test\",\\n            value=123,\\n            meta={\"sensor\": \"bme680\"},\\n            timestamp=time.time_ns(),\\n        )\\n\\n        with Plugin() as subscriber:\\n            subscriber.subscribe(\"test\")\\n            time.sleep(1)\\n\\n            with get_admin_connection() as conn, conn.channel() as ch:\\n                ch.basic_publish(\"data.topic\", \"test\", wagglemsg.dump(msg))\\n\\n            msg2 = subscriber.get(1)\\n            self.assertEqual(msg, msg2)\\n\\n\\nclass TestPluginLogDir(unittest.TestCase):\\n    def test_log_dir(self):\\n        import sage_data_client\\n\\n        with TemporaryDirectory() as dir:\\n            dir = Path(dir)\\n\\n            # create dummy upload file\\n            upload_file = Path(dir, \"hello.txt\")\\n            upload_file.write_text(\"hello\")\\n\\n            timestamp = get_timestamp()\\n\\n            # set env var and run plugin\\n            try:\\n                os.environ[\"PYWAGGLE_LOG_DIR\"] = str(dir)\\n                with Plugin() as plugin:\\n                    plugin.publish(\"test\", 123, timestamp=timestamp)\\n                    plugin.publish(\\n                        \"test.with.meta\",\\n                        456,\\n                        meta={\"user\": \"data\"},\\n                        timestamp=timestamp + 10000,\\n                    )\\n                    plugin.upload_file(upload_file, timestamp=timestamp + 20000)\\n            finally:\\n                del os.environ[\"PYWAGGLE_LOG_DIR\"]\\n\\n            df = sage_data_client.load(Path(dir, \"data.ndjson\"))\\n\\n            # ensure records match what was published\\n            self.assertEqual(len(df), 3)\\n\\n            # TODO(sean) test timestamps\\n            self.assertEqual(df.loc[0, \"name\"], \"test\")\\n            self.assertEqual(df.loc[0, \"value\"], 123)\\n\\n            self.assertEqual(df.loc[1, \"name\"], \"test.with.meta\")\\n            self.assertEqual(df.loc[1, \"value\"], 456)\\n            self.assertEqual(df.loc[1, \"meta.user\"], \"data\")\\n\\n            self.assertEqual(df.loc[2, \"name\"], \"upload\")\\n            self.assertEqual(df.loc[2, \"meta.filename\"], \"hello.txt\")\\n\\n            # ensure all uploads exist\\n            for path in df[df.name == \"upload\"].value:\\n                self.assertTrue(Path(path).exists())\\n\\n\\ndef assertDictContainsSubset(t, a, b):\\n    t.assertLessEqual(a.items(), b.items())\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()\\n'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='import unittest\\nfrom waggle.data.audio import AudioFolder, AudioSample\\nfrom waggle.data.vision import RGB, BGR, ImageFolder, ImageSample, resolve_device\\nfrom waggle.data.timestamp import get_timestamp\\nimport numpy as np\\nfrom tempfile import TemporaryDirectory\\nfrom pathlib import Path\\nimport os.path\\nfrom itertools import product\\n\\n\\ndef generate_audio_data(samplerate, channels, dtype):\\n    if dtype == np.float32:\\n        return np.random.uniform(-1, 1, (samplerate, channels)).astype(dtype)\\n    if dtype == np.float64:\\n        return np.random.uniform(-1, 1, (samplerate, channels)).astype(dtype)\\n    if dtype == np.int16:\\n        return np.random.randint(\\n            -(2**15), 2**15, (samplerate, channels), dtype=dtype\\n        )\\n    if dtype == np.int32:\\n        return np.random.randint(\\n            -(2**31), 2**31, (samplerate, channels), dtype=dtype\\n        )\\n    raise ValueError(\"unsupported audio settings\")\\n\\n\\ndef generate_audio_sample(samplerate, channels, dtype):\\n    return AudioSample(generate_audio_data(samplerate, channels, dtype), samplerate, 0)\\n\\n\\nclass TestData(unittest.TestCase):\\n    def test_colors(self):\\n        for fmt in [RGB, BGR]:\\n            data = np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8)\\n            data2 = fmt.format_to_cv2(fmt.cv2_to_format(data))\\n            self.assertTrue(\\n                np.all(np.isclose(data, data2, 1.0)), f\"checking format {fmt}\"\\n            )\\n\\n    def test_resolve_device(self):\\n        self.assertEqual(\\n            resolve_device(Path(\"test.jpg\")), str(Path(\"test.jpg\").absolute())\\n        )\\n        self.assertEqual(\\n            resolve_device(\"file://path/to/test.jpg\"),\\n            str(Path(\"path/to/test.jpg\").absolute()),\\n        )\\n        self.assertEqual(\\n            resolve_device(\"http://camera-ip.org/image.jpg\"),\\n            \"http://camera-ip.org/image.jpg\",\\n        )\\n        self.assertEqual(\\n            resolve_device(\"rtsp://camera-ip.org/image.jpg\"),\\n            \"rtsp://camera-ip.org/image.jpg\",\\n        )\\n        self.assertEqual(resolve_device(0), 0)\\n\\n    def test_image_save(self):\\n        with TemporaryDirectory() as dir:\\n            sample = ImageSample(\\n                np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8), 0, RGB\\n            )\\n            for fmt in [\"jpg\", \"png\"]:\\n                name = f\"sample.{fmt}\"\\n                sample.save(os.path.join(dir, name))\\n                sample.save(Path(dir, name))\\n\\n    def test_image_save_load(self):\\n        with TemporaryDirectory() as dir:\\n            sample = ImageSample(\\n                np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8), 0, RGB\\n            )\\n            sample.save(Path(dir, \"sample.png\"))\\n            samples = ImageFolder(dir, RGB)\\n            self.assertTrue(np.allclose(sample.data, samples[0].data))\\n\\n    def test_audio_save(self):\\n        test_formats = [\"wav\", \"flac\"]\\n        test_samplerates = [22050, 44100, 48000]\\n        test_channels = [1, 2]\\n        test_dtypes = [np.float32, np.float64, np.int16, np.int32]\\n\\n        for format, samplerate, channels, dtype in product(\\n            test_formats, test_samplerates, test_channels, test_dtypes\\n        ):\\n            name = f\"sample.{format}\"\\n            with TemporaryDirectory() as dir:\\n                sample = generate_audio_sample(\\n                    samplerate, channels=channels, dtype=dtype\\n                )\\n                # test saving as any PathLike\\n                sample.save(os.path.join(dir, name))\\n                sample.save(Path(dir, name))\\n\\n    def test_audio_save_load(self):\\n        test_formats = [\"wav\", \"flac\"]\\n        test_samplerates = [22050, 44100, 48000]\\n        test_channels = [1, 2]\\n        test_dtypes = [np.float32, np.float64]\\n\\n        for format, samplerate, channels, dtype in product(\\n            test_formats, test_samplerates, test_channels, test_dtypes\\n        ):\\n            name = f\"sample.{format}\"\\n            with TemporaryDirectory() as dir:\\n                sample = generate_audio_sample(\\n                    samplerate, channels=channels, dtype=dtype\\n                )\\n                sample.save(Path(dir, name))\\n                samples = AudioFolder(dir)\\n                self.assertTrue(\\n                    np.allclose(sample.data, samples[0].data, atol=1e-4),\\n                    msg=f\"failed: format={format} samplerate={samplerate} channels={channels} dtypes={dtype}\",\\n                )\\n\\n    def test_get_timestamp(self):\\n        ts = get_timestamp()\\n        self.assertIsInstance(ts, int)\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/__init__.py'}, page_content=''),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/time.py'}, page_content=\"import time\\n\\n# BUG This *must* be addressed with the behavior written up in the plugin spec.\\n# We don't want any surprises in terms of accuraccy\\ntry:\\n    from time import time_ns as get_timestamp\\nexcept ImportError:\\n\\n    def get_timestamp():\\n        return int(time.time() * 1e9)\\n\\n\\n# NOTE to preserve the best accuracy, we implement the backwards compatible perf\\n# counter by only abstracting how to measure the duration between two times in\\n# nanoseconds\\ntry:\\n    from time import perf_counter_ns as timeit_perf_counter\\n\\n    def timeit_perf_counter_duration(start, finish):\\n        return finish - start\\n\\nexcept ImportError:\\n    from time import perf_counter as timeit_perf_counter\\n\\n    def timeit_perf_counter_duration(start, finish):\\n        return int((finish - start) * 1e9)\\n\"),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/config.py'}, page_content='from typing import NamedTuple\\n\\n\\nclass PluginConfig(NamedTuple):\\n    \"\"\"\\n    PluginConfig represents the config required to setup and run a Plugin.\\n    \"\"\"\\n\\n    # TODO generalize to support different backends\\n    username: str\\n    password: str\\n    host: str\\n    port: int\\n    app_id: str\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/uploader.py'}, page_content='import hashlib\\nimport json\\nfrom pathlib import Path\\nfrom shutil import copyfile\\nfrom .time import get_timestamp\\n\\n\\nclass Uploader:\\n    def __init__(self, root):\\n        self.root = Path(root)\\n\\n    # NOTE uploads are stored in the following directory structure:\\n    # root/\\n    #   timestamp-sha1sum/\\n    #     data\\n    #     meta\\n    def upload_file(self, path, meta={}, timestamp=None, keep=False):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n\\n        path = Path(path)\\n        checksum = sha1sum_for_file(path)\\n\\n        # create upload dir\\n        upload_dir = Path(self.root, f\"{timestamp}-{checksum}\")\\n        upload_dir.mkdir(parents=True, exist_ok=True)\\n\\n        # stage data file\\n        # NOTE we do a copy instead of move, as the upload dir may\\n        # be mounted from another disk.\\n        copyfile(path, Path(upload_dir, \"data\"))\\n        if not keep:\\n            path.unlink()\\n\\n        # stage meta file\\n        metafile = {\\n            \"timestamp\": timestamp,\\n            \"shasum\": checksum,\\n            \"labels\": {k: v for k, v in meta.items()},\\n        }\\n        metafile[\"labels\"][\"filename\"] = path.name\\n        write_json_file(Path(upload_dir, \"meta\"), metafile)\\n\\n        return upload_dir\\n\\n\\ndef sha1sum_for_file(path):\\n    h = hashlib.sha1()\\n    with open(path, \"rb\") as f:\\n        while True:\\n            chunk = f.read(32768)\\n            if chunk == b\"\":\\n                break\\n            h.update(chunk)\\n    return h.hexdigest()\\n\\n\\ndef write_json_file(path, obj):\\n    with open(path, \"w\") as f:\\n        json.dump(obj, f, separators=(\",\", \":\"), sort_keys=True)\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/__init__.py'}, page_content='from .config import PluginConfig\\nfrom .plugin import Plugin\\nfrom .uploader import Uploader\\nfrom .time import get_timestamp\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='import logging\\nfrom threading import Thread, Event\\nfrom queue import Queue, Empty\\nimport time\\nimport pika\\nimport pika.exceptions\\nimport wagglemsg\\nfrom .config import PluginConfig\\n\\n\\nlogger = logging.getLogger(__name__)\\n# pika is very verbose at DEBUG level. we turn it down here.\\nlogging.getLogger(\"pika\").setLevel(logging.CRITICAL)\\n\\n\\nclass RabbitMQPublisher:\\n    \"\"\"\\n    RabbitMQPublisher manages a connection to RabbitMQ and publishes messages from the provided queue.\\n\\n    This is done in a background thread which must be stopped by setting the provided stop Event.\\n    \"\"\"\\n\\n    def __init__(self, config: PluginConfig, messages: Queue, stop: Event):\\n        self.config = config\\n        self.params = get_connection_parameters_for_config(config)\\n        self.messages = messages\\n        self.stop = stop\\n        self.done = Event()\\n        Thread(target=self.__main).start()\\n\\n    def __main(self):\\n        logger.debug(\"publisher started.\")\\n        try:\\n            while not self.stop.is_set():\\n                try:\\n                    self.__connect_and_flush_messages()\\n                except Exception:\\n                    if logger.isEnabledFor(logging.DEBUG):\\n                        logger.exception(\"__connect_and_flush_messages exception\")\\n                    time.sleep(1)\\n        finally:\\n            self.done.set()\\n            logger.debug(\"publisher stopped.\")\\n\\n    def __connect_and_flush_messages(self):\\n        logger.debug(\"publisher connecting to rabbitmq...\")\\n        with pika.BlockingConnection(self.params) as conn, conn.channel() as ch:\\n            while not self.stop.is_set():\\n                self.__flush_messages(ch)\\n            logger.debug(\"publisher stopping...\")\\n            # attempt to flush any remaining messages\\n            self.__flush_messages(ch)\\n\\n    def __flush_messages(self, ch):\\n        while True:\\n            try:\\n                logger.debug(\"publisher checking for message...\")\\n                item = self.messages.get(timeout=1)\\n            except Empty:\\n                return\\n\\n            properties = pika.BasicProperties(\\n                delivery_mode=2, user_id=self.params.credentials.username\\n            )\\n\\n            # NOTE app_id is used by data service to validate and tag additional metadata provided by k3s scheduler.\\n            if self.config.app_id != \"\":\\n                properties.app_id = self.config.app_id\\n\\n            if logger.isEnabledFor(logging.DEBUG):\\n                logger.debug(\\n                    \"publishing message to rabbitmq: %s\", wagglemsg.load(item.body)\\n                )\\n\\n            try:\\n                ch.basic_publish(\\n                    exchange=\"to-validator\",\\n                    routing_key=item.scope,\\n                    properties=properties,\\n                    body=item.body,\\n                )\\n            except Exception:\\n                if logger.isEnabledFor(logging.DEBUG):\\n                    logger.exception(\\n                        \"basic_publish to rabbitmq failed. will requeue message...\"\\n                    )\\n                # requeue message so we can again later\\n                # NOTE(sean) this will reorder messages. if we realized we *must* preserve message\\n                # order, we must to change this to avoid subtle bugs!\\n                self.messages.put(item)\\n                # propagate error up to trigger reconnect\\n                raise\\n\\n\\nclass RabbitMQConsumer:\\n    \"\"\"\\n    RabbitMQConsumer manages a connection to RabbitMQ and puts received messages into the provided queue.\\n\\n    This is done in a background thread which must be stopped by setting the provided stop Event.\\n    \"\"\"\\n\\n    def __init__(self, topics, config: PluginConfig, messages: Queue, stop: Event):\\n        self.topics = topics\\n        self.config = config\\n        self.params = get_connection_parameters_for_config(config)\\n        self.messages = messages\\n        self.stop = stop\\n        self.done = Event()\\n        Thread(target=self.__main).start()\\n\\n    def __main(self):\\n        logger.debug(\"consumer started.\")\\n        try:\\n            while not self.stop.is_set():\\n                try:\\n                    self.__connect_and_consume_messages()\\n                except Exception:\\n                    if logger.isEnabledFor(logging.DEBUG):\\n                        logger.exception(\"__connect_and_consume_messages exception\")\\n                    time.sleep(1)\\n        finally:\\n            self.done.set()\\n            logger.debug(\"consumer stopped.\")\\n\\n    def __connect_and_consume_messages(self):\\n        logger.debug(\"consumer connecting to rabbitmq...\")\\n        with pika.BlockingConnection(self.params) as conn, conn.channel() as ch:\\n            # setup subscriber queue and bind to topics\\n            queue = ch.queue_declare(\"\", exclusive=True).method.queue\\n            ch.basic_consume(queue, self.__process_message, auto_ack=True)\\n\\n            for topic in self.topics:\\n                ch.queue_bind(queue, \"data.topic\", topic)\\n                logger.debug(\"consumer binding queue %s to topic %s\", queue, topic)\\n\\n            def check_stop():\\n                if self.stop.is_set():\\n                    logger.debug(\"consumer stopping...\")\\n                    ch.stop_consuming()\\n                else:\\n                    conn.call_later(1, check_stop)\\n\\n            conn.call_later(1, check_stop)\\n            logger.debug(\"consumer start processing messages...\")\\n            ch.start_consuming()\\n\\n    def __process_message(self, ch, method, properties, body):\\n        try:\\n            logger.debug(\"consumer processing message %s...\", body)\\n            msg = wagglemsg.load(body)\\n        except TypeError:\\n            logger.debug(\"unsupported message type: %s %s\", properties, body)\\n            return\\n        logger.debug(\"consumer putting message in waiting queue\")\\n        self.messages.put(msg)\\n\\n\\ndef get_connection_parameters_for_config(\\n    config: PluginConfig,\\n) -> pika.ConnectionParameters:\\n    return pika.ConnectionParameters(\\n        host=config.host,\\n        port=config.port,\\n        credentials=pika.PlainCredentials(\\n            username=config.username,\\n            password=config.password,\\n        ),\\n        connection_attempts=1,\\n        socket_timeout=1.0,\\n    )\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='import logging\\nimport re\\nimport wagglemsg\\n\\nfrom contextlib import contextmanager\\nfrom datetime import datetime\\nfrom os import getenv\\nfrom pathlib import Path\\nfrom queue import Queue, Empty\\nfrom threading import Event\\nfrom typing import NamedTuple\\n\\nfrom .config import PluginConfig\\nfrom .rabbitmq import RabbitMQPublisher, RabbitMQConsumer\\nfrom .time import get_timestamp, timeit_perf_counter, timeit_perf_counter_duration\\nfrom .uploader import Uploader\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass PublishData(NamedTuple):\\n    scope: str\\n    body: bytes\\n\\n\\n# Nanoseconds since epoch for 2000-01-01T00:00:00Z\\nMIN_TIMESTAMP_NS = 946706400000000000\\n\\n\\nclass FilesystemPublisher:\\n    def __init__(self, root):\\n        self.root = Path(root)\\n        self.root.mkdir(parents=True, exist_ok=True)\\n        self.datafile = Path(root, \"data.ndjson\").open(\"a\")\\n        self.uploads_dir = Path(root, \"uploads\")\\n        self.uploads_dir.mkdir(parents=True, exist_ok=True)\\n\\n    def close(self):\\n        self.datafile.close()\\n\\n    def publish(self, msg: wagglemsg.Message):\\n        import json\\n\\n        out = {\\n            \"name\": msg.name,\\n            \"value\": msg.value,\\n            \"meta\": msg.meta,\\n            # python doesn\\'t have builtin support for nanosecond\\n            \"timestamp\": isoformat_time_ns(msg.timestamp),\\n        }\\n        print(\\n            json.dumps(out, sort_keys=True, separators=(\",\", \":\")),\\n            file=self.datafile,\\n            flush=True,\\n        )\\n\\n    def upload_file(self, path, timestamp, meta):\\n        from shutil import copyfile\\n\\n        src = Path(path)\\n        dst = Path(self.uploads_dir, f\"{timestamp}-{src.name}\")\\n        copyfile(src, dst)\\n        meta = meta.copy()\\n        meta[\"filename\"] = Path(src).name\\n        self.publish(\\n            wagglemsg.Message(\\n                name=\"upload\",\\n                value=str(dst.absolute()),\\n                meta=meta,\\n                timestamp=timestamp,\\n            )\\n        )\\n\\n\\ndef isoformat_time_ns(ns: int) -> str:\\n    # python doesn\\'t have builtin support for nanosecond timestamps and formatting, so we provide\\n    # a backfill for it. this is only intended to be used in the run log for testing.\\n    nanostr = f\"{ns%1000:03d}\"\\n    return datetime.fromtimestamp(ns / 1e9).isoformat() + nanostr\\n\\n\\nclass Plugin:\\n    \"\"\"\\n    Plugin provides methods to publish and consume messages inside the Waggle ecosystem.\\n\\n    Examples\\n    --------\\n\\n    The simplest example is creating a Plugin and publishing a message. This can be done using:\\n\\n    ```python\\n    from waggle.plugin import Plugin\\n\\n    with Plugin() as plugin:\\n        plugin.publish(\"test_value\", 99)\\n    ```\\n    \"\"\"\\n\\n    def __init__(\\n        self, config=None, uploader=None, file_publisher: FilesystemPublisher = None\\n    ):\\n        self.config = config or get_default_plugin_config()\\n        self.uploader = uploader or get_default_plugin_uploader()\\n        self.send = Queue()\\n        self.recv = Queue()\\n        self.stop = Event()\\n        self.tasks = []\\n\\n        # TODO(sean) can we use ExitStack to clean up???\\n\\n        self.file_publisher = file_publisher\\n\\n        if self.file_publisher is None and getenv(\"PYWAGGLE_LOG_DIR\") is not None:\\n            self.file_publisher = FilesystemPublisher(getenv(\"PYWAGGLE_LOG_DIR\"))\\n\\n    def __enter__(self):\\n        self.tasks.append(RabbitMQPublisher(self.config, self.send, self.stop))\\n        return self\\n\\n    def __exit__(self, exc_type, exc_value, exc_traceback):\\n        self.stop.set()\\n\\n        if self.file_publisher is not None:\\n            self.file_publisher.close()\\n\\n        for task in self.tasks:\\n            task.done.wait()\\n\\n    def subscribe(self, *topics):\\n        self.tasks.append(RabbitMQConsumer(topics, self.config, self.recv, self.stop))\\n        # TODO(sean) add mock or integration testing against rabbitmq to actually test this\\n\\n    def get(self, timeout=None):\\n        try:\\n            return self.recv.get(timeout=timeout)\\n        except Empty:\\n            pass\\n        raise TimeoutError(\"plugin get timed out\")\\n\\n    def publish(self, name, value, meta={}, timestamp=None, scope=\"all\", timeout=None):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n        raise_for_invalid_publish_name(name)\\n        self.__publish(name, value, meta, timestamp, scope, timeout)\\n\\n    # NOTE __publish is used internally by publish and upload_file to do an unchecked\\n    # message publish. the main reason this exists is to guard against reserved names\\n    # like \"upload\" in publish but still allow upload_file to use it.\\n    def __publish(self, name, value, meta, timestamp, scope=\"all\", timeout=None):\\n        if not isinstance(value, (int, float, str)):\\n            raise TypeError(\"Value must be an int, float or str.\")\\n        if not isinstance(timestamp, int):\\n            raise TypeError(\\n                \"Timestamp must be an int and have units of nanoseconds since epoch. Please see the documentation for more information on setting timestamps.\"\\n            )\\n        if timestamp < MIN_TIMESTAMP_NS:\\n            raise ValueError(\\n                \"Timestamp probably has wrong units and is being processed as before 2000-01-01T00:00:00Z. Timestamp must have units of nanoseconds since epoch. Please see the documentation for more information on setting timestamps.\"\\n            )\\n        if not valid_meta(meta):\\n            raise TypeError(\"Meta must be a dictionary of strings to strings.\")\\n        msg = wagglemsg.Message(name=name, value=value, timestamp=timestamp, meta=meta)\\n\\n        # hack to use file publisher for everything except uploads\\n        if self.file_publisher is not None and name != \"upload\":\\n            self.file_publisher.publish(msg)\\n\\n        logger.debug(\"adding message to outgoing queue: %s\", msg)\\n        self.send.put(PublishData(scope, wagglemsg.dump(msg)), timeout=timeout)\\n\\n    def upload_file(self, path, meta={}, timestamp=None, keep=False):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n\\n        if self.file_publisher is not None:\\n            self.file_publisher.upload_file(path, meta=meta, timestamp=timestamp)\\n\\n        if self.uploader is not None:\\n            meta = meta.copy()\\n            meta[\"filename\"] = Path(path).name\\n            upload_path = self.uploader.upload_file(\\n                path=path, meta=meta, timestamp=timestamp, keep=keep\\n            )\\n            self.__publish(\"upload\", upload_path.name, meta, timestamp)\\n\\n    @contextmanager\\n    def timeit(self, name):\\n        logger.debug(\"starting timeit block %s\", name)\\n        start = timeit_perf_counter()\\n        yield\\n        finish = timeit_perf_counter()\\n        duration = timeit_perf_counter_duration(start, finish)\\n        self.publish(name, duration)\\n        logger.debug(\"finished timeit block %s\", name)\\n\\n\\ndef get_default_plugin_config() -> PluginConfig:\\n    return PluginConfig(\\n        username=getenv(\"WAGGLE_PLUGIN_USERNAME\", \"plugin\"),\\n        password=getenv(\"WAGGLE_PLUGIN_PASSWORD\", \"plugin\"),\\n        host=getenv(\"WAGGLE_PLUGIN_HOST\", \"rabbitmq\"),\\n        port=int(getenv(\"WAGGLE_PLUGIN_PORT\", 5672)),\\n        app_id=getenv(\"WAGGLE_APP_ID\", \"\"),\\n    )\\n\\n\\ndef valid_meta(meta):\\n    return isinstance(meta, dict) and all(isinstance(v, str) for v in meta.values())\\n\\n\\ndef get_default_plugin_uploader():\\n    if (\\n        getenv(\"WAGGLE_PLUGIN_UPLOAD_PATH\") is None\\n        and getenv(\"PYWAGGLE_LOG_DIR\") is not None\\n    ):\\n        return None\\n    return Uploader(Path(getenv(\"WAGGLE_PLUGIN_UPLOAD_PATH\", \"/run/waggle/uploads\")))\\n\\n\\npublish_name_part_pattern = re.compile(\"^[a-z0-9_]+$\")\\n\\n\\ndef raise_for_invalid_publish_name(s: str):\\n    if not isinstance(s, str):\\n        raise TypeError(f\"publish name must be a string: {s!r}\")\\n    if len(s) > 128:\\n        raise ValueError(f\"publish must be at most 128 characters: {s!r}\")\\n    if s == \"upload\":\\n        raise ValueError(f\"name {s!r} is reserved for system use only\")\\n    parts = s.split(\".\")\\n    for p in parts:\\n        if not publish_name_part_pattern.match(p):\\n            raise ValueError(\\n                f\"publish name invalid: {s!r} part: {p!r} (names must consist of [a-z0-9_] and may be joined by .)\"\\n            )\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='import cv2\\nfrom pathlib import Path\\nimport numpy\\nfrom typing import Union\\nimport os\\nfrom os import PathLike\\nimport random\\nimport json\\nimport re\\nimport threading\\nimport time\\nfrom base64 import b64encode\\nfrom .timestamp import get_timestamp\\nfrom shutil import which\\nimport ffmpeg\\nimport logging\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass BGR:\\n    @classmethod\\n    def cv2_to_format(cls, data):\\n        return data\\n\\n    @classmethod\\n    def format_to_cv2(cls, data):\\n        return data\\n\\n\\nclass RGB:\\n    @classmethod\\n    def cv2_to_format(cls, data):\\n        return cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\\n\\n    @classmethod\\n    def format_to_cv2(cls, data):\\n        return cv2.cvtColor(data, cv2.COLOR_RGB2BGR)\\n\\n\\nWAGGLE_DATA_CONFIG_PATH = Path(\\n    os.environ.get(\"WAGGLE_DATA_CONFIG_PATH\", \"/run/waggle/data-config.json\")\\n)\\n\\n\\ndef read_device_config(path):\\n    config = json.loads(Path(path).read_text())\\n    return {\\n        section[\"match\"][\"id\"]: section\\n        for section in config\\n        if \"id\" in section[\"match\"]\\n    }\\n\\n\\n# TODO use format spec like rgb vs bgr in config file\\nclass ImageSample:\\n    data: numpy.ndarray\\n    timestamp: int\\n    format: Union[BGR, RGB]\\n\\n    def __init__(self, data, timestamp, format):\\n        self.format = format\\n        self.data = self.format.cv2_to_format(data)\\n        self.timestamp = timestamp\\n\\n    def save(self, path: PathLike):\\n        path = Path(path)\\n        data = self.format.format_to_cv2(self.data)\\n        cv2.imwrite(str(path), data)\\n\\n    def _repr_html_(self):\\n        data = self.format.format_to_cv2(self.data)\\n        ok, buf = cv2.imencode(\".png\", data)\\n        if not ok:\\n            raise RuntimeError(\"could not encode image\")\\n        b64data = b64encode(buf.ravel()).decode()\\n        return f\\'<img src=\"data:image/png;base64,{b64data}\" />\\'\\n\\n\\nclass VideoSample:\\n    path: str\\n    timestamp: int\\n\\n    def __init__(self, path, timestamp, format=RGB):\\n        self.format = format\\n        self.path = path\\n        self.timestamp = timestamp\\n        self.capture = None\\n\\n    def __enter__(self):\\n        self.capture = cv2.VideoCapture(self.path)\\n        if not self.capture.isOpened():\\n            raise RuntimeError(\\n                f\"unable to open video capture for file {self.path!r}\"\\n            )\\n        self.fps = self.capture.get(cv2.CAP_PROP_FPS)\\n        if self.fps > 100.:\\n            self.fps = 0.\\n            logger.debug(f\\'pywaggle cannot calculate timestamp because the fps ({self.fps}) is too high.\\')\\n            self.timestamp_delta = 0\\n        else:\\n            self.timestamp_delta = 1 / self.fps\\n        self._frame_count = 0\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        if self.capture.isOpened():\\n            self.capture.release()\\n\\n    def __iter__(self):\\n        self._frame_count = 0\\n        return self\\n\\n    def __next__(self):\\n        if self.capture == None or not self.capture.isOpened():\\n            raise RuntimeError(\"video is not opened. use the Python WITH statement to open the video\")\\n        ok, data = self.capture.read()\\n        if not ok or data is None:\\n            raise StopIteration\\n        # timestamp must be an integer in nanoseconds\\n        approx_timestamp = self.timestamp + int(self.timestamp_delta * self._frame_count * 1e9)\\n        self._frame_count += 1\\n        return ImageSample(data=data, timestamp=approx_timestamp, format=self.format)\\n\\n\\ndef resolve_device(device):\\n    if isinstance(device, Path):\\n        return resolve_device_from_path(device)\\n    # objects that are not paths or strings are considered already resolved\\n    if not isinstance(device, str):\\n        return device\\n    match = re.match(r\"([A-Za-z0-9]+)://(.*)$\", device)\\n    # non-url like paths refer to data shim devices\\n    if match is None:\\n        return resolve_device_from_data_config(device)\\n    # return file:// urls as path\\n    if match.group(1) == \"file\":\\n        return resolve_device_from_path(Path(match.group(2)))\\n    # return other urls as-is\\n    return device\\n\\n\\ndef resolve_device_from_path(path):\\n    return str(path.absolute())\\n\\n\\ndef resolve_device_from_data_config(device):\\n    config = read_device_config(WAGGLE_DATA_CONFIG_PATH)\\n    section = config.get(device)\\n    if section is None:\\n        raise KeyError(f\"no device found {device!r}\")\\n    try:\\n        return section[\"handler\"][\"args\"][\"url\"]\\n    except KeyError:\\n        raise KeyError(f\"missing .handler.args.url field for device {device!r}.\")\\n\\nclass Camera:\\n    INPUT_TYPE_FILE = \"file\"\\n    INPUT_TYPE_OTHER = \"other\"\\n\\n    def __init__(self, device=0, format=RGB):\\n        self.capture = _Capture(resolve_device(device), format)\\n        match = re.match(r\"([A-Za-z0-9]+)://(.*)$\", device)\\n        if match is not None and match.group(1) == \"file\":\\n            self.input_type = self.INPUT_TYPE_FILE\\n        else:\\n            self.input_type = self.INPUT_TYPE_OTHER\\n\\n    def __enter__(self):\\n        if self.input_type == self.INPUT_TYPE_FILE:\\n            logger.info(f\\'input is a file. the background thread disabled for grabbing frames\\')\\n            self.capture.enable_daemon = False\\n        else:\\n            self.capture.enable_daemon = True\\n        self.capture.__enter__()\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.capture.__exit__(exc_type, exc_val, exc_tb)\\n\\n    def snapshot(self):\\n        with self.capture:\\n            return self.capture.snapshot()\\n\\n    def stream(self):\\n        with self.capture:\\n            yield from self.capture.stream()\\n\\n    def record(self, duration, file_path=\"./sample.mp4\", skip_second=1):\\n        return self.capture.record(duration, file_path, skip_second)\\n\\n\\nclass _Capture:\\n    def __init__(self, device, format):\\n        self.device = device\\n        self.format = format\\n        self.context_depth = 0\\n        self.enable_daemon = False\\n        self.daemon_need_to_stop = threading.Event()\\n        self._ready_for_next_frame = threading.Event()\\n        self.daemon = threading.Thread(target=self._run, daemon=True)\\n        self.lock = threading.Lock()\\n\\n    def __enter__(self):\\n        if self.context_depth == 0:\\n            self.capture = cv2.VideoCapture(self.device)\\n            if not self.capture.isOpened():\\n                raise RuntimeError(\\n                    f\"unable to open video capture for device {self.device!r}\"\\n                )\\n            # spin up a thread to keep up with the camera frame rate\\n            if self.enable_daemon:\\n                self.daemon_need_to_stop.clear()\\n                self.daemon.start()\\n        self.context_depth += 1\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.context_depth -= 1\\n        if self.context_depth == 0:\\n            if self.enable_daemon:\\n                self.daemon_need_to_stop.set()\\n            self.capture.release()\\n    \\n    def _run(self):\\n        # we sleep slighly shorter than FPS to drain the buffer efficiently\\n        # NOTE: OpenCV\\'s FPS get function is inaccurate as a USB webcam gives 1 FPS while\\n        #       a RTSP stream returns 180000. none of them are correct. therefore, we cannot\\n        #       decide the sleep time based on obtained FPS\\n        # fps = self.capture.get(cv2.CAP_PROP_FPS)\\n        sleep = 0.01\\n        # if fps > 0 and fps < 100:\\n        #    sleep = 1 / (fps + 1)\\n        # logging.debug(f\\'camera FPS is {fps}. the background thread sleeps {sleep} seconds in between grab()\\')\\n        while not self.daemon_need_to_stop.is_set():\\n            try:\\n                self.lock.acquire()\\n                ok = self.capture.grab()\\n                if not ok:\\n                    raise RuntimeError(\"failed to grab a frame\")\\n                self.timestamp = get_timestamp()\\n            finally:\\n                self.lock.release()\\n            self._ready_for_next_frame.set()\\n            time.sleep(sleep)\\n\\n    def grab_frame(self):\\n        if self.daemon.is_alive():\\n            if not self._ready_for_next_frame.wait(timeout=10.):\\n                raise RuntimeError(\"failed to grab a frame from the background thread: timed out\")\\n            self._ready_for_next_frame.clear()\\n            try:\\n                self.lock.acquire(timeout=1)\\n                timestamp = self.timestamp\\n                ok, data = self.capture.retrieve()\\n                if not ok:\\n                    raise RuntimeError(\"failed to retrieve the taken snapshot\")\\n            finally:\\n                self.lock.release()\\n            return ImageSample(data=data, timestamp=timestamp, format=self.format)\\n        else:\\n            ok = self.capture.grab()\\n            if not ok:\\n                raise RuntimeError(\"failed to take a snapshot\")\\n            timestamp = get_timestamp()\\n            ok, data = self.capture.retrieve()\\n            if not ok:\\n                raise RuntimeError(\"failed to retrieve the taken snapshot\")\\n            return ImageSample(data=data, timestamp=timestamp, format=self.format)\\n\\n    def snapshot(self):\\n        return self.grab_frame()\\n\\n    def stream(self):\\n        try:\\n            while True:\\n                yield self.grab_frame()\\n        except:\\n            pass\\n\\n    def record(self, duration, file_path=\"./sample.mp4\", skip_second=1):\\n        if which(\"ffmpeg\") == None:\\n            raise RuntimeError(\"ffmpeg does not exist to record video. please install ffmpeg\")\\n        if self.context_depth > 0:\\n            raise RuntimeError(f\\'the stream {self.device} is already open. please close first or use without the Python\\\\\\'s WITH statement\\')\\n        if isinstance(self.device, str) and self.device.startswith(\"rtsp\"):\\n            c = ffmpeg.input(self.device, rtsp_transport=\"tcp\", ss=skip_second)\\n        else:\\n            c = ffmpeg.input(self.device, ss=skip_second)\\n        c = ffmpeg.output(c, file_path, codec=\"copy\", f=\\'mp4\\', t=duration).overwrite_output()\\n        timestamp = get_timestamp()\\n        _, stderr = ffmpeg.run(c, quiet=True)\\n        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\\n            return VideoSample(path=file_path, timestamp=timestamp)\\n        else:\\n            raise RuntimeError(f\\'error while recording: {stderr}\\')\\n        \\n\\n\\nclass ImageFolder:\\n    available_formats = {\".jpg\", \".jpeg\", \".png\"}\\n\\n    def __init__(self, root, format=RGB, shuffle=False):\\n        self.files = sorted(\\n            p.absolute()\\n            for p in Path(root).glob(\"*\")\\n            if p.suffix in self.available_formats\\n        )\\n        self.format = format\\n        if shuffle:\\n            random.shuffle(self.files)\\n\\n    def __len__(self):\\n        return len(self.files)\\n\\n    def __getitem__(self, i):\\n        data = cv2.imread(str(self.files[i]))\\n        timestamp = Path(self.files[i]).stat().st_mtime_ns\\n        return ImageSample(data=data, timestamp=timestamp, format=self.format)\\n\\n    def __repr__(self):\\n        return f\"ImageFolder{self.files!r}\"\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/__init__.py'}, page_content='# Maintaining backwards compatibility for now.\\nfrom .data_shim import open_data_source\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/audio.py'}, page_content='from os import PathLike\\nfrom pathlib import Path\\nimport numpy\\nimport soundfile\\nfrom typing import NamedTuple\\nimport random\\nfrom base64 import b64encode\\nfrom io import BytesIO\\nfrom .timestamp import get_timestamp\\n\\n\\nclass AudioSample(NamedTuple):\\n    data: numpy.ndarray\\n    samplerate: int\\n    timestamp: int\\n\\n    def save(self, path: PathLike):\\n        path = Path(path)\\n        soundfile.write(str(path), self.data, self.samplerate)\\n\\n    def _repr_html_(self):\\n        with BytesIO() as buf:\\n            soundfile.write(\\n                buf, self.data, self.samplerate, format=\"flac\", closefd=False\\n            )\\n            b64data = b64encode(buf.getvalue()).decode()\\n        return f\"\"\"\\n<audio controls=\"controls\" autobuffer=\"autobuffer\">\\n<source src=\"data:audio/wav;base64,{b64data}\" />\\n</audio>\\n\"\"\"\\n\\n\\nclass Microphone:\\n    def __init__(self, samplerate=48000, channels=1, name=None):\\n        import soundcard\\n\\n        self.microphone = soundcard.default_microphone()\\n        self.samplerate = samplerate\\n        self.channels = channels\\n        self.name = name\\n\\n    def record(self, duration):\\n        timestamp = get_timestamp()\\n        data = self.microphone.record(\\n            samplerate=self.samplerate,\\n            numframes=int(duration * self.samplerate),\\n            channels=self.channels,\\n        )\\n        return AudioSample(data, self.samplerate, timestamp=timestamp)\\n\\n\\nclass AudioFolder:\\n    available_formats = {\".\" + s.lower() for s in soundfile.available_formats().keys()}\\n\\n    def __init__(self, root, shuffle=False):\\n        self.files = sorted(\\n            p.absolute()\\n            for p in Path(root).glob(\"*\")\\n            if p.suffix in self.available_formats\\n        )\\n        if shuffle:\\n            random.shuffle(self.files)\\n\\n    def __len__(self):\\n        return len(self.files)\\n\\n    def __getitem__(self, i):\\n        data, samplerate = soundfile.read(str(self.files[i]), always_2d=True)\\n        timestamp = Path(self.files[i]).stat().st_mtime_ns\\n        return AudioSample(data, samplerate, timestamp=timestamp)\\n\\n    def __repr__(self):\\n        return f\"AudioFolder{self.files!r}\"\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/timestamp.py'}, page_content='try:\\n    from time import time_ns as get_timestamp\\nexcept ImportError:\\n    from time import time\\n\\n    def get_timestamp():\\n        return int(time() * 1e9)\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='import logging\\nimport numpy as np\\nfrom urllib.request import urlopen\\nfrom threading import Thread, Event\\nfrom queue import Queue, Empty, Full\\nimport time\\nimport os\\nimport socket\\nfrom pathlib import Path\\nimport json\\nimport random\\nimport re\\n\\nlogger = logging.getLogger(__name__)\\n\\ntry:\\n    import cv2\\nexcept ImportError:\\n    logger.warning(\\n        \"cv2 module not found. pywaggle requires this to capture image and video data.\"\\n    )\\n\\n\\n# BUG This *must* be addressed with the behavior written up in the plugin spec.\\n# We don\\'t want any surprises in terms of accuraccy\\ntry:\\n    from time import time_ns\\nexcept ImportError:\\n    logger.warning(\"using backwards compatible implementation of time_ns\")\\n\\n    def time_ns():\\n        return int(time.time() * 1e9)\\n\\n\\ndef cvtColor(bgr_img, pixel_format=\"rgb\"):\\n    if pixel_format == \"rgb\":\\n        return cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\\n    return bgr_img\\n\\n\\nclass ImageHandler:\\n    def __init__(self, query, url, pixel_format=\"rgb\"):\\n        self.url = url\\n        self.pixel_format = pixel_format\\n\\n    def get(self, timeout=None):\\n        try:\\n            with urlopen(self.url, timeout=timeout) as f:\\n                data = f.read()\\n                ts = time_ns()\\n                arr = np.frombuffer(data, np.uint8)\\n                bgr_img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\\n                return ts, cvtColor(bgr_img, self.pixel_format)\\n        except socket.timeout:\\n            raise TimeoutError(\"get timed out\")\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, *exc):\\n        pass\\n\\n\\ndef video_worker(handler):\\n    try:\\n        while not handler.quit.is_set():\\n            ok, bgr_img = handler.cap.read()\\n            if not ok:\\n                break\\n            img = cvtColor(bgr_img, handler.pixel_format)\\n            item = (time_ns(), img)\\n\\n            # attempt to add an item to the queue\\n            try:\\n                handler.queue.put_nowait(item)\\n                continue\\n            except Full:\\n                logger.debug(\"video frame queue full. evicting oldest frame...\")\\n            # evict an item from queue\\n            try:\\n                handler.queue.get_nowait()\\n            except Empty:\\n                pass\\n            # queue should have space to add now. (assuming this\\n            # is the only producer adding to this queue)\\n            handler.queue.put_nowait(item)\\n    finally:\\n        handler.cap.release()\\n        handler.released.set()\\n\\n\\n# TODO We need to use a flexible model where the data returned is\\n# extensible. For example, serial data won\\'t really have a good\\n# notion of \"timestamp\". Maybe it\\'s better to not include that.\\n\\n\\nclass VideoHandler:\\n    def __init__(self, query, url, pixel_format=\"rgb\"):\\n        self.pixel_format = pixel_format\\n        self.cap = cv2.VideoCapture(url)\\n        if not self.cap.isOpened():\\n            raise RuntimeError(f\\'could not open camera at \"{url}\".\\')\\n        self.queue = Queue(8)\\n        self.quit = Event()\\n        self.released = Event()\\n        # NOTE(sean) no further mutation can be done on VideoHandler state. all\\n        # interaction with cap *must* be done in the worker thread or via queue\\n        # and quit primitives\\n        worker = Thread(target=video_worker, args=(self,), daemon=True)\\n        worker.start()\\n\\n    def get(self, timeout=None):\\n        try:\\n            return self.queue.get(timeout=timeout)\\n        except Empty:\\n            raise TimeoutError(\"get timed out\")\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, *exc):\\n        self.quit.set()\\n        self.released.wait()  # <- wait for cleanup in worker thread\\n\\n\\nWAGGLE_DATA_CONFIG_PATH = Path(\\n    os.environ.get(\"WAGGLE_DATA_CONFIG_PATH\", \"/run/waggle/data-config.json\")\\n)\\n\\ntry:\\n    config = json.loads(WAGGLE_DATA_CONFIG_PATH.read_text())\\nexcept FileNotFoundError:\\n    logger.debug(\\n        \"could not find data config file %s. using empty resource list.\",\\n        WAGGLE_DATA_CONFIG_PATH,\\n    )\\n    config = []\\n\\n\\ndef dict_is_subset(a, b):\\n    return all(k in b and re.match(b[k], a[k]) for k in a.keys())\\n\\n\\ndef find_all_matches(query):\\n    return [c for c in config if dict_is_subset(query, c[\"match\"])]\\n\\n\\ndef find_match(query):\\n    matches = find_all_matches(query)\\n    if len(matches) == 0:\\n        raise RuntimeError(\"no matches found\")\\n    if len(matches) > 1:\\n        raise RuntimeError(\"multiple devices found\")\\n    return matches[0]\\n\\n\\nhandlers = {\\n    \"image\": ImageHandler,\\n    \"video\": VideoHandler,\\n}\\n\\n\\n# optimizations *could* happen here, on demand...\\ndef open_data_source(**query):\\n    match = find_match(query)\\n    handler = handlers[match[\"handler\"][\"type\"]]\\n    args = match[\"handler\"][\"args\"]\\n    return handler(query, **args)\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/measurements.py'}, page_content='from datetime import datetime\\nimport json\\nimport time\\n\\n\\nclass MeasurementsFile:\\n    def __init__(self, filename):\\n        self.records = []\\n\\n        with open(filename, \"r\") as f:\\n            for r in map(json.loads, f):\\n                # 2021-06-25T18:52:15.404690128Z\\n                r[\"timestamp\"] = datetime.strptime(\\n                    r[\"timestamp\"][:26], \"%Y-%m-%dT%H:%M:%S.%f\"\\n                )\\n                self.records.append(r)\\n        self.records.sort(key=lambda r: r[\"timestamp\"])\\n\\n    def play(self, nodelay=False):\\n        if len(self.records) == 0:\\n            return\\n        last_record = self.records[0]\\n        for r in self.records:\\n            delta = r[\"timestamp\"] - last_record[\"timestamp\"]\\n            if not nodelay:\\n                time.sleep(delta.total_seconds())\\n            yield r\\n            last_record = r\\n\\n\\n# MessagePlayer can take a SDR format file and replay the contents\\n# this will help support use cases where someone wants to inject known\\n# data into their plugin from a file.\\n# (think about name?)\\n# other features:\\n# should sort by timestamp / or leave no sort as flag?\\n# should be able to decide starting time\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load all python files\n",
    "py_docs = repo_class_loader(paths, './*.py', PythonLoader)\n",
    "py_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 83.6 ms, sys: 57 ms, total: 141 ms\n",
      "Wall time: 168 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sage-data-client/examples/plotting_example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Basic Plotting Example\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import sage_data_client\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"First, we\\'ll query the last 7 days of temperature data from W022\\'s BME680 sensor.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df = sage_data_client.query(\\\\n\\', \\'    start=\"-7d\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\",\\\\n\\', \\'        \"vsn\": \"W022\",\\\\n\\', \\'        \"sensor\": \"bme680\",\\\\n\\', \\'    }\\\\n\\', \\')\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"Next, we\\'ll plot a simple line chart.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df.set_index(\"timestamp\").value.plot()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"Finally, we\\'ll plot the temperature distribution.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df.value.hist(bins=100)\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Geospatial Mapping Example\\\\n\\', \\'Within this example, we walk through how to query for SAGE data, filter our values, and plot maps of the data using Cartopy, Matplotlib, and hvPlot!\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'## Imports\\\\n\\', \\'We import our sage_data_client, along with the plotting libraries matplotlib, Cartopy, hvPlot and holoviews.\\\\n\\', \\'\\\\n\\', \\'If you have not installed these packages already, make sure to run this line!\\']\\'\\n\\n\\'code\\' cell: \\'[\\'!pip3 install matplotlib bokeh holoviews hvplot cartopy pandas metpy\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import sage_data_client\\\\n\\', \\'from bokeh.models.formatters import DatetimeTickFormatter\\\\n\\', \\'import hvplot.pandas\\\\n\\', \\'import holoviews as hv\\\\n\\', \\'import cartopy.crs as ccrs\\\\n\\', \\'import cartopy.feature as cfeature\\\\n\\', \\'import matplotlib.pyplot as plt\\\\n\\', \\'from metpy.plots import USCOUNTIES\\\\n\\', \\'import pandas as pd\\\\n\\', \\'import warnings\\\\n\\', \\'\\\\n\\', \\'warnings.filterwarnings(\"ignore\")\\\\n\\', \\'hv.extension(\"bokeh\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'# Query and load data into pandas data frame\\\\n\\', \\'We have two queries we are interested in:\\\\n\\', \\'- Temperature\\\\n\\', \\'- Location data (latitude and longitude of the sensor)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'temperature_df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\",\\\\n\\', \\'    }\\\\n\\', \\')\\\\n\\', \\'\\\\n\\', \\'location_df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"sys.gps.*\",\\\\n\\', \\'    }\\\\n\\', \\')\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Investigate the Temperature Dataframe\\\\n\\', \\'Notice how the dataframe containing temperature data stores the temperature value as `value`, along with several `meta.` fields.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'temperature_df\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Investigate the Location Dataframe\\\\n\\', \\'This dataframe does not have as many metadata fields... but we do have enough to join our two dataframes. Another issue with this dataframe is that the location values are stored as individual rows, when we would ideally like these to be their own columns (ex. latitude and longitude for a given location)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'location_df\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Clean Up the Data\\\\n\\', \\'\\\\n\\', \"Let\\'s start the data cleaning process!\\\\n\", \\'\\\\n\\', \\'### Join Latitude and Longitude into the Same Rows\\\\n\\', \\'Our first step is to join our latitude and longitude values into the same row. We start by\\\\n\\', \\'- Subsetting for latitude and longitude in the dataframe\\\\n\\', \\'- Rename the fields accordingly\\\\n\\', \\'- Join the columns based on the timestamp, host, and node\\\\n\\', \\'- Drop any extra columns\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Subset the latitude values, and rename to latitude\\\\n\\', \"lat = location_df.loc[location_df.name == \\'sys.gps.lat\\']\\\\n\", \"lat_df = lat.rename(columns={\\'value\\':\\'latitude\\'})\\\\n\", \\'\\\\n\\', \\'# Subset the longitude values, and rename to latitude\\\\n\\', \"lon = location_df.loc[location_df.name == \\'sys.gps.lon\\']\\\\n\", \"lon_df = lon.rename(columns={\\'value\\':\\'longitude\\'})\\\\n\", \\'\\\\n\\', \\'# Join the latitude and longitude dataframes, returning a dataframe with shared latitude and longitude information\\\\n\\', \"joined_lats_lons = pd.merge(lat_df, lon_df, on=[\\'timestamp\\', \\'meta.host\\', \\'meta.node\\'],  how=\\'outer\\', )\\\\n\", \\'\\\\n\\', \\'# Filter out unwanted columns\\\\n\\', \"joined_lats_lons = joined_lats_lons[[x for x in joined_lats_lons.columns if ((\\'x\\' not in x) and (\\'y\\' not in x) and (\\'timestamp\\' not in x))]]\\\\n\", \\'\\\\n\\', \\'# Return our dataframe\\\\n\\', \\'joined_lats_lons\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Join our Latitude and Longitude Information with the Temperature Dataframe\\\\n\\', \\'Now that we have our location dataframe cleaned up, we can join this with the temperature dataframe, so we know where our temperature values are collected!\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Merge the dataframes using the host and node as the shared fields to join on\\\\n\\', \"df = pd.merge(temperature_df, joined_lats_lons,  on=[\\'meta.host\\', \\'meta.node\\'], how=\\'right\\')\\\\n\", \\'\\\\n\\', \\'# Drop any duplicates, based on the timestamp, host, and node\\\\n\\', \"df_filtered = df.drop_duplicates([\\'timestamp\\', \\'meta.host\\', \\'meta.node\\'])\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Run Statistics on our Dataframe\\\\n\\', \"Let\\'s say we are interested in hourly mean temperature... we can calculate that!\"]\\'\\n\\n\\'code\\' cell: \\'[\\'hours = df_filtered.timestamp.dt.hour.unique()\\']\\'\\n\\n\\'code\\' cell: \\'[\\'hourly_mean = df_filtered.groupby([df_filtered.timestamp.dt.hour,\\\\n\\', \"                                   \\'meta.node\\']).mean()\"]\\'\\n\\n\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \\'    fig = plt.figure(figsize=(14,8))\\\\n\\', \\'    ax = plt.subplot(projection=ccrs.PlateCarree())\\\\n\\', \"    s = hourly_mean.loc[hour].plot.scatter(ax=ax, x=\\'longitude\\', y=\\'latitude\\', c=\\'value\\', cmap=\\'Spectral_r\\')\\\\n\", \\'    ax.add_feature(cfeature.LAND)\\\\n\\', \\'    ax.add_feature(cfeature.STATES)\\\\n\\', \\'    ax.add_feature(cfeature.LAKES)\\\\n\\', \\'    ax.set_extent([-125, -66.5, 20, 50])\\\\n\\', \\'    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\\\\n\\', \"                  linewidth=2, color=\\'gray\\', alpha=0.5, linestyle=\\'--\\')\\\\n\", \\'    gl.top_labels = False\\\\n\\', \\'    gl.right_labels = False\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    plt.title(f\\'Average Temperature (degF) at \\\\\\\\n {time_label}\\', fontsize=16);\\\\n\", \\'    plt.show()\\\\n\\', \\'    plt.close()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Zoom in on the Chicago Area\\\\n\\', \"It\\'s nice having a national map, but let\\'s zoom into Chicago for a higher resolution view of the sensors around the city and surrounding suburbs.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \\'    fig = plt.figure(figsize=(14,8))\\\\n\\', \\'    ax = plt.subplot(projection=ccrs.PlateCarree())\\\\n\\', \"    s = hourly_mean.loc[hour].plot.scatter(ax=ax, x=\\'longitude\\', y=\\'latitude\\', c=\\'value\\', cmap=\\'Spectral_r\\', vmin=28, vmax=38)\\\\n\", \\'    ax.add_feature(cfeature.LAND)\\\\n\\', \\'    ax.add_feature(cfeature.STATES)\\\\n\\', \\'    ax.add_feature(cfeature.LAKES)\\\\n\\', \\'    ax.add_feature(USCOUNTIES)\\\\n\\', \\'    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\\\\n\\', \"                  linewidth=2, color=\\'gray\\', alpha=0.5, linestyle=\\':\\')\\\\n\", \\'    gl.top_labels = False\\\\n\\', \\'    gl.right_labels = False\\\\n\\', \\'    ax.set_extent([-89, -87, 41, 43])\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    plt.title(f\\'Average Temperature (degF) at \\\\\\\\n {time_label}\\', fontsize=16);\\\\n\", \\'    plt.show()\\\\n\\', \\'    plt.close()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Plot Interactive National Maps\\\\n\\', \\'We can use hvPlot here to plot interactive national maps\\']\\'\\n\\n\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    display(hourly_mean.loc[hour].hvplot.points(x=\\'longitude\\',\\\\n\", \"                                                y=\\'latitude\\',\\\\n\", \"                                                color=\\'value\\',\\\\n\", \"                                                cmap=\\'Spectral_r\\',\\\\n\", \\'                                                geo=True,\\\\n\\', \"                                                tiles=\\'CartoLight\\',\\\\n\", \"                                                title=f\\'Average Temperature (degF) at {time_label}\\',\\\\n\", \"                                                clabel=\\'Temperature (degF)\\',\\\\n\", \\'                                                crs=ccrs.PlateCarree()))\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Plot an Interactive Regional Map of Chicago\\\\n\\', \\'And the same for the region around Chicago\\\\n\\', \\'\\\\n\\', \\'We start first by subsetting points out of our dataframe around the Northern Illinois area.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'illinois_points = hourly_mean.loc[(hourly_mean.latitude > 41.) & (hourly_mean.longitude > -89)]\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'Now that we have this, we loop through and plot our data!\\']\\'\\n\\n\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    display(illinois_points.loc[hour].hvplot.points(x=\\'longitude\\',\\\\n\", \"                                                    y=\\'latitude\\',\\\\n\", \"                                                    color=\\'value\\',\\\\n\", \"                                                    cmap=\\'Spectral_r\\',\\\\n\", \"                                                    tiles=\\'CartoLight\\',\\\\n\", \\'                                                    geo=True,\\\\n\\', \"                                                    title=f\\'Average Temperature (degF) at {time_label}\\',\\\\n\", \"                                                    clabel=\\'Temperature (degF)\\'))\"]\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/sage-interactive-plot.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Interactive Plotting Example\\\\n\\', \\'Within this example, we walk through how to query for SAGE data, filter our values, and plot using the hvPlot interactive plotting library!\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Imports and Query\\\\n\\', \\'We import our `sage_data_client`, along with the plotting libraries `hvPlot` and `holoviews`\\\\n\\', \\'\\\\n\\', \\'**If you have not installed these packages already, make sure to run this line!**\\']\\'\\n\\n\\'code\\' cell: \\'[\\'!pip3 install matplotlib bokeh holoviews hvplot\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import sage_data_client\\\\n\\', \\'from bokeh.models.formatters import DatetimeTickFormatter\\\\n\\', \\'import hvplot.pandas\\\\n\\', \\'import holoviews as hv\\\\n\\', \\'import warnings\\\\n\\', \\'\\\\n\\', \\'# query and load data into pandas data frame\\\\n\\', \\'df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\"\\\\n\\', \\'    }\\\\n\\', \\')\\\\n\\', \\'\\\\n\\', \\'warnings.filterwarnings(\"ignore\")\\\\n\\', \\'hv.extension(\"bokeh\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Clean our Data\\\\n\\', \\'When we first visualize our dataset (temperature), notice how we have some **very** low values (< -100 degrees Fahrenheit).\\']\\'\\n\\n\\'code\\' cell: \\'[\\'df.value.plot();\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'We can flag these values as bad data using the following:\\']\\'\\n\\n\\'code\\' cell: \\'[\\'df = df[df.value > -100]\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'Now, if we visualize our data again, notice how we do not have these abnormally low values.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'df.value.plot();\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Create an Interactive Plot\\\\n\\', \\'We can use hvPlot to plot our data! Instead of using `.plot()` like we did before, which creates a matplotlib static plot, we can use `.hvplot()` which creates an interactive plot. \\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Set\\\\n\\', \\'formatter = DatetimeTickFormatter(days=\"%d %b %Y \\\\\\\\n %H:%M UTC\",\\\\n\\', \\'                                  hours=\"%d %b %Y \\\\\\\\n %H:%M UTC\",\\\\n\\', \\'                                  minutes=\"%d %b %Y \\\\\\\\n %H:%M UTC\",)\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \"df.hvplot.line(x=\\'timestamp\\',\\\\n\", \"               y=\\'value\\',\\\\n\", \"               ylabel=\\'Temperature (degF)\\',\\\\n\", \"               xlabel=\\'Time\\',\\\\n\", \"               by=\\'meta.vsn\\',\\\\n\", \\'               groupby=[\"meta.sensor\"],\\\\n\\', \\'               xformatter=formatter, \\\\n\\', \"               color=hv.Palette(\\'Category20\\'),\\\\n\", \\'               height=400,\\\\n\\', \\'               width=600)\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load all notebook files\n",
    "ipynb_docs = repo_class_loader(paths, './*.ipynb', NotebookLoader)\n",
    "ipynb_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Helper Functions and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits and combines all md, py, ipynb docs based on chunk size and chunk overlap\n",
    "# Returns combined splits and size\n",
    "def split_and_combine_docs(chunk_size, chunk_overlap):\n",
    "     # Split Markdown files\n",
    "    md_splitter = MarkdownTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    md_splits = md_splitter.split_documents(md_docs)\n",
    "\n",
    "    # Split Python files\n",
    "    py_splitter = PythonCodeTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    py_splits = py_splitter.split_documents(py_docs)\n",
    "\n",
    "    # Split Notebook files\n",
    "    ipynb_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    ipynb_splits = ipynb_splitter.split_documents(ipynb_docs)\n",
    "\n",
    "    # combine splits\n",
    "    combined_splits = md_splits + py_splits + ipynb_splits\n",
    "\n",
    "    return combined_splits, len(combined_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to combine the list of contexts retrieved during RAG\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of questions\n",
    "text_questions = [\n",
    "    \"Why is the sky blue?\",\n",
    "    \"What is the Sage project?\",\n",
    "    \"Is there a concrete example of a Sage node being used?\",\n",
    "    \"Can you give me a breakdown of the Sage and Waggle architecture/infrastructure?\"\n",
    "]\n",
    "\n",
    "code_questions = [\n",
    "    \"Can you provide the code to make a blue box in Python?\",\n",
    "    \"Can you provide a simple Waggle plugin that captures an image and publishes the average RGB values?\",\n",
    "    \"Can you provide the code to plot the temperature of node W0B0 for the last three hours?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sage template and prompt\n",
    "sage_template = \"\"\"You are an expert on the Sage project, and you can answer any question related to it.\n",
    "Use ONLY the context about the Sage project to answer in as much detail possible the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "If asked to provide code, only generate code provided in the context.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "sage_prompt = ChatPromptTemplate.from_template(sage_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base model for generation\n",
    "llm = Ollama(model='llama3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aldomalaquias/miniconda3/envs/sage/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/aldomalaquias/miniconda3/envs/sage/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/aldomalaquias/miniconda3/envs/sage/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Embedding models\n",
    "mxbai_embed_large = OllamaEmbeddings(model='mxbai-embed-large')\n",
    "\n",
    "\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "mpnet_base_v2 = HuggingFaceEmbeddings(\n",
    "                model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "                model_kwargs=model_kwargs,\n",
    "                encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test Speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test speed of embedding given 3 different split amounts at a fixed chunk size, chunk overlap, and vectorbase\n",
    "def plot_embed_speeds(models):\n",
    "    chunk_size = 300\n",
    "    chunk_overlap = 0\n",
    "    number_points = 3\n",
    "    splits, size = split_and_combine_docs(chunk_size, chunk_overlap)\n",
    "    splits_string = [doc.page_content for doc in splits]\n",
    "    indices = np.linspace(0, size, number_points, dtype=int)\n",
    "    times1 = []\n",
    "    times2 = []\n",
    "\n",
    "    for amount in indices:\n",
    "        st = time.time()\n",
    "        vb = Chroma.from_documents(splits[0:amount+1], models[0])\n",
    "        et = time.time()\n",
    "        diff = et - st\n",
    "        times1.append(diff)\n",
    "\n",
    "        st = time.time()\n",
    "        vb2 = FAISS.from_texts(splits_string[0:amount+1], models[1])\n",
    "        et = time.time()\n",
    "        diff = et - st\n",
    "        times2.append(diff)\n",
    "    \n",
    "    plt.scatter(indices, times1, c='b', marker='o')\n",
    "    plt.scatter(indices, times2, c='g', marker='x')\n",
    "    classes = ['mxbai-embed-large', 'mpnet-base-v2']\n",
    "    plt.legend(labels=classes)\n",
    "    plt.xlabel(\"Number of splits\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_embed_speeds([mxbai_embed_large, mpnet_base_v2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
